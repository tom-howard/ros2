{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#_1","title":"Home","text":""},{"location":"#ros-2-lab-courses-in-the-diamond","title":"ROS 2 Lab Courses in The Diamond","text":"<p>Practical Robotics Lab Courses with ROS 2 and the TurtleBot3 Waffle.</p> <p> </p> A TurtleBot3 Waffle Image courtesy of Andy Brown <p>Find out more...</p> <p></p>"},{"location":"about/","title":"About this Site","text":""},{"location":"about/#about-this-site","title":"About this Site","text":"<p>This is the home of ROS 2 course materials developed by Tom Howard. Tom is a University Teacher in the Multidisciplinary Engineering Education (MEE) Team at the University of Sheffield.</p> <p>These resources are designed to support a number of practical robotics lab courses run in The Diamond: </p> <ul> <li>Robotics (COM2009): an undergraduate module for The School of Computer Science</li> <li>Mobile Robotics and Autonomous Systems (ELE434): a masters-level Mechatronic and Robotic Engineering module in The School of Electrical and Electronic Engineering</li> <li>Industry 4.0 (AMR31001): A third-year module on the AMRC Training Centre Degree Apprenticeship</li> </ul> <p>These courses are designed to teach students how to use ROS 2 (The Robot Operating System, Version 2) with TurtleBot3 Waffle robots, using a mix of simulation-based learning and real robot hardware. Most of the initial learning is done in simulation, after which students are able to apply their new-found ROS 2 knowledge to our real TurtleBot3 Waffle Robots.</p> <p>This is an Open Educational Resource (OER).</p>"},{"location":"about/changelog/","title":"Version History","text":""},{"location":"about/changelog/#iteration-2","title":"Iteration 2","text":"<p>Academic Year: 2025-26</p> <ul> <li>Course materials for all modules now moved over to this single site.</li> <li>Course material (and robots) upgraded for ROS 2 Jazzy (where last year they were based around ROS 2 Humble)</li> <li>Simulation resources now updated for Gazebo Ignition</li> </ul>"},{"location":"about/changelog/#iteration-1","title":"Iteration 1","text":"<p>Academic Year: 2024-25 </p> <ul> <li>This course used to live here, and was based on ROS 1 Noetic. This year, we've upgraded everything to ROS 2, re-written the courses and moved everything over to this new site. See the previous version history here.</li> <li>Other notable changes:<ul> <li>COM2009 Assignment #2 now only involves three programming tasks (where previously it was four), with an additional assessment on documentation now included instead.</li> </ul> </li> </ul>"},{"location":"about/license/","title":"License","text":""},{"location":"about/license/#license","title":"License","text":"<p>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</p> <p>You are free to distribute, remix, adapt, and build upon this work (for non-commercial purposes only) as long as credit is given to the original author.</p> <p></p><p></p>"},{"location":"about/robots/","title":"Introducing the Robots","text":""},{"location":"about/robots/#robots","title":"The TurtleBot3 Waffle","text":""},{"location":"about/robots/#turtlebot-what","title":"Turtlebot what?!","text":"<p>To teach ROS here we use the TurtleBot3 Waffle robot, made by Robotis. This is the 3rd Generation Robot in the TurtleBot family (which has been the reference hardware platform for ROS since 2010). The TurtleBot Robot family exists to provide accessible and relatively low-cost hardware and open-source software on a robot platform, to encourage people to learn robotics and ROS and make it as easy as possible to do so.</p>"},{"location":"about/robots/#our-waffles","title":"Our Waffles","text":"<p>Here in the Diamond we have a total of 50 customised TurtleBot3 Waffles (aka \"The Waffles\") specifically for teaching the courses here:</p> <p> </p> <p>Our robots are an enhanced version of the TurtleBot3 WafflePi that you can buy from Robotis. We've made a few adjustments, as shown below:</p> <p></p> <p>The Waffles have the following core hardware elements:</p> <ul> <li>An OpenCR Micro-Controller Board to power and control the wheel motors, distribute power to other hardware elements and provide an interface for additional sensors.</li> <li>An UP Squared Single-Board Computer (SBC) with an Intel Processor and 32GB of on-board eMMC storage. This board acts as the \"brain\" of the robot.</li> <li>Independent left and right wheel motors (DYNAMIXEL XM430\u2019s) to drive the robot using a differential drive configuration.</li> </ul> <p>This drive configuration allows the robots to move with the following maximum velocities: </p> <p></p><p></p> Velocity Component Upper Limit Units Linear 0.26 m/s Angular 1.82 rad/s <p></p><p></p> <p>In addition to this, the robots are equipped with the following sensors:</p> <ul> <li>A Light Detection and Ranging (or LiDAR) sensor, which spins continuously when the robot is in operation. This uses light in the form of laser pulses to allow the robot to measure the distance to surrounding objects, providing it with a 360\u00b0 view of its environment.</li> <li>An Intel RealSense D435 Camera with left and right imaging sensors, allowing depth sensing as well as standard image capture.</li> <li>A 9-Axis Inertial Measurement Unit (or IMU) on-board the OpenCR Micro Controller board, which uses an accelerometer, gyroscope and magnetometer to measure the robot's specific force, acceleration and orientation. </li> <li>Encoders in each of the DYNAMIXEL wheel motors, allowing measurement of speed and rotation count for each of the wheels.</li> </ul>"},{"location":"about/robots/#software","title":"Software","text":"<p>Our robots currently run ROS 2 Jazzy Jalisco (or \"Jazzy\" for short). The courses here are therefore based around this version of ROS. The easiest way to install Jazzy is via Deb packages for Ubuntu Noble Numbat (24.04). This is the setup we recommend and - as such - all our robotics hardware runs with this OS/Software setup.</p> <p>To deliver the simulation-based parts of this course, we've created a custom simulation environment using the Windows Subsystem for Linux (WSL). This has been developed primarily to run on University of Sheffield Managed Desktop Computers, which run Windows 11, but it's also possible to run this on other machines too. We call this simulation environment \"WSL-ROS2\". See here for more details. (You can find out more about installing ROS on your own system here too).</p>"},{"location":"about/robots/#laptops","title":"Laptops","text":"<p>In the Diamond, we have dedicated Robot Laptops running the same OS &amp; ROS version as above. We use these when working with the robots in the lab. See here for more details.</p>"},{"location":"amr/","title":"AMR31001 Industry 4.0","text":""},{"location":"amr/#industry-40","title":"Industry 4.0","text":"<p>As part of this module you will take part in two lab sessions in the Diamond, where you will learn about how ROS can be used to program and control robots. You'll do some Python programming and look at how sensor data can be used to control a robot's actions. </p> <ul> <li>Lab 1: Mobile Robotics </li> <li>Lab 2: Feedback Control</li> </ul>"},{"location":"amr/lab1/","title":"Lab 1: Mobile Robotics","text":"<p>Info</p> <p>You should be able to complete exercises 1-7 on this page within a two-hour lab session.</p>"},{"location":"amr/lab1/#introduction","title":"Introduction","text":"<p>In this first AMR31001 'Industry 4.0' Lab you will learn how to use ROS 2 (the latest version of the Robot Operating System) to control a robot's motion.</p> <p>ROS 2 is an open-source, industry-standard robot programming framework, used in a range of industries such as agriculture, warehouse and factory automation and advanced manufacturing. </p> <p>ROS 2 allows us to programme robots using a range of different programming languages (including C++, Java, MATLAB etc.), but we'll be using Python for these labs. In addition to this, ROS 2 runs on top of a Linux operating system called 'Ubuntu', and so we'll also learn a bit about how to use this too.</p> <p>We'll be working with robots called 'TurtleBot3 Waffles', which you can find out a bit more about here. </p> <p>Pre-Lab Work</p> <p>You must have completed the Pre-Lab Test before you can make a start on this lab. This is available on the AMR31001 Blackboard Course Page.</p>"},{"location":"amr/lab1/#aims","title":"Aims","text":"<p>In this lab you'll learn how to use ROS 2 to make a robot move, and we'll also look at how to create our own basic ROS 2 script (or 'Node'), using Python.</p> <p>From here on, we'll refer to ROS 2 as \"ROS\" for convenience!</p>"},{"location":"amr/lab1/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Control a TurtleBot3 Waffle Robot, from a laptop, using ROS.</li> <li>Launch ROS applications on the laptop and the robot using <code>ros2 launch</code> and <code>ros2 run</code>.</li> <li>Interrogate a ROS network using ROS command-line and graphical tools.</li> <li>Use ROS Communication Methods to publish messages.</li> <li>Use a Linux operating system and work within a Linux Terminal.</li> </ol>"},{"location":"amr/lab1/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Launching ROS and Making your Robot Move</li> <li>Exercise 2: Seeing the Waffle's Sensors in Action!</li> <li>Exercise 3: Visualising the ROS Network</li> <li>Exercise 4: Exploring ROS Topics and Messages</li> <li>Exercise 5: Publishing Velocity Commands to the <code>/cmd_vel</code> Topic</li> <li>Exercise 6: Creating a ROS Package</li> <li>Exercise 7: A Python node to make the robot move</li> <li>Exercise 8 (Advanced): Alternative Motion Paths</li> </ul>"},{"location":"amr/lab1/#the-lab","title":"The Lab","text":"<p>Assessment Info</p> <p>This lab is summatively assessed.</p> <ol> <li>There's a post-lab quiz that you'll need to complete after this lab session has taken place, which will be released on Blackboard.</li> <li>You'll also be marked on the work that you do in the lab for Exercise 7.</li> </ol>"},{"location":"amr/lab1/#getting-started","title":"Getting Started","text":"<p>Before you do anything, you'll need to get your robot up and running, and make sure ROS is launched.</p>"},{"location":"amr/lab1/#ex1","title":"Exercise 1: Launching ROS and Making your Robot Move","text":"<p>You should have already been provided with a Robot and a Laptop (in fact, you're probably already reading this on the laptop!) </p> <ol> <li> <p>First, identify the robot that you have been provided with. Each of our robots are uniquely named: <code>dia-waffleX</code>, where <code>X</code> is the 'Robot Number' (a number between 1 and 50). Check the label printed on top of the robot to find out which one you have!</p> </li> <li> <p>Open up a terminal instance on the laptop, either by pressing the Ctrl+Alt+T buttons on your keyboard all at the same time, or by clicking the Terminal App icon in the favourites bar on the left-hand side of the desktop:</p> <p></p> <p></p> <p>We'll refer to this terminal as TERMINAL 1.</p> </li> <li> <p>In TERMINAL 1 type the following command to pair the laptop and robot, so that they can work together:</p> <p>TERMINAL 1: </p><pre><code>waffle X pair\n</code></pre> ... replacing <code>X</code> with the number of the robot that you have been provided with.<p></p> </li> <li> <p>Enter the password for the robot when requested (we'll tell you what this is in the lab).</p> <p>You may see a message like this early on in the pairing process:</p> <p></p> <p></p> <p>If so, just type <code>yes</code> and then hit Enter to confirm that you want to continue.</p> </li> <li> <p>Once the pairing process is finished you should see a message saying <code>pairing complete</code>, displayed in blue in the terminal. </p> </li> <li> <p>Then, in the same terminal (TERMINAL 1), enter the following command:</p> <p>TERMINAL 1: </p><pre><code>waffle X term\n</code></pre> (again, replacing <code>X</code> with the number of your robot).<p></p> <p>Any text that was in the terminal should now disappear, and a green banner should appear across the bottom of the terminal window:</p> <p></p> <p></p> <p>This is a terminal instance running on the robot, and any commands that you enter here will be executed on the robot (not the laptop!)</p> </li> <li> <p>Now, launch ROS on the robot by entering the following command:</p> <p>TERMINAL 1: </p><pre><code>ros2 launch tuos_tb3_tools ros.launch.py enable_depth:=true\n</code></pre><p></p> <p>Tip</p> <p>To paste text into a Linux terminal you'll need to use the Control + Shift + V keyboard keys: Ctrl+Shift+V</p> <p>If all is well then the robot will play a nice \"do-re-me\" sound and a message like this should appear (amongst all the other text):</p> <pre><code>[tb3_status.py-#] ######################################\n[tb3_status.py-#] ### dia-waffleX is up and running! ###\n[tb3_status.py-#] ######################################\n</code></pre> <p>You shouldn't need to interact with this terminal instance any more now, but the screen will provide you with some regular real-time info related to the status of the robot. As such, keep this terminal open in the background and check on the <code>Battery</code> indicator every now and then:</p> <pre><code>Battery: 12.40V [100%]\n</code></pre> <p>Low Battery </p> <p>The robot's battery won't last a full 2-hour lab session!!</p> <p>When the capacity indicator reaches around 15% then it will start to beep, and when it reaches ~10% it will stop working all together.  Let a member of the teaching team know when the battery is running low and we'll replace it for you. (It's easier to do this when it reaches 15%, rather than waiting until it runs below 10%!)</p> <p>ROS is now up and running on the robot, and we're ready to go!</p> <p>You should leave TERMINAL 1 alone now, just leave it running in the background for the rest of the lab.</p> </li> <li> <p>The next crucial step is to connect the laptop to the ROS network that we've just established on the robot. The two devices will communicate with one another via the University Wireless network, but there's one more step required to link them together. </p> <p>Open up a new terminal instance on the laptop (either by using the Ctrl+Alt+T keyboard shortcut, or by clicking the Terminal App icon) and enter the following command:</p> <p>TERMINAL 2:</p> <pre><code>ros2 run rmw_zenoh_cpp rmw_zenohd\n</code></pre> <p>Leave both of these terminals alone, but keep them running in the background at all times while working with your robot.</p> </li> <li> <p>Next, open up a new terminal instance on the laptop (by pressing Ctrl+Alt+T or clicking the Terminal App desktop icon, as you did before). We'll call this one TERMINAL 3.</p> </li> <li> <p>In TERMINAL 3 enter the following command:</p> <p>TERMINAL 3: </p><pre><code>ros2 run turtlebot3_teleop teleop_keyboard\n</code></pre><p></p> </li> <li> <p>Follow the instructions provided in the terminal to drive the robot around using specific buttons on the keyboard:</p> <p></p> <p></p> </li> <li> <p>Enter Ctrl+C in TERMINAL 3 to stop the Teleop node when you've had enough fun.</p> </li> </ol>"},{"location":"amr/lab1/#packages-and-nodes","title":"Packages and Nodes","text":"<p>ROS applications are organised into packages. Packages are basically folders containing scripts, configurations and launch files (ways to launch those scripts and configurations).  </p> <p>Scripts tell the robot what to do and how to act. In ROS, these scripts are called nodes. ROS Nodes are executable programs that perform specific robot tasks and operations. These are typically written in C++ or Python, but it's possible to write ROS Nodes using other programming languages too.</p> <p>In Exercise 1 you launched a whole range of different nodes on the ROS Network using the <code>ros2 launch</code> and <code>ros2 run</code> commands: </p> <ol> <li><code>ros2 launch tuos_tb3_tools ros.launch.py ...</code> (on the robot, in TERMINAL 1)</li> <li><code>ros2 run rmw_zenoh_cpp rmw_zenohd</code> (on the laptop, in TERMINAL 2)</li> <li><code>ros2 run turtlebot3_teleop teleop_keyboard</code> (on the laptop, in TERMINAL 3)</li> </ol> <p>The first of the above was a ROS <code>launch</code> command, which has the following key parts to it (after the <code>ros2 launch</code> bit):</p> <pre><code>ros2 launch {[1] Package name} {[2] Launch file} {[3] Arguments (optional)}\n</code></pre> <p>The first two of these are the most important: </p> <p>Part [1] specifies the name of the ROS package containing the functionality that we want to execute. Part [2] is a file within that package that tells ROS exactly what scripts ('nodes') that we want to launch. We can launch multiple nodes at the same time from a single launch file.  </p> <p>The second and third commands were ROS <code>run</code> commands: </p> <pre><code>ros2 run {[1] Package name} {[2] Node name}\n</code></pre> <p>Here, Part [1] is the same as the <code>ros2 launch</code> command, but Part [2] is slightly different: <code>{[2] Node name}</code>. Here we are directly specifying a single script that we want to execute. We therefore use <code>ros2 run</code> if we only want to launch a single node on the ROS network (e.g. <code>teleop_keyboard</code>, which is a Python script).</p> <p>Post-lab</p> <p>What were the names of the three packages that we invoked in Exercise 1?</p>"},{"location":"amr/lab1/#ex2","title":"Exercise 2: Seeing the Waffle's Sensors in Action!","text":"<p>Our Waffles have some pretty sophisticated sensors on them, allowing them to \"see\" the world around them. We won't really make much use of these during this lab, but this next exercise will allow you to see how the data from these devices could be used to help our robots do some very advanced things (with some clever programming, of course!)</p>"},{"location":"amr/lab1/#part-1-the-camera","title":"Part 1: The Camera","text":"<ol> <li> <p>There shouldn't be anything running in TERMINAL 3 now, after you closed down the <code>teleop_keyboard</code> node at the end of the previous exercise (Ctrl+C). Return to this terminal and launch the <code>rqt_image_view</code> node:</p> <p>TERMINAL 3: </p><pre><code>ros2 run rqt_image_view rqt_image_view\n</code></pre><p></p> <p>Post-lab</p> <ol> <li>We're using <code>ros2 run</code> here again, what does this mean?</li> <li>Why did we have to type <code>rqt_image_view</code> twice?</li> </ol> </li> <li> <p>A new window should open. Maximise this (if it isn't already) and then select <code>/camera/color/image_raw</code> from the dropdown menu at the top-left of the application window.</p> </li> <li>Live images from the robot's camera should now be visible! Stick your face in front of the camera and see yourself appear on the laptop screen!</li> <li> <p>Close down the window once you've had enough (enter Ctrl+C in TERMINAL 3). This should release TERMINAL 3 so that you can enter commands in it again.</p> <p>The camera on the robot is quite a clever device. Inside the unit is two separate image sensors, giving it - effectively - both a left and right eye. The device then combines the data from both of these sensors and uses the combined information to infer depth from the images as well. Let's have a look at that in action now...</p> </li> <li> <p>In TERMINAL 3 enter the following command:</p> <p>TERMINAL 3: </p><pre><code>ros2 launch tuos_tb3_tools rviz.launch.py\n</code></pre><p></p> <p>This will launch an application called RViz, which is a handy tool that allows us to visualise the data from all the sensors on-board our robots. When RViz opens, you should see something similar to the following:</p> <p></p> <p></p> <p>In the \"Displays\" menu on the left-hand side, click on the tick box next to the \"DepthCloud\" item. </p> <p></p> <p></p> <p>The strange wobbly sheet of colour that should appear in front of the robot is the live image stream from the camera with depth applied to it at the same time. The camera is able to determine how far away each image pixel is from the camera lens, and then uses that to generate this 3-dimensional representation. </p> </li> <li> <p>Again, place your hand or your face in front of the camera and hold steady for a few seconds (there may be a bit of a lag as all of this data is transmitted over the WiFi network). You should see yourself rendered in 3D in front of the robot! </p> </li> </ol>"},{"location":"amr/lab1/#part-2-the-lidar-sensor","title":"Part 2: The LiDAR Sensor","text":"<p>In RViz you may have also noticed a lot of green dots scattered around the robot. This is a representation of the displacement data coming from the LiDAR sensor (the black device on the top of the robot). The LiDAR sensor spins continuously, sending out laser pulses into the environment as it does so. When a pulse hits an object it is reflected back to the sensor, and the time it takes for this to happen is used to calculate how far away the object is.</p> <p>The LiDAR sensor spins and performs this process continuously at 1\u00b0 increments, so a full 360\u00b0 scan of the environment can be generated. This data is therefore really useful for things like obstacle avoidance and mapping. We'll have a quick look at the latter now.</p> <ol> <li> <p>Close down RViz (click the \"Close without saving\" button, if asked).</p> </li> <li> <p>Head back to TERMINAL 3 and run the following command:</p> <p>TERMINAL 3: </p><pre><code>ros2 launch tuos_tb3_tools slam.launch.py\n</code></pre><p></p> <p>A new RViz screen will open up, this time showing the robot from a top-down view, and with the LiDAR data represented by multi-coloured dots this time instead.</p> <p></p> <p></p> <p>Underneath the LiDAR dots you should notice a map starting to form, with black lines representing fixed objects in the environment and white areas representing free space that the robot could travel around. ROS is using a process called SLAM (Simultaneous Localisation and Mapping) to generate a map of the environment, using the data from the LiDAR sensor.</p> </li> <li> <p>Open up a new terminal instance now, we'll call this one TERMINAL 4. Launch the <code>teleop_keyboard</code> node in this one, in the same way that you did earlier:</p> <p>TERMINAL 4: </p><pre><code>ros2 run turtlebot3_teleop teleop_keyboard\n</code></pre><p></p> </li> <li> <p>Drive the robot around a bit and watch how the map in RViz is updated as the robot explores new parts of the environment.</p> </li> <li> <p>Enter Ctrl+C in TERMINAL 4 to stop the <code>teleop_keyboard</code> node.</p> </li> <li> <p>Close down the RViz window, or enter Ctrl+C in TERMINAL 3 to stop it too.</p> </li> </ol> <p>We've now used both <code>ros2 launch</code> and <code>ros2 run</code> to launch ROS applications. These are both ROS command-line tools, and there are many others at our disposal. </p> <p>Using <code>ros2 run</code> and <code>ros2 launch</code>, as we have done so far, it's easy to end up with a lot of different processes or ROS Nodes running on the network, some of which we will interact with, but others may just be running in the background. It is often useful to know exactly what is running on the ROS network, and there are a number of ways to do this.</p>"},{"location":"amr/lab1/#ex3","title":"Exercise 3: Visualising the ROS Network","text":"<ol> <li> <p>There shouldn't be anything running in TERMINAL 3 now, so return to this terminal and use the <code>ros2 node</code> command to list the nodes that are currently running on the robot:</p> <p>TERMINAL 3: </p><pre><code>ros2 node list\n</code></pre><p></p> <p>You should see a list of 6 items.</p> </li> <li> <p>We can visualise the connections between the active nodes by using a ROS node called <code>rqt_graph</code>. Launch this as follows:</p> <p>TERMINAL 3: </p><pre><code>ros2 run rqt_graph rqt_graph\n</code></pre><p></p> </li> <li> <p>In the window that opens, select <code>Nodes/Topics (active)</code> from the dropdown menu in the top left. </p> <p>What you should then see is a map of all the nodes in the list from above (as ovals), and arrows to illustrate the flow of information between them. This is a visual representation of the ROS network!</p> <p></p> <p></p> <p>Items that have a rectangular border are ROS Topics. ROS Topics are essentially communication channels, and ROS nodes can read (subscribe) or write (publish) to these topics to access sensor data, pass information around the network and make things happen.</p> </li> <li> <p>Return to TERMINAL 4 and launch the <code>teleop_keyboard</code> node again:</p> <p>TERMINAL 4: </p><pre><code>ros2 run turtlebot3_teleop teleop_keyboard\n</code></pre><p></p> </li> <li> <p>Go back to the RQT Graph window now and hit the refresh icon (to the left of the <code>Nodes/Topics (active)</code> dropdown menu). </p> <p></p> <p></p> <p>Post-lab</p> <p>What's changed? Make sure you know how to interpret these graphs.</p> </li> </ol> <p>A ROS Robot could have hundreds of individual nodes running simultaneously to carry out all its necessary operations and actions. Each node runs independently, but uses ROS communication methods to communicate and share data with the other nodes on the ROS Network.</p>"},{"location":"amr/lab1/#publishers-and-subscribers-a-ros-communication-method","title":"Publishers and Subscribers: A ROS Communication Method","text":"<p>ROS Topics are key to making things happen on a robot. Nodes can publish (write) and/or subscribe to (read) ROS Topics in order to share data around the ROS network. We have to use standardised data structures in ROS in order for this to all work. Different topics use different data structures, and there are a lot of different data structure types available for us to use (we can even define our own, but this is beyond the scope if this lab session). Let's have a look at Topics and their data structures in a bit more detail now...</p>"},{"location":"amr/lab1/#ex4","title":"Exercise 4: Exploring ROS Topics and Interfaces","text":"<p>Much like the <code>ros2 node list</code> command, we can use <code>ros2 topic list</code> to list all the topics that are currently active on the network.</p> <ol> <li> <p>Close down the <code>rqt_graph</code> window if you haven't done so already. This will release TERMINAL 3 so that we can enter commands in it again. Leave the <code>teleop_keyboard</code> node in TERMINAL 4 running. Return to TERMINAL 3 and enter the following:</p> <p>TERMINAL 3: </p><pre><code>ros2 topic list\n</code></pre><p></p> <p>A much larger list of items should be printed to the terminal now. See if you can spot <code>/cmd_vel</code> in the list.</p> <p>This topic is used to control the velocity of the robot ('command velocity').</p> </li> <li> <p>Let's find out more about this using the <code>ros2 topic info</code> command.</p> <p>TERMINAL 3: </p><pre><code>ros2 topic info /cmd_vel\n</code></pre><p></p> <p>This should provide an output similar to the following: </p> <pre><code>Type: geometry_msgs/msg/TwistStamped\nPublisher count: 1\nSubscription count: 1\n</code></pre> <p>This tells us a few things: </p> <ol> <li>The <code>/cmd_vel</code> topic currently has 1 publisher (i.e. 1 node writing data to the topic).</li> <li>There's also 1 subscriber (i.e. another node reading the data being written to the topic).</li> <li>If we think back to <code>rqt_graph</code> (from the previous exercise), we know that the publisher is the <code>/teleop_keyboard</code> node, and the subscriber is a node called <code>/turtlebot3_node</code>. This node turns the topic data into motor commands, resulting in actual motion of the robot's wheels.</li> <li> <p>The type of data structure used by the <code>/cmd_vel</code> topic is defined as:  </p> <p></p><code>geometry_msgs/msg/TwistStamped</code><p></p> <p>This is a ROS \"Interface\". </p> <p>Interfaces</p> <p>Data structures in ROS 2 are called Interfaces. </p> <p>From the output above, <code>Type</code> refers to the type of data structure (i.e. the type of interface). The <code>Type</code> definition has three parts to it: <code>geometry_msgs</code>, <code>msg</code> and <code>TwistStamped</code>:</p> <ol> <li><code>geometry_msgs</code> is the name of the ROS package that this interface (data structure) belongs to</li> <li><code>msg</code> tells us that it's a topic message interface (rather than another interface type, of which there are others, but we don't need to worry about them here)</li> <li><code>TwistStamped</code> is the name of the message interface. </li> </ol> <p>We have just learnt then, that if we want to make the robot move we need to publish <code>TwistStamped</code> interface messages to the <code>/cmd_vel</code> topic. </p> </li> </ol> </li> <li> <p>We can use the <code>ros2 interface</code> command to find out more about the <code>TwistStamped</code> message:</p> <p>TERMINAL 3: </p><pre><code>ros2 interface show geometry_msgs/msg/TwistStamped\n</code></pre><p></p> <p>From this, the bottom bit is of most interest to us: </p> <pre><code>Twist twist\n        Vector3  linear\n                float64 x\n                float64 y\n                float64 z\n        Vector3  angular\n                float64 x\n                float64 y\n                float64 z\n</code></pre> <p>Hmmm, this looks complicated. Let's find out what it all means...</p> </li> </ol>"},{"location":"amr/lab1/#velocity-control","title":"Velocity Control","text":"<p>The motion of any mobile robot can be defined in terms of its three principal axes: <code>X</code>, <code>Y</code> and <code>Z</code>. In the context of our TurtleBot3 Waffle, these axes (and the motion about them) are defined as follows:</p> <p></p> <p>In theory then, a robot can move linearly or angularly about any of these three axes, as shown by the arrows in the figure. That's six Degrees of Freedom (DOFs) in total, achieved based on a robot's design and the actuators it is equipped with. Take a look back at the <code>ros2 interface show</code> output in TERMINAL 3. Hopefully it's a bit clearer now that these topic messages are formatted to give a ROS Programmer the ability to ask a robot to move in any one of its six DOFs. </p> <pre><code>Vector3  linear\n        float64 x  &lt;-- Forwards (or Backwards)\n        float64 y  &lt;-- Left (or Right)\n        float64 z  &lt;-- Up (or Down)\nVector3  angular\n        float64 x  &lt;-- \"Roll\"\n        float64 y  &lt;-- \"Pitch\"\n        float64 z  &lt;-- \"Yaw\"\n</code></pre> <p>Our TurtleBot3 only has two motors, so it doesn't actually have six DOFs! These two motors can be controlled independently, in a \"differential drive\" configuration, but this still only allows it to move with two degrees of freedom in total, as illustrated below.</p> <p></p> <p>Velocity can therefore only be applied linearly in the x-axis (Forwards/Backwards) and angularly in the z-axis (Yaw). </p> <p>Post-lab</p> <p>Take note of all this, there may be a question on it!</p>"},{"location":"amr/lab1/#ex5","title":"Exercise 5: Publishing Velocity Commands to the \"cmd_vel\" Topic","text":"<ol> <li>Stop the <code>teleop_keyboard</code> node now by entering Ctrl+C in TERMINAL 4. We're going to use another graphical tool to help us publish messages to the <code>/cmd_vel</code> topic directly now.</li> <li> <p>Go back to TERMINAL 3 and enter the following command to launch the RQT Message Publisher node:</p> <p>TERMINAL 3: </p><pre><code>ros2 run rqt_publisher rqt_publisher\n</code></pre><p></p> <p></p> <p></p> </li> <li> <p>In the \"Topic\" dropdown menu select <code>/cmd_vel</code>.</p> </li> <li> <p>Move along to the right and enter a value of <code>10</code> in the box next to the \"Freq.\" label.</p> </li> <li> <p>Further to the right, click on the  box to add this as a publisher to the main \"Publisher Table\".</p> </li> <li> <p>In the Publisher Table, click on the  next to <code>/cmd_vel</code>, to expand the item and reveal two further items: <code>header</code> and <code>twist</code>:</p> <p></p> <p></p> </li> <li> <p>Click on the  icon next to <code>twist</code>, and then the subsequent  icons next to the <code>linear</code> and <code>angular</code> items that appear below this. Finally, you'll see some values in the \"expression\" column:</p> <p></p> <p></p> <p>Does this look familiar to the interface definition as we viewed it in the terminal before?</p> </li> <li> <p>Using what we learnt above about the way the robot can actually move, change one of the six values in the \"expression\" column that you think might make robot rotate on the spot. Before you do this, it's worth noting the following things:</p> <ol> <li>The unit of linear velocity is meters per second (m/s).</li> <li>The unit of angular velocity is radians per second (rad/s).</li> <li>Our Waffle robots can move with a maximum linear velocity of 0.26 m/s and a maximum angular velocity of 1.82 rad/s.</li> </ol> </li> <li> <p>Once you've entered a value, click on the checkbox to the left of <code>/cmd_vel</code> to start publishing these values to the topic. Observe what your robot does!</p> </li> <li> <p>Set the value back to <code>0.0</code> and then hit Enter to make the robot stop moving.</p> </li> <li> <p>Next, find an alternative velocity value that you can set in order to make the robot move forwards this time. (Don't forget to set the value back to <code>0.0</code> to make the robot stop moving again afterwards.)</p> </li> <li> <p>Finally, enter a combination of velocity values to make the robot move in a circle.</p> </li> <li> <p>Once you're finished, set all velocities back to <code>0.0</code>, make sure the robot is no longer moving, and then uncheck the box next to <code>/cmd_vel</code> to stop publishing messages. Click on the  button in the top right-hand corner of the Message Publisher window to close it down.</p> </li> </ol> <p>Hopefully you can see now that, in order to make a robot move, it's simply a case of publishing the right ROS Interface Message (<code>TwistStamped</code>) to the right ROS Topic (<code>/cmd_vel</code>). Earlier on in the lab we used the <code>teleop_keyboard</code> node to drive the robot around, a bit like a remote control car. In the background here all that was really happening was that the node was converting our keyboard button presses into velocity commands and publishing these to the <code>/cmd_vel</code> topic. In the previous exercise we looked at this in a bit more detail by actually directly applying values to the right message attributes and using the RQT Message Publisher to publish these for us. As I'm sure you can appreciate though, there's a limit to what we can achieve by working in this way though(circular and straight line motion is about it!)</p> <p>In reality, robots need to be able to move around complex environments autonomously, which is quite a difficult task, and requires us to build bespoke applications. We can build these applications using Python, and we'll look at the core concepts behind this in the following exercises, starting by building a simple Node that will allow us to make our robot a bit more \"autonomous\". What we will do here forms the basis of the more complex approaches used by robotics engineers to really bring robots to life!</p>"},{"location":"amr/lab1/#ex6","title":"Exercise 6: Creating a ROS Package","text":"<p>As we learnt earlier, all ROS nodes must be contained within packages, so in order for us to create our own node, we first need to create our own package.</p> <ol> <li> <p>In TERMINAL 3 run the following command to navigate to a folder called the \"ROS Workspace\" using the <code>cd</code> (\"change directory\") command:</p> <pre><code>cd ~/ros2_ws/src\n</code></pre> </li> <li> <p>Next run the following command to copy a package template from GitHub into the ROS Workspace folder:</p> <pre><code>git clone https://github.com/tom-howard/ros2_pkg_template.git\n</code></pre> </li> <li> <p>Now, run a script from within this template, to initialise the package for use:</p> <pre><code>./ros2_pkg_template/init_pkg.sh amr31001_lab1\n</code></pre> </li> <li> <p>We're going to open this package in a text editor called Visual Studio Code (aka \"VS Code\") now, so that we can start making changes to it:</p> <pre><code>code ./amr31001_lab1\n</code></pre> </li> <li> <p>When VS Code opens, you should see a File Explorer on the left-hand side which allows you to access all the files and folders within your package. </p> <p></p> <p></p> <p>Look for a file here called <code>package.xml</code> and click on it. This will open this file in the main VS Code window, to allow you to edit it.</p> </li> <li> <p>Look for the following lines in the <code>package.xml</code> file:</p> package.xml<pre><code>&lt;maintainer email=\"your.name.1@sheffield.ac.uk\"&gt;Name 1&lt;/maintainer&gt;\n&lt;maintainer email=\"your.name.2@sheffield.ac.uk\"&gt;Name 2&lt;/maintainer&gt;\n</code></pre> <p>Change <code>Name 1</code> to your name, and then change <code>your.name.1@sheffield.ac.uk</code> to your Sheffield email address! Then, do the same for your other Group member on the line below it. (If you're working in a group of more than 2 people, then you can add additional lines below this for your other group members.)</p> <p>Post-lab</p> <p>This is important for the post-lab!</p> <p>We'll be assessing your work here as part of the post-lab, so it's important that we can identify each member of your group. If any group members aren't listed here, then they won't receive any marks for this! </p> <p>When entering your names, make sure you provide first names AND surnames for each group member.</p> </li> <li> <p>Go back to TERMINAL 3 now and run the following three commands:</p> <ol> <li> <p>First: </p> <pre><code>cd ~/ros2_ws\n</code></pre> </li> <li> <p>Then:</p> <pre><code>colcon build --symlink-install --packages-select amr31001_lab1\n</code></pre> </li> <li> <p>And finally:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> </ol> <p>OK, package creation is now complete, so we're ready to start some Python programming...</p>"},{"location":"amr/lab1/#ex7","title":"Exercise 7: A Python node to make the robot move","text":"<p>Go back to VS Code now, and (in the File Explorer) look for a folder called <code>scripts</code>. Click on the  icon next to this to expand the folder and reveal its content. A file called <code>basic_velocity_control.py</code> should be revealed. Click on this to open it in the main editor window.</p> <p></p> <p>This is a (fairly) basic ROS 2 Python Node that will control the velocity of the robot. Let's talk through it:</p> <ol> <li> <p>First, we have some imports:</p> <pre><code>import rclpy # (1)!\nfrom geometry_msgs.msg import TwistStamped # (2)!\nimport time # (3)!\n</code></pre> <ol> <li><code>rclpy</code> is the ROS client library for Python. We need this so that our Python node can interact with ROS.</li> <li>We know from earlier that in order to make a robot move we need to publish messages to the <code>/cmd_vel</code> topic, and that this topic uses a data structure (or Interface) called <code>geometry_msgs/msg/TwistStamped</code>. This is how we import the interface into our Python node so that we can create velocity commands for our robot (which we'll get to shortly...)</li> <li>We'll use this to control timing in our node.</li> </ol> <p>Click on the  icons above to reveal more information about each line of the code.</p> </li> <li> <p>Next, we declare some variables that we can use and adapt during the main execution of our code:</p> <pre><code>state = 1 # (1)!\nvel = TwistStamped() # (2)!\n</code></pre> <ol> <li>Inside the <code>while</code> loop (explained shortly) we define two different operational states for the robot, and we can control which one is active by changing this value from <code>1</code> to <code>2</code> (and visa-versa).</li> <li> <p>We're instantiating a <code>TwistStamped</code> Interface message here and calling it <code>vel</code>. We'll assign velocity values to this in the <code>while</code> loop later on.</p> <p>Recall that a <code>TwistStamped</code> message contains six different components that we can assign values to. Which two are relevant to our robot?</p> </li> </ol> </li> <li> <p>Next we configure some important ROS-related things:</p> <pre><code>rclpy.init(args=None) # (1)!\nnode = rclpy.create_node(\"basic_velocity_control\")  # (2)!\nvel_pub = node.create_publisher(TwistStamped, \"cmd_vel\", 10)  # (3)!\n</code></pre> <ol> <li>Initialise <code>rclpy</code> and all the ROS communications that are necessary for our node. </li> <li>Initialise this Python script as an actual ROS node, providing a name for it to be registered on the ROS network with (\"basic_velocity_control\" in this case).</li> <li>Here we're setting up a publisher to the <code>/cmd_vel</code> topic so that the node can send velocity commands to the robot (using <code>TwistStamped</code> data).</li> </ol> </li> <li> <p>After this, we're defining another variable:</p> <pre><code>timestamp = node.get_clock().now().nanoseconds # (1)!\n</code></pre> <ol> <li>What time is it right now? This tells us the current \"ROS Time\" (in nanoseconds), which will be useful to compare against in the while loop.</li> </ol> </li> <li> <p>Now, we enter into a <code>while</code> loop, which is where our code will spend the majority of its time once it's running:</p> <pre><code>while rclpy.ok(): # (1)!\n    time_now = node.get_clock().now().nanoseconds # (2)!\n    elapsed_time = (time_now - timestamp) * 1e-9 # (3)!\n\n    ...\n</code></pre> <ol> <li>This returns <code>True</code> as long as the node is alive, so all the code inside the <code>while</code> loop will continue to execute as long as this is the case.</li> <li>What time is it now? Check the time at the start of each iteration of the <code>while</code> loop, and assign this to a variable called <code>time_now</code>.</li> <li>Determine how much time has elapsed (in seconds) since the <code>timestamp</code> was last updated.</li> </ol> <p>Everything that's indented below the <code>while rclpy.ok():</code> line will continue to be executed over and over again until we ask our node to stop. The code will execute line-by-line from top-to-bottom within this <code>while</code> loop, and will then go back to the top again and repeat it all over and over and over again! Each repeat is called an \"iteration\".</p> <ol> <li> <p>An <code>if</code> statement now controls the state of operation for our robot. </p> <ol> <li> <p>In state <code>1</code> we set velocities that will make the robot move forwards (linear X velocity only) for a certain amount of time and then stop. How long will the robot move forwards for, and at what velocity?</p> <pre><code>if state == 1: \n    if elapsed_time &lt; 2: # (1)!\n        vel.twist.linear.x = 0.05 # (2)!\n        vel.twist.angular.z = 0.0\n    else: # (3)!\n        vel.twist.linear.x = 0.0 # (4)!\n        vel.twist.angular.z = 0.0\n        state = 2 # (5)!\n        timestamp = node.get_clock().now().nanoseconds # (6)!\n</code></pre> <ol> <li>If the elapsed time is less than 2 seconds...</li> <li>Set a linear velocity so that the robot will move forwards.</li> <li>If the elapsed time has exceeded 2 seconds...</li> <li>Set our robot's velocities to <code>0.0</code> to make it stop.</li> <li>In the next loop iteration, go into state 2 instead.</li> <li>Reset the timestamp to start counting up again. </li> </ol> </li> <li> <p>In state <code>2</code> we set velocities that will make the robot turn on the spot (angular Z velocity only) for a certain amount of time and then stop. How long will it do this for, and at what velocity?</p> <pre><code>elif state == 2:\n    if elapsed_time &lt; 4: # (1)!\n        vel.twist.linear.x = 0.0\n        vel.twist.angular.z = 0.2 # (2)!\n    else: # (3)!\n        vel.twist.linear.x = 0.0 # (4)!\n        vel.twist.angular.z = 0.0 \n        state = 1 # (5)!\n        timestamp = node.get_clock().now().nanoseconds # (6)!\n</code></pre> <ol> <li>While the elapsed time is less than 4 seconds...</li> <li>Apply an angular velocity to the robot to make it turn on the spot.</li> <li>Once the elapsed time has exceeded 4 seconds...</li> <li>Set the robot's velocities back to <code>0.0</code> to make it stop.</li> <li>In the next loop iteration, go back into state 1 again (moving forwards).</li> <li>Reset the timestamp to start counting up once more. </li> </ol> </li> </ol> </li> <li> <p>And after the <code>if</code> statement:</p> <pre><code>node.get_logger().info( # (1)!\n    f\"\\n[State = {state}] Publishing velocities:\\n\"\n    f\"  - linear.x: {vel.twist.linear.x:.2f} [m/s]\\n\"\n    f\"  - angular.z: {vel.twist.angular.z:.2f} [rad/s].\",\n    throttle_duration_sec=1,\n)\nvel_pub.publish(vel) # (2)!\n</code></pre> <ol> <li>This (and the following 5 lines) will print a message to the terminal, to provide us with regular updates on what state the node is currently in and what velocities have been set (in the <code>if</code> statement above).</li> <li> <p>This line is crucial: this operation actual publishes the velocity commands to the <code>/cmd_vel</code> topic, to make the robot actual act on our instructions.</p> <p>Regardless of what happens in the <code>if</code> states above, we always publish a velocity command to the <code>/cmd_vel</code> topic here (every loop iteration).</p> </li> </ol> </li> </ol> </li> <li> <p>Go back to TERMINAL 3 now, run the code and see what happens. Make sure the robot is on the floor and has enough room to roam around before you do this!</p> <p>TERMINAL 3: </p><pre><code>ros2 run amr31001_lab1 basic_velocity_control.py\n</code></pre><p></p> <p>Enter Ctrl+C in TERMINAL 3 to stop the node from running once you've seen enough.</p> <p>Warning</p> <p>The robot will continue to move even after you've stopped the node! Run the following command to stop it:</p> <pre><code>ros2 run amr31001_lab1 stop_me.py\n</code></pre> </li> <li> <p>Your Task:</p> <p>The aim here is to make the robot follow a square motion path of dimensions 0.5m x 0.5m. As it is though, the <code>basic_velocity_control.py</code> node doesn't actually do this yet, and you need to fix it!</p> <p>Edit the code so that the robot actually follows a 0.5m x 0.5m square motion path!</p> <p>Post-lab</p> <p>As discussed above, your completion of this exercise will be assessed as part of the post-lab!</p> </li> </ol>"},{"location":"amr/lab1/#ex8","title":"Exercise 8 (Advanced): Alternative Motion Paths","text":"<p>If you have time, have a go at this now...</p> <p>How could you adapt the code further to achieve some more interesting motion profiles?</p> <ol> <li> <p>First, go back to TERMINAL 3 and make sure you're in the right file system location:</p> <pre><code>cd ~/ros2_ws/src/amr31001_lab1/scripts\n</code></pre> </li> <li> <p>Then, make a copy of the <code>basic_velocity_control.py</code> code using the <code>cp</code> command (copy):</p> <p></p><pre><code>cp basic_velocity_control.py alt_velocity_control.py\n</code></pre> Which will create a copy called <code>alt_velocity_control.py</code><p></p> </li> <li> <p>Use the following command to open up a text file in VS Code:</p> <pre><code>code ../CMakeLists.txt\n</code></pre> </li> <li> <p>In this file, locate the lines (near the bottom of the file) that read:</p> <pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/basic_velocity_control.py\n  scripts/stop_me.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> <p>Insert a new line below the one that reads <code>scripts/basic_velocity_control.py</code>, so that it now looks like this:</p> <pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/basic_velocity_control.py\n  scripts/alt_velocity_control.py\n  scripts/stop_me.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> <p>You've just added <code>alt_velocity_control.py</code> as a new node within your package. </p> <p>Save the file and close it.</p> </li> <li> <p>Go back to TERMINAL 3 and run the following 3 commands again, in order:</p> <ol> <li> <p>First: </p> <pre><code>cd ~/ros2_ws\n</code></pre> </li> <li> <p>Then:</p> <pre><code>colcon build --symlink-install --packages-select amr31001_lab1\n</code></pre> </li> <li> <p>And finally:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Go back to VS Code and find your new <code>alt_velocity_control.py</code> file. Click on it to open it in the editor.</p> </li> <li> <p>NOW see if you can edit this to achieve either of the more complex motion profiles illustrated below.</p> <p></p> <p></p> <ol> <li>Profile (a): The robot needs to follow a figure-of-eight shaped path, where a linear and angular velocity command are set simultaneously to generate circular motion. Velocities will need to be defined in order to achieve a path diameter of 1m for each of the two loops. Having set the velocities appropriately, you'll then need to work out how long it would take the robot to complete each loop, so that you can determine when the robot should have got back to its starting point. At this point you'll need to change the turn direction, so that the robot switches from anti-clockwise to clockwise turning. </li> <li>Profile (b): The robot needs to start and end in the same position, but move through intermediate points 1-7, in sequence, to generate the stacked square profile as shown. Each of the two squares must be 1m x 1m in size, so you'll need to find the right velocity and duration pairs for moving forward and turning. You'll also need to change the turn direction once the robot reaches Point 3, and then again at Point 7!</li> </ol> </li> <li> <p>To run the file and test it out, you'll need to use <code>ros2 run ...</code>. How would you format this command (recall this)?<sup>1</sup></p> <p>Whenever you need to stop the node, enter Ctrl+C in the terminal.</p> <p>Remember</p> <p>The robot will continue to move even after you've stopped the node! Run the following command to stop it whenever you need to:</p> <pre><code>ros2 run amr31001_lab1 stop_me.py\n</code></pre> </li> </ol>"},{"location":"amr/lab1/#wrapping-up","title":"Wrapping Up","text":"<p>Before you leave, please shut down your robot! Enter the following command in TERMINAL 3 to do so:</p> <p>TERMINAL 3: </p><pre><code>waffle X off\n</code></pre> ... again, replacing <code>X</code> with the number of the robot that you have been working with today.<p></p> <p>You'll need to enter <code>y</code> and then hit Enter to confirm this.</p> <p>Please then shut down the laptop, which you can do by clicking the battery icon in the top right of the desktop, clicking the Power icon (illustrated below) and then selecting \"Power Off...\" in the menu.</p> <p></p> <p></p><p></p> <p>AMR31001 Lab 1 Complete! See you in the new year for Lab 2!</p> <p></p><p></p> <ol> <li> <p><code>ros2 run amr31001_lab1 alt_velocity_control.py</code> \u21a9</p> </li> </ol>"},{"location":"amr/lab2/","title":"Lab 2: Feedback Control","text":""},{"location":"amr/lab2/#introduction","title":"Introduction","text":"<p>In Lab 1 we explored how ROS works and how to bring a robot to life. Let's quickly recap the key points:</p> <p>Nodes</p> <ul> <li>Are executable programs (Python, C++ scripts) that perform specific robot tasks and operations.</li> <li>Typically, there'll be many Nodes running on a robot simultaneously in order to make it work.</li> <li>We can create our own Nodes on top of what's already running, to add extra functionality.</li> <li>You may recall that we created our own Node (in Python) to make our TurtleBot3 Waffle follow a square motion path.</li> </ul> <p></p> <p>Topics and Message interfaces</p> <ul> <li>All the ROS Nodes running on a network can communicate and pass data between one another using a Publisher/Subscriber-based Communication Principle.</li> <li>ROS Topics are key to this - they are essentially the communication channels (or the plumbing) on which all data is passed around between the nodes.</li> <li>Different topics communicate different types of information using standardised data structures (called \"Message Interfaces\").</li> <li>Any Node can publish (write) and/or subscribe to (read) any ROS Topic in order to pass information around or make things happen.</li> </ul> <p></p> <p>One of the key ROS Topics that we worked with last time was <code>/cmd_vel</code>, which is a topic that communicates velocity commands to make a robot move. You may recall that to make our TurtleBot3 Waffle move, we publish <code>TwistStamped</code> Interface Messages to the <code>/cmd_vel</code> topic. Interfaces messages are structured data types defined in ROS, and we will remind ourselves about the structure of the <code>TwistStamped</code> data type shortly...</p> <p>Open-Loop Control</p> <p>We used a time-based method to control the motion of our robot in order to get it to generate a square motion path. This type of control is open-loop: we hoped that the robot had moved (or turned) by the amount that was required, but had no feedback to tell us whether this had actually been achieved.</p> <p>Closed-Loop Control</p> <p>In this lab we'll look at how this can be improved, making use of some of our robot's on-board sensors to tell us where the robot is or what it can see in its environment, in order to complete a task more reliably and be able to better adapt to changes and uncertainty in the environment.</p>"},{"location":"amr/lab2/#aims","title":"Aims","text":"<p>In this lab, we'll build some ROS Nodes (in Python) that incorporate data from some of our robot's sensors. This sensor data is published to specific topics on the ROS Network, and we can build ROS Nodes to subscribe to these. We'll see how the data from these sensors can be used as feedback to inform decision-making, thus allowing us to implement some different forms of closed-loop control, and make our robot more autonomous. </p>"},{"location":"amr/lab2/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Interpret the data from a ROS Robot's Odometry System and understand what this tells you about a Robot's position and orientation within its environment.</li> <li>Use feedback from a robot's odometry system to control its position in an environment.</li> <li>Use data from a Robot's LiDAR sensor to make a robot follow a wall.</li> <li>Analyse images from a Robot's Camera and use this information to follow a coloured line on the floor.</li> </ol>"},{"location":"amr/lab2/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Exploring Odometry Data</li> <li>Exercise 2: Odometry-based Navigation</li> <li>Exercise 3: Wall Following</li> <li>Exercise 4: Line Following</li> </ul>"},{"location":"amr/lab2/#the-lab","title":"The Lab","text":"<p>Assessment Info</p> <p>This lab is summatively assessed.</p> <ol> <li>There's a post-lab quiz that you'll need to complete after this lab session has taken place, which will be released on Blackboard.</li> <li>You'll also be marked on the work that you do in the lab for Exercises 3 &amp; 4.</li> </ol>"},{"location":"amr/lab2/#getting-started","title":"Getting Started","text":""},{"location":"amr/lab2/#creating-a-ros-package","title":"Creating a ROS Package","text":"<p>We'll need a ROS package to work with for this lab session. We've created a template for you, which contains all the resources that you'll need for today. Download and install this as follows.</p> <ol> <li> <p>Open up a terminal instance on the laptop, either by using the Ctrl+Alt+T keyboard shortcut, or by clicking the Terminal App icon in the favourites bar on the left-hand side of the desktop:</p> <p></p> <p></p> <p>We'll call this TERMINAL 1.</p> </li> <li> <p>In TERMINAL 1, run the following commands in order:</p> <p>Tip</p> <p>To paste the following commands into the terminal use Ctrl+Shift+V</p> <p>TERMINAL 1: </p><pre><code>cd ~/ros2_ws/src/\n</code></pre><p></p> <pre><code>git clone https://github.com/tom-howard/amr31001_lab2.git\n</code></pre> <pre><code>cd ~/ros2_ws &amp;&amp; \\\n  colcon build --symlink-install \\\n  --packages-select amr31001_lab2 &amp;&amp; \\\n  source ~/.bashrc \n</code></pre> </li> <li> <p>Next, open the package in VS Code:</p> <pre><code>code ./src/amr31001_lab2\n</code></pre> </li> <li> <p>When VS Code opens, navigate to the File Explorer: </p> <p></p> <p></p> <p>... find a file here called <code>package.xml</code> and click on it. </p> </li> <li> <p>Look for the following lines in the <code>package.xml</code> file:</p> package.xml<pre><code>&lt;maintainer email=\"name1@sheffield.ac.uk\"&gt;Name 1&lt;/maintainer&gt;\n&lt;maintainer email=\"name2@sheffield.ac.uk\"&gt;Name 2&lt;/maintainer&gt;\n</code></pre> <p>Change <code>Name 1</code> to your name, and then change <code>name1@sheffield.ac.uk</code> to your Sheffield email address! Then, do the same for your other Group member on the line below it. (If you're working in a group of more than 2 people, then you can add additional lines below this for your other group members.)</p> <p>Post-lab</p> <p>This is important for the post-lab!</p> <p>Some of the work that you do in this lab will be assed as part of the post-lab, so it's important that we can identify each member of your group. If any group members aren't listed here, then they won't receive any marks! </p> <p>When entering your names, make sure you provide first names AND surnames for each group member.</p> </li> <li> <p>Save the changes that you have just made to your <code>package.xml</code> file.</p> </li> </ol>"},{"location":"amr/lab2/#launching-ros","title":"Launching ROS","text":"<p>Much the same as last time, you'll now need to get ROS up and running on your robot. </p> <ol> <li> <p>First, identify the robot that you have been provided with. Each of our robots are uniquely named: <code>dia-waffleX</code>, where <code>X</code> is the 'Robot Number' (a number between 1 and 50). Check the label printed on top of the robot to find out which one you have!</p> </li> <li> <p>In TERMINAL 1 type the following command to pair the laptop and robot, so that they can work together:</p> <p>TERMINAL 1: </p><pre><code>waffle X pair\n</code></pre> ... replacing <code>X</code> with the number of the robot that you have been provided with.<p></p> </li> <li> <p>Enter the password for the robot when requested (we'll tell you what this is in the lab).</p> <p>You may see a message like this early on in the pairing process:</p> <p></p> <p></p> <p>If so, just type <code>yes</code> and then hit Enter to confirm that you want to continue.</p> </li> <li> <p>Once the pairing process is finished you should see a message saying <code>pairing complete</code>, displayed in blue in the terminal. </p> </li> <li> <p>Then, in the same terminal (TERMINAL 1), enter the following command:</p> <p>TERMINAL 1: </p><pre><code>waffle X term\n</code></pre> (again, replacing <code>X</code> with the number of your robot).<p></p> <p>Any text that was in the terminal should now disappear, and a green banner should appear across the bottom of the terminal window:</p> <p></p> <p></p> <p>This is a terminal instance running on the robot, and any commands that you enter here will be executed on the robot (not the laptop!)</p> </li> <li> <p>Now, launch ROS on the robot by entering the following command:</p> <p>TERMINAL 1: </p><pre><code>ros2 launch tuos_tb3_tools ros.launch.py\n</code></pre><p></p> <p>Tip</p> <p>To paste text into a Linux terminal you'll need to use the Control + Shift + V keyboard keys: Ctrl+Shift+V</p> <p>If all is well then the robot will play a nice \"do-re-me\" sound and a message like this should appear (amongst all the other text):</p> <pre><code>[tb3_status.py-#] ######################################\n[tb3_status.py-#] ### dia-waffleX is up and running! ###\n[tb3_status.py-#] ######################################\n</code></pre> <p>ROS is now up and running on the robot, and we're ready to go!</p> </li> <li> <p>Next, connect the laptop to the ROS network that we've just established on the robot. The Robot and Laptop communicate with one another via the University Wireless network, but there's one more step required to link them together. </p> <p>Open up a new terminal instance on the laptop (either by using the Ctrl+Alt+T keyboard shortcut, or by clicking the Terminal App icon) and enter the following command:</p> <p>TERMINAL 2: </p><pre><code>ros2 run rmw_zenoh_cpp rmw_zenohd\n</code></pre><p></p> <p>Leave TERMINAL 1 and TERMINAL 2 running in the background at all times while working with your robot in the lab today.</p> </li> </ol>"},{"location":"amr/lab2/#odometry","title":"Odometry","text":"<p>First, let's look at our robot's odometry system, and what this is useful for.</p> <p>Odometry is the use of data from motion sensors to estimate change in position over time. It is used in robotics by some legged or wheeled robots to estimate their position relative to a starting location. <sup>1</sup></p> <p>Our robot can therefore keep track of its position (and orientation) as it moves around. It does this using data from two sources:</p> <ol> <li>Wheel encoders: Our robot has two wheels, each is equipped with an encoder that measures the number of rotations that the wheel makes. </li> <li>An Inertial Measurement Unit (IMU): Using accelerometers, gyroscopes and compasses, the IMU can monitor the linear and angular velocity of the robot, and which direction it is heading, at all times.</li> </ol> <p>This data is published to a ROS Topic called <code>/odom</code>. </p>"},{"location":"amr/lab2/#ex1","title":"Exercise 1: Exploring Odometry Data","text":"<p>In the previous lab we used some ROS commands to identify and interrogate active topics on the ROS network, let's give that another go now, but on the <code>/odom</code> topic this time.</p> <ol> <li> <p>Open up a new terminal instance on the laptop (by pressing Ctrl+Alt+T, or clicking the Terminal App desktop icon, as you did before). We\u2019ll call this one TERMINAL 3.</p> </li> <li> <p>As you may recall from last time, we can use the <code>ros2 topic</code> command to list all the topics that are currently active on the network. Enter the following in TERMINAL 3:</p> <p>TERMINAL 3: </p><pre><code>ros2 topic list\n</code></pre><p></p> <p>A large list of items should appear on the screen. Can you spot the <code>/odom</code> topic?</p> </li> <li> <p>Let's find out more about this using the <code>ros2 topic info</code> command.</p> <p>TERMINAL 3: </p><pre><code>ros2 topic info /odom\n</code></pre><p></p> <p>This should provide the following output:</p> <pre><code>Type: nav_msgs/msg/Odometry\nPublisher count: 1\nSubscription count: 0\n</code></pre> <p>Post-lab Quiz</p> <p>What does all this mean? We discussed this last time (in relation to the <code>/cmd_vel</code> topic), and you may want to have a look back at this to refresh your memory! </p> <p>Based on the above, we know that the <code>/odom</code> topic uses a <code>nav_msgs/msg/Odometry</code> data structure (or \"interface\"). </p> <p>Interfaces (revisited)</p> <p>Recall from Lab 1 that data structures in ROS 2 are called Interfaces. </p> <p>From the output above, <code>Type</code> refers to the type of data structure (i.e. the type of interface). The <code>Type</code> definition always has three parts to it, in this case: <code>nav_msgs</code>, <code>msg</code> and <code>Odometry</code>:</p> <ol> <li><code>nav_msgs</code> is the name of the ROS package that this interface (data structure) belongs to</li> <li><code>msg</code> tells us that it's a topic message interface (rather than another interface type, of which there are others, but we don't need to worry about them here)</li> <li><code>Odometry</code> is the name of the message interface.</li> </ol> </li> <li> <p>We can use the <code>ros2 interface</code> command to find out more about this:</p> <p>TERMINAL 3: </p><pre><code>ros2 interface show nav_msgs/msg/Odometry\n</code></pre><p></p> <p>You'll see a lot of information there, but try to find the line that reads <code>Pose pose</code>: </p> <pre><code>Pose pose\n        Point position\n                float64 x\n                float64 y\n                float64 z\n        Quaternion orientation\n                float64 x 0\n                float64 y 0\n                float64 z 0\n                float64 w 1\n</code></pre> <p>Here's where we'll find information about the robot's position and orientation (aka \"Pose\") in the environment. Let's have a look at this data in real time...</p> </li> <li> <p>We can look at the live data being streamed across the <code>/odom</code> topic, using the <code>ros2 topic echo</code> command. We know that the data type is called <code>nav_msgs/msg/Odometry</code>, and nested within this is the <code>pose</code> attribute that we are interested in, so:</p> <p>TERMINAL 3: </p><pre><code>ros2 topic echo /odom --field pose.pose\n</code></pre><p></p> </li> <li> <p>What we're presented with now is live Odometry data from the robot.</p> <p>Let's drive the robot around a bit and observe how our robot's pose changes as we do so.</p> </li> <li> <p>Open up a new terminal instance by pressing Ctrl+Alt+T, or clicking the Terminal App desktop icon, as you did before. We'll call this one TERMINAL 4:</p> <p>TERMINAL 4: </p><pre><code>ros2 run turtlebot3_teleop teleop_keyboard\n</code></pre><p></p> </li> <li> <p>Follow the instructions provided in the terminal to drive the robot around:</p> Reminder <p></p> <p></p> <p>As you're doing this, look at how the <code>position</code> and <code>orientation</code> data is changing in TERMINAL 3, in real-time!</p> <p>Post-lab Quiz</p> <p>Which position and orientation values change (by a significant amount) when:</p> <ol> <li>The robot turns on the spot (i.e. only an angular velocity is applied)?</li> <li>The robot moves forwards (i.e. only a linear velocity is applied)?</li> <li>The robot moves in a circle (i.e. both a linear and angular velocity are applied simultaneously)?</li> </ol> <p>Make a note of the answers to these questions, as they may feature in the post-lab quiz!</p> </li> <li> <p>When you've seen enough enter Ctrl+C in TERMINAL 4 to stop the <code>teleop_keyboard</code> node. Then, enter Ctrl+C in TERMINAL 3 as well, which will stop the live stream of Odometery messages from being displayed.</p> </li> </ol>"},{"location":"amr/lab2/#summary","title":"Summary","text":"<p>Pose is a combination of a robot's position and orientation in its environment.</p> <p>Position tells us the location (in meters) of the robot in its environment. Wherever the robot was when it was turned on is the reference point, and so the distance values that we observed in the exercise above were all quoted relative to this initial position.</p> <p>You should have noticed that (as the robot moved around) the <code>x</code> and <code>y</code> terms changed, but the <code>z</code> term should have remained at zero. This is because the <code>X-Y</code> plane is the floor, and any change in <code>z</code> position would mean that the robot was floating or flying above the floor! </p> <p>Orientation tells us where the robot is pointing in its environment, expressed in units of Quaternions; a four-term orientation system. You should have noticed some of these values changing too, but it may not have been immediately obvious what the values really meant! For the further exercises in this lab we'll convert this to Euler angles (in degrees/radians) for you, to make the data a bit easier to understand.</p> <p>Ultimately though, our robots position can change in both the <code>X</code> and <code>Y</code> axes (i.e. the plane of the floor), while its orientation can only change about the <code>Z</code> axis (i.e. it can only \"yaw\"): </p> <p></p>"},{"location":"amr/lab2/#ex2","title":"Exercise 2: Odometry-based Navigation","text":"<p>Now that we know about the odometry system and what it tells us, let's see how this could be used as a feedback signal to inform robot navigation. You may recall that last time you created a ROS Node to make your robot to follow a square motion path on the floor. This was time-based though: for a given speed of motion (turning or moving forwards) how long would it take for the robot to move by a required distance? Having determined this, we then used timers to control the execution of two different motion states: moving forwards and turning on the spot, in order to generate the square motion path (approximately). </p> <p>In theory though, we can do all this much more effectively with odometry data instead, so let's have a go at that now...</p> <ol> <li> <p>Head back to VS Code, which should still be open from earlier.</p> </li> <li> <p>In the File Explorer on the left-hand side find a folder called <code>scripts</code>, and click on the <code>ex2.py</code> file in here, to display it in the editor.</p> </li> <li> <p>Have a look through the code and see if you can work out what's going on. There are a few things to be aware of:</p> <ol> <li> <p>Motion control is handled by an external Python class called <code>Motion</code>, which is imported on line 7 (along with another class called <code>Pose</code> which we'll talk about shortly):</p> <pre><code>from amr31001_lab2_modules.tb3_tools import Motion, Pose\n</code></pre> <p>The <code>Motion</code> class is instantiated on line 15:</p> <pre><code>self.motion = Motion(self) # (1)!\n</code></pre> <ol> <li> <p>Most of the code in the <code>ex2.py</code> file is contained within a Python class called <code>Square</code>. See line 10:</p> <pre><code>class Square(Node):\n    ...\n</code></pre> <p><code>self</code> allows our class to refer to itself!</p> <p><code>self.motion</code> for example allows us to access the <code>motion</code> attribute elsewhere within the class (as long as we refer to it as <code>self.motion</code>). </p> <p>See this in action below...</p> </li> </ol> <p>A class method called <code>move_square()</code> contains the main part of the code, and it's here that we call the <code>motion</code> attribute to make the robot move, e.g.:</p> <ol> <li> <p>To make the robot move at a linear velocity of <code>x</code> (m/s) and/or an angular velocity of <code>y</code> (rad/s):</p> <pre><code>self.motion.move_at_velocity(\n    linear = x, angular = y\n)\n</code></pre> </li> <li> <p>To make the robot stop moving:</p> <pre><code>self.motion.stop()\n</code></pre> </li> </ol> </li> <li> <p>Obtaining our robot's Odometry data was discussed in the previous exercise, where we learnt that can be done by subscribing to the <code>/odom</code> topic, which provides us with this data in a <code>nav_msgs/msg/Odometry</code>-type format. This is quite a complex data structure, so to make life easier during this lab, we've done all the hard work for you, inside another class called <code>Pose</code> (also imported earlier). This class is instantiated on line 16:</p> <pre><code>self.pose = Pose(self)\n</code></pre> <p>We can then use this to access the robot's odometry data, by calling the appropriate attribute whenever we need it:</p> <ol> <li><code>self.pose.posx</code> to obtain the robot's current position (in meters) in the <code>X</code> axis.</li> <li><code>self.pose.posy</code> to obtain the robot's current position (in meters) in the <code>Y</code> axis.</li> <li><code>self.pose.yaw</code> to obtain the robot's current orientation (in degrees) about the <code>Z</code> axis.</li> </ol> </li> </ol> </li> <li> <p>Run the code in TERMINAL 3 and observe what happens:</p> <p>TERMINAL 3: </p><pre><code>ros2 run amr31001_lab2 ex2.py\n</code></pre><p></p> <p>The robot should start turning on the spot, and you should see some interesting information being printed to the terminal. After it has turned by 45\u00b0 the robot should stop. </p> </li> <li> <p>Stop the Node by entering Ctrl+C in TERMINAL 3 and then run it again if you missed what happened the first time!</p> </li> <li> <p>What you need to do:</p> <ol> <li> <p>The <code>move_square()</code> class method is called over and over again at a controlled rate. This was established in the <code>__init__()</code> class method: </p> <pre><code>self.create_timer(\n    timer_period_sec=0.05, # (1)!\n    callback=self.move_square, # (2)!\n)\n</code></pre> <ol> <li>The rate at which to execute a \"callback function\". Note: defined in terms of period (in seconds), not frequency (in Hz).</li> <li>The callback function to execute at the specified rate, i.e. <code>move_square()</code>.</li> </ol> </li> <li> <p>The <code>move_square()</code> class method is therefore essentially the main part of our code: a series of operations that will be called over and over again at a specified rate.</p> <p>It's this part of the code that you will need to modify!</p> <p>Within this there is an <code>if</code> statement that controls whether the robot should be turning or moving forwards: </p> <pre><code>if self.turn:\n    # Turning State\n    ...\nelse:\n    # Moving Forwards \n    ...\n</code></pre> <p>... where <code>self.turn</code> is a boolean whose value can either be <code>True</code> or <code>False</code>.</p> </li> <li> <p>Within this, look at what happens in the <code>Turning State</code>. Consider how the robot's yaw angle is being monitored and updated as the robot turns. Then, look at how the turn angle is being controlled. See if you can adapt this to make sure the robot turns by 90\u00b0.</p> </li> <li> <p>Ultimately, after the robot has turned by the desired angle it needs to move forwards by 0.5m, in order to achieve a 0.5x0.5m square motion path.</p> <p>Moving forwards is handled in the <code>Moving Forwards</code> state.</p> <p>See if you can adapt the code within this block to make the robot move forwards by the required amount (0.5 meters) in between each turn. </p> Hint <p>Consider how the turn angle is monitored and updated whist turning (<code>current_yaw</code>), and take a similar approach with the linear displacement (<code>current_distance</code>). Bear in mind that you'll need to consider the euclidean distance, which you'll need to calculate based on the robot's position in both the <code>x</code> and <code>y</code> axis.</p> <p></p> <p></p> </li> <li> <p>Make sure that you've saved any changes to the code (in VS Code) before trying to test it out on the robot!</p> <p>Do this by using the Ctrl+S keyboard shortcut, or going to <code>File</code> &gt; <code>Save</code> from the menu at the top of the screen.</p> </li> <li> <p>Once you've saved it, you can re-run the code at any time by using the same <code>ros2 run</code> command as before:</p> <p>TERMINAL 3: </p><pre><code>ros2 run amr31001_lab2 ex2.py\n</code></pre><p></p> <p>... and you can stop it at any time by entering Ctrl+C in the terminal.</p> Python Tips <p>You'll need to do a bit of maths here (see the \"Hint\" above). Here's how to implement a couple of mathematical functions in Python:</p> <ol> <li> <p>To the power of X: </p> <p>Use <code>**</code> to raise a number to the power of another number (i.e. \\(2^{3}\\)):</p> <pre><code>&gt;&gt;&gt; 2**3\n8\n</code></pre> <p>Or, use the <code>pow()</code> method:</p> <pre><code>&gt;&gt;&gt; pow(2, 3)\n8\n</code></pre> </li> <li> <p>Square Root: </p> <p>To calculate the square root of a number (i.e. \\(\\sqrt{4}\\)):</p> <pre><code>&gt;&gt;&gt; sqrt(4)\n2.0 \n</code></pre> </li> </ol> </li> </ol> </li> </ol>"},{"location":"amr/lab2/#the-lidar-sensor","title":"The LiDAR Sensor","text":"<p>As you'll know, the black spinning device on the top of your robot is a LiDAR Sensor. As discussed previously, this sensor uses laser pulses to measure the distance to nearby objects. The sensor spins continuously so that it can fire these laser pulses through a full 360\u00b0 arc, and generate a full 2-dimensional map of the robot's surroundings.</p> <p>This data is published onto the ROS network to a topic called <code>/scan</code>. Use the same methods that you used in Exercise 1 to find out what data type (\"interface\") this topic uses.</p> <p>Post-lab Quiz</p> <p>Make a note of this, there'll be a post-lab quiz question on it!</p> <p>Launch RViz, so that we can see the data coming from this sensor in real-time:</p> <p>TERMINAL 3: </p><pre><code>ros2 launch tuos_tb3_tools rviz.launch.py environment:=real\n</code></pre><p></p> <p></p> <p>The green dots illustrate the LiDAR data. Hold your hand out to the robot and see if you can see it being detected by the sensor... a cluster of green dots should form on the screen to indicate where your hand is located in relation to the robot. Move your hand around and watch the cluster of dots move accordingly. Move your hand closer and farther away from the robot and observe how the dots also move towards or away from the robot on the screen. </p> <p>This data is really useful and (as we observed during the previous lab session) it allows us to build up 2-dimensional maps of an environment with considerable accuracy. This is, of course, a very valuable skill for a robot to have if we want it to be able to navigate autonomously, and we'll explore this further later on. For now though, we'll look at how we can use the LiDAR data ourselves to build Nodes that make the robot detect and follow walls!</p> <p>Once you're done, close down RViz by hitting Ctrl+C in TERMINAL 3. </p>"},{"location":"amr/lab2/#ex3","title":"Exercise 3: Wall Following","text":"<ol> <li> <p>In VS Code, click on the <code>ex3.py</code> file in the File Explorer to display it in the editor.</p> </li> <li> <p>Have a look through the code and see if you can work out what's going on. Here's a few points to start with:</p> <ol> <li> <p>Velocity control is handled in the same way as in the previous exercise:</p> <ol> <li> <p>To make the robot move at a linear velocity of <code>x</code> (m/s) and/or an angular velocity of <code>y</code> (rad/s):</p> <pre><code>self.motion.move_at_velocity(\n    linear = x, angular = y\n)\n</code></pre> </li> <li> <p>To make the robot stop moving:</p> <pre><code>self.motion.stop()\n</code></pre> </li> </ol> </li> <li> <p>The data from the LiDAR sensor has been preprocessed and encapsulated in a separate class (much like <code>Pose</code> in the previous exercise). This one is called <code>Lidar</code>, which is instantiated on line 15:</p> <pre><code>self.lidar = Lidar(self)\n</code></pre> <p>This class splits up data from the LiDAR sensor into a number of different segments to focus on a number of distinct zones around the robot's body (to make the data a bit easier to deal with). For each of the segments (as shown in the figure below) a single distance value can be obtained, which represents the average distance to any object(s) within that particular angular zone:</p> <p></p> <p></p> <p>In the code, we can obtain the distance measurement (in meters) from each of the above zones as follows:</p> <ol> <li><code>self.lidar.distance.front</code> to obtain the average distance to any object(s) in front of the robot (within the frontal zone).</li> <li><code>self.lidar.distance.l1</code> to obtain the average distance to any object(s) located within LiDAR zone L1.</li> <li><code>self.lidar.distance.r1</code> to obtain the average distance to any object(s) located within LiDAR zone R1.     and so on...</li> </ol> </li> <li> <p>The code template has been developed to detect a wall on the robot's left-hand side.</p> <ol> <li>We use distance measurements from LiDAR zones L3 and L4 to determine the alignment of the robot to a left-hand wall.</li> <li> <p>This is determined by calculating the difference between the distance measurements reported from these two zones:</p> <pre><code>wall_slope = self.lidar.distance.l3 - self.lidar.distance.l4\n</code></pre> </li> <li> <p>This is a spatial difference between LiDAR beams <code>l3</code> and <code>l4</code>, and can be used as a simple measure of local wall slope or relative offset between the robot and the wall.</p> <p>If this value is close to zero, then the robot and the wall are well aligned. If not, then the robot is at an angle to the wall, and it needs to adjust its angular velocity in order to correct for this:</p> <p></p> <p></p> </li> </ol> </li> </ol> </li> <li> <p>Run the node as it is, from TERMINAL 3:</p> <p>TERMINAL 3: </p><pre><code>ros2 run amr31001_lab2 ex3.py\n</code></pre><p></p> <p>When you do this, you'll notice that the robot doesn't move at all (yet!), but the following data appears in the terminal:</p> <ol> <li>The distance measurements from each of the LiDAR zones.</li> <li>The current value of the <code>wall_slope</code> parameter, i.e. how well aligned the robot currently is to a wall on its left-hand side.</li> <li>The decision that has been made by the <code>if</code> statement on the appropriate action that should be taken, given the current value of <code>wall_slope</code>.</li> </ol> </li> <li> <p>Now look at the code. The \"main\" part of the code is once again controlled by a timer.</p> <p>Post-lab Quiz</p> <ul> <li>What is the name of the main control method in <code>ex3.py</code>?</li> <li>At what rate (in Hz) will this control method be executed?</li> </ul> </li> <li> <p>Adapting the code:</p> <ol> <li>First, modify the <code>wall_slope</code> calculation so that the robot observes a wall on its right-hand side NOT its left. </li> <li>Next, place the robot on the floor with a wall on its right-hand side</li> <li> <p>Manually vary the alignment of the robot and the wall and observe how the information that is being printed to the terminal changes as you do so.</p> <p>Question</p> <p>The node will tell you if it thinks the robot needs to turn right or left in order to improve its current alignment with the wall. Is it making the correct decision?</p> </li> <li> <p>Currently, all velocity parameters inside the <code>follow_wall()</code> method are set to zero.</p> <ul> <li> <p>You'll need to set a constant linear velocity, so that the robot is always moving forwards. Set an appropriate value for this now, by editing the line that currently reads:</p> <pre><code>lin_vel = 0.0\n</code></pre> </li> <li> <p>The angular velocity of the robot will need to be adjusted conditionally, in order to ensure that the value of <code>wall_slope</code> is kept as low as possible at all times (i.e. the robot is kept in alignment with the wall). </p> </li> </ul> <p>Adjust the value of <code>ang_vel</code> in each of the <code>if</code> statement blocks so that this is achieved under each of the three possible scenarios.</p> </li> <li> <p>Hopefully, by following the steps above, you will get to the point where you can make the robot follow a right-hand wall reasonably well, as long as the wall remains reasonably straight! Consider what would happen however if the robot were faced with either of the following situations:</p> <p></p> <p></p> <p>You may have already observed this during your testing... how could you adapt the code so that such situations can be achieved?</p> Hints <ol> <li>You may need to consider the distance measurements from some other LiDAR zones!</li> <li> <p>The <code>ex3.py</code> template that was provided to you uses an <code>if</code> statement with three different cases:</p> <pre><code>if ...:\n\nelif ...:\n\nelse:\n</code></pre> <p>You may need to add in some further cases to this to accommodate the additional situations discussed above, e.g.:</p> <pre><code>if ...:\n\nelif ...:\n\nelif ...:\n\nelif ...:\n\nelse:\n</code></pre> </li> </ol> </li> </ol> <p>Post-lab</p> <p>As discussed above, your completion of this exercise will be assessed as part of the post-lab!</p> </li> </ol>"},{"location":"amr/lab2/#cameras-and-robot-vision","title":"Cameras and Robot Vision","text":"<p>Our robot's have cameras, providing them with the ability to \"see\" their environment. Camera data can be used as yet another feedback signal to inform closed-loop control, which we will leverage now to implement line following. We will achieve this using a well established control algorithm known as PID Control, using the data from our robot's camera and applying some image processing techniques to this to detect and locate a coloured line printed on the floor.</p> <p>Consider the following image obtained from a robot's camera, with a green line visible on the floor: </p> <p></p> <p>PID Control is a clever algorithm that aims to minimise the Error between a Reference Input: a desired condition that we would like our robot to maintain; and a Feedback Signal: the condition that the robot is currently in (based on real-world data). The PID algorithm calculates an appropriate Controlled Output for our system that (when tuned appropriately) will act to minimise this error.   </p> <p>If we want our robot to successfully follow a coloured line on the floor, we will need it to keep that line in the centre of its view point at all times by minimising the error between where the line currently is (the feedback signal) and where it should be (the reference input, i.e. the centre of its view point). In this case then, the PID algorithm provides us with an angular velocity command (the controlled output) to achieve this.</p> <p>The full PID algorithm is as follows:</p> \\[ u(t)=K_{P} e(t) + K_{I}\\int e(t)dt + K_{D}\\dfrac{d}{dt}e(t) \\] <p>Where \\(u(t)\\) is the Controlled Output, \\(e(t)\\) is the Error (as illustrated in the figure above) and \\(K_{P}\\), \\(K_{I}\\) and \\(K_{D}\\) are Proportional, Integral and Differential Gains respectively, which each have different effects on a system in terms of its ability to maintain the desired state (the reference input). We must establish appropriate values for these gains by a process called tuning.</p> <p>In fact, to allow our TurtleBot3 to follow a line, we actually only really need a proportional gain, so our control algorithm can be simplified considerably:</p> \\[ u(t)=K_{P} e(t) \\] <p>This is what's referred to as a \"P\" Controller, and the only gain we need to establish here is therefore \\(K_{P}\\).</p>"},{"location":"amr/lab2/#ex4","title":"Exercise 4: Line Following","text":""},{"location":"amr/lab2/#ex4a","title":"Part A: Establishing a Feedback Signal (Detecting the Line)","text":"<ol> <li> <p>Launch RViz again in TERMINAL 3:</p> <p>TERMINAL 3: </p><pre><code>ros2 launch tuos_tb3_tools rviz.launch.py environment:=real\n</code></pre><p></p> </li> <li> <p>In the \"Displays\" menu on the left-hand side, tick the box next to the \"Camera\" item. Live images from the robot's camera should then be displayed in the bottom left-hand corner of the RViz window.</p> </li> <li> <p>Place the robot in the arena so that line on the arena floor is visible in the robot's vision.</p> </li> <li> <p>Now, launch the <code>ex4_colour_detection.py</code> node in TERMINAL 4:</p> <p>TERMINAL 4: </p><pre><code>ros2 run amr31001_lab2 ex4_colour_detection.py\n</code></pre><p></p> <p>After a brief pause, a window should open displaying a scatter plot alongside a raw image obtained from the robot's camera.</p> <p></p> <p></p> <p>The scatter plot shows all the different colours that are present in the raw camera image. These are plotted in terms of the Hue and Saturation values of each pixel in the image.</p> </li> <li> <p>In the plot, you should be able to identify a cluster of data points that are the same colour as the line on the floor. From the plot, make a note of the range of Hue and Saturation values that these dots reside within.</p> <p></p> <p></p> </li> <li> <p>Once you've done this, close the figure by clicking the  icon in the top right corner, and also close the RViz window too. This should release TERMINAL 3 and TERMINAL 4.</p> </li> </ol>"},{"location":"amr/lab2/#ex4b","title":"Part B: Implementing Proportional Control (Following the Line)","text":"<ol> <li> <p>In VS Code, click on the <code>ex4_line_following.py</code> file in the File Explorer to display it in the editor.</p> </li> <li> <p>Have a look through the code and see if you can work out what's going on. Here's a few points to start with:</p> <ol> <li> <p>Once again, velocity control is handled in the same way as the previous exercises, using the <code>Motion()</code> class and calls to <code>self.motion.move_at_velocity()</code> and <code>self.motion.stop()</code> commands.</p> </li> <li> <p>The data from the robot's camera is once again handled by a separate class (much like <code>Pose</code> and <code>Lidar</code> in the previous exercises). This one (instantiated on line 15) is called <code>Camera</code>:</p> <pre><code>self.camera = Camera(self)\n</code></pre> <p>We'll look at how to use this shortly...</p> </li> </ol> </li> <li> <p>The \"main\" part of the code is once again controlled by a timer and a \"callback\" function. </p> <p>Post-lab Quiz</p> <ul> <li>What is the name of the callback function (aka the \"main\" control method)?</li> <li>At what rate (in Hz) will this run at?</li> </ul> </li> <li> <p>Run the node as it is, from TERMINAL 3:</p> <p>TERMINAL 3: </p><pre><code>ros2 run amr31001_lab2 ex4_line_following.py\n</code></pre><p></p> <p>To begin with, the robot shouldn't do anything, but a window should open showing a live feed from the robot's camera. You should also see a small blue circle hovering around somewhere in the image too.</p> <p>Make sure the line on the floor is visible to the robot before proceeding any further.</p> </li> <li> <p>Stop the node with Ctrl+C.</p> </li> <li> <p>In VS Code, locate the line in the <code>ex4_line_following.py</code> file that reads:</p> <pre><code>self.camera.colour_filter()\n</code></pre> <p>Into this, you can provide the Hue and Saturation ranges that you identified in Part A:</p> <pre><code>self.camera.colour_filter(\n    hue=[MIN, MAX],\n    saturation=[MIN, MAX]\n)\n</code></pre> <p>Replace <code>MIN</code> and <code>MAX</code> with your upper and lower hue and saturation values.</p> </li> <li> <p>Run the code again. If your Hue and Saturation ranges are correct, and the line is in view then it should now be isolated in the image (all other pixels in the camera stream should be black).</p> <p>The small blue circle should also now be located roughly in the middle of the line. If you move the robot now (whilst keeping the line in view) then the blue circle should move with the line, indicating that the line is successfully being detected by your filtering, and the image processing algorithms.</p> <p></p> <p></p> <p>If the line isn't successfully isolated then go back to Part A and run the <code>ex4_colour_detection.py</code> node again. </p> </li> <li> <p>The robot is now able to locate the position of the line in its viewpoint, so we have now successfully established the Feedback Signal for our proportional controller. In the code, this can be accessed as follows:</p> <pre><code>self.camera.line_position_pixels\n</code></pre> <p>Consider the figure from above, once again:</p> <p></p> <p></p> <p>In our code, we can now use this to calculate the robot's current positional error. Locate the lines that read:</p> <pre><code>reference_input = self.camera.image_width / 2\nerror = 0.0 # TODO\n</code></pre> <p>... and edit the <code>error = ...</code> line to correctly calculate the robot's positional error based on the real-time position of the line in its view point (<code>self.camera.line_position_pixels</code>).</p> </li> <li> <p>Angular Velocity is the Controlled Output of our P Controller, calculated (once again) according to:</p> \\[ u(t)=K_{P} e(t) \\] <p>This is reflected in the code by the line that reads:</p> <pre><code>ang_vel = kp * error\n</code></pre> <p>Modify the <code>self.motion.move_at_velcity()</code> line in the code to apply this angular velocity to the robot along with a constant (and moderate) linear velocity too. </p> </li> <li> <p>Finally, tune the P Controller by identifying an appropriate proportional gain <code>kp</code> so that the robot successfully follows the line smoothly and consistently.</p> <p>Post-lab</p> <p>As discussed above, your completion of this exercise will be assessed as part of the post-lab!</p> </li> </ol>"},{"location":"amr/lab2/#wrapping-up","title":"Wrapping Up","text":"<p>Before you leave, please shut everything down properly:</p> <ol> <li>Enter Ctrl+C in any terminals that are still active.</li> <li> <p>Turn off your robot by entering the following command in TERMINAL 1:</p> <p>TERMINAL 1: </p><pre><code>waffle NUM off\n</code></pre> ... replacing <code>NUM</code> with the number of the robot that you have been working with today.<p></p> <p>You'll need to enter <code>y</code> and then hit Enter to confirm this.</p> </li> <li> <p>Please then shut down the laptop, which you can do by clicking the battery icon in the top right of the desktop and selecting the \"Power Off / Log Out\" option in the drop-down menu.</p> </li> </ol> <p></p> <p></p><p></p> <p>AMR31001 Lab 2 Complete! </p> <p></p><p></p> <ol> <li> <p>https://en.wikipedia.org/wiki/Odometry\u00a0\u21a9</p> </li> </ol>"},{"location":"com/","title":"COM2009 Course Assignments","text":""},{"location":"com/#com2009-course-assignments","title":"COM2009 Course Assignments","text":"<p>For the COM2009 Robotics course you must complete two lab assignments:</p> <ul> <li> <p>Assignment #1: \"An Introduction to ROS (the Robot Operating System)\".</p> <p>Here you will learn what ROS is and how to use it. You will complete this assignment individually, and in your own time.</p> <p>Weighting: 25% of the overall COM2009 module mark.</p> </li> <li> <p>Assignment #2: \"Team Robotics Project\".</p> <p>Here you will work in teams of 3-4 to complete a series of real-world robotics tasks using our Tutlebot3 Waffle Robots in the Lab (Diamond Computer Room 5).</p> <p>Weighting: 30% of the overall COM2009 module mark</p> </li> </ul>"},{"location":"com/assignment1/","title":"The ROS Course","text":""},{"location":"com/assignment1/#overview","title":"Overview","text":"<p>For Assignment #1 you will complete the 6-part ROS 2 course in full and in order. The course is designed to be completed in simulation, so you will therefore need access to a ROS 2 installation which can either be installed on your own machine, or accessed on a range of managed desktop computers across the University of Sheffield campus. See here for more information on how to access or install ROS 2.</p> <p>Each part of the course comprises a series of step-by-step instructions and exercises to teach you how ROS works, and introduces you to the core principles of the framework. The exercises give you the opportunity to see how to apply these principles to practical robotic applications. Completing this course is essential for obtaining all the necessary skills for Assignment #2: the Team Robotics Project, where you will work in teams to program our real TurtleBot3 Waffle robots.  </p>"},{"location":"com/assignment1/#the-course","title":"The Course","text":"<ul> <li> <p>Part 1: Getting Started with ROS 2</p> <p>Learn the basics of ROS 2 and become familiar with some key tools and principles, allowing you to program robots and work with ROS 2 applications effectively.</p> </li> <li> <p>Part 2: Odometry &amp; Navigation</p> <p>Learn about Odometry, which informs us of a robot's position and orientation in an environment. Apply both open and closed-loop velocity control methods to a Waffle.</p> </li> <li> <p>Part 3: Beyond the Basics</p> <p>Execute ROS applications more efficiently using launch files, and learn how to affect the behaviour of nodes during run-time using parameters. Learn about the LiDAR sensor, the data that it generates, and see the benefits of this for tools like \"SLAM\".</p> </li> <li> <p>Part 4: Services</p> <p>Learn about an alternative way that ROS nodes can communicate across a ROS network, and the situations where this might be useful.</p> </li> <li> <p>Part 5: Actions</p> <p>Learn about another key ROS communication method which is similar to a ROS Service, but with a few key benefits and alternative use-cases.</p> </li> <li> <p>Part 6: Cameras, Machine Vision &amp; OpenCV</p> <p>Learn how to work with images from a robot's camera. Learn techniques to detect features within these images, and use this to inform robot decision-making.</p> </li> </ul>"},{"location":"com/assignment1/#assessment","title":"Assessment","text":"<p>This assignment is worth 25% of the overall mark for COM2009, and is assessed via an on-campus Blackboard-based test taking place in week 7 or 8 of the Spring Semester (see Blackboard and/or your timetable for the exact date &amp; time). </p>"},{"location":"com/assignment2/","title":"Assignment #2: Team Robotics Project","text":""},{"location":"com/assignment2/#overview","title":"Overview","text":"<p>In Assignment #2 you will put into practice everything that you are learning about ROS 2 in Assignment #1, and explore the capabilities of the framework further.</p> <p>You will attend a 2-hour lab session per week in Diamond Computer Room 5 for the full 12-week semester. Here, you will work in teams to develop ROS Nodes for our real TurtleBot3 Waffles, allowing them to successfully complete a number of robotics tasks in a real-world environment. </p> <p>Assignment #2 is split into two parts: Part A and Part B. You will complete Part A in the first half of the semester (Weeks 1-6) and then move on to Part B in the second half of the semester (Weeks 7-12). </p>"},{"location":"com/assignment2/#the-tasks","title":"The Tasks","text":"Part Tasks Marks(/100) Submission A Task 1: Velocity ControlTask 2: Avoiding Obstacles 2020 Friday of Week 6 at 6pm (GMT) B Tasks 3 &amp; 4More details coming soon... 60 Friday of Week 12 at 6pm (BST) <p>As shown above, there are four tasks in total that you must complete for Assignment #2, worth a total of 100 marks overall. Exact submission deadlines will be stated on Blackboard.</p> <p>Tasks 1, 2 &amp; 3 are programming tasks, requiring you to develop ROS nodes for the Waffles in order for them to complete certain real-world objectives in the robot arena in Computer Room 5. </p>"},{"location":"com/assignment2/#assessment","title":"Assessment","text":"<p>This assignment is worth 30% of the overall mark for COM2009. As a team you will be assessed on a ROS package that you develop to satisfy the above tasks.</p> <p>Your ROS package will be assessed by the teaching team in the weeks following the submission deadlines (as above). You will receive your marks, plus video recordings of the assessment within 3 weeks of submission<sup>1</sup>.</p>"},{"location":"com/assignment2/#tasks-1-2-3","title":"Tasks 1, 2 &amp; 3","text":"<p>Each submission will be assessed by deploying your ROS package on one of the robotics laptops used extensively throughout the lab sessions. Nodes within your package will then be executed on the laptop to control a real robot in the Diamond Computer Room 5 Robot Arena.</p>"},{"location":"com/assignment2/#task-4","title":"Task 4","text":"<p>Details coming soon...</p>"},{"location":"com/assignment2/#submissions","title":"Submissions","text":"<p>Before you get started on any of the tasks, as a team you'll need to create a single ROS package and host this on GitHub (which you'll do in the Week 1 Lab Session). You can then add all the necessary functionality for each task as you go along. On each of the submission deadlines (as summarised above and detailed on Blackboard) we will pull your team's work from your GitHub repo. See here for further details.</p> <p>Note</p> <p>You should work on each task as a team, and create only one ROS package/GitHub repo per team for this assignment.</p>"},{"location":"com/assignment2/#your-ros-package","title":"Your ROS Package","text":""},{"location":"com/assignment2/#launching-your-code","title":"Launching Your Code","text":"<p>In order to launch the necessary functionality within your package for a given task you will need to include correctly named launch files: <code>task1.launch.py</code>, <code>task2.launch.py</code>, etc. This will allow you to ensure that all the required functionality is executed when your submission is assessed, and also ensures that we know exactly how to launch this functionality in order to assess it. Full details of the requirements for each launch file are provided on the associated task page.</p> <p>For more information on how to create <code>.launch.py</code> files, refer to the following resources:</p> <ol> <li>Assignment #1 Part 3</li> <li>Launch Files (Advanced) </li> </ol>"},{"location":"com/assignment2/#preparing-for-the-deadlines","title":"Preparing for the Deadlines","text":"<p>You can find all the Key Information regarding assessment of the programming tasks on this page. It's extremely important that you follow all the Key Requirements outlined here regarding the structure, content and configuration of your ROS package, so please be sure to read this page in full at your earliest convenience!</p> <ol> <li> <p>If this falls over a holiday period (i.e. Easter), then the deadline for marking and the return of marks extends to 5 weeks (as per standard University policy).\u00a0\u21a9</p> </li> </ol>"},{"location":"com/assignment2/assessment/","title":"Key Assessment Info & Requirements","text":"<p>Warning</p> <p>Failure to follow all the requirements listed on this page could result in penalties being applied to your mark, or zero marks being awarded for a submission point and/or assignment task!</p> <p>Your ROS package should be hosted on GitHub, it should be setup as a private repository, and you should have added <code>tom-howard</code> and <code>AlexandrLucas</code> as collaborators. </p> <p>In addition to this, you should have registered your package with the teaching team (via the Google Form), so that we know where to find it on the submission deadlines.</p> <p>All the above was covered in the Getting Started section, which you should have completed in the Week 1 Lab.</p> <p>Having completed all of this successfully, we'll be able to pull your package on each of the submission dates so that your team's Assignment #2 work can be assessed. If you haven't completed all this, then you could receive zero marks!</p> <p>Hello </p> <p>At some point within the first few weeks of the course a <code>hello.md</code> file will be pushed to your repo (by Tom) to confirm that it has been registered correctly.</p>"},{"location":"com/assignment2/assessment/#submission-points","title":"Submission Points","text":"<p>As discussed here, there are two submission points for Assignment #2 and four tasks to complete overall: </p> <p></p><p></p> Part Tasks Marks(/100) Submission A Tasks 1 &amp; 2 40 Friday of Week 6 at 6pm (GMT) B Tasks 3 &amp; 4 60 Friday of Week 12 at 6pm (BST) <p></p><p></p> <p>See the task pages for full details on each of the four tasks.</p>"},{"location":"com/assignment2/assessment/#dependencies","title":"Dependencies","text":"<p>You may draw upon any pre-existing Python libraries or ROS 2 packages in your own work for Assignment #2 as long as they are pre-installed on the real robotics hardware (i.e. the Linux laptops in the lab). The WSL-ROS2 environment is equivalent to the software setup on the real robotics hardware, so any packages that exist in one will also exist in the other.</p> <p>Note</p> <p>You will not be able to request for any additional libraries/packages to be installed.</p>"},{"location":"com/assignment2/assessment/#key-requirements","title":"Key Requirements","text":"<p>In addition to registering your package correctly (as above), you must also ensure that the following Key Requirements are met for each of the submission points (A and B): </p> <ul> <li> <p> The name of your ROS package must be:</p> <pre><code>com2009_teamXX_2026\n</code></pre> <p>... where <code>XX</code> should be replaced with your team number.</p> </li> <li> <p> It must be possible to build your package by running the following command from the root of the local ROS 2 Workspace, and this must build without errors:</p> <pre><code>colcon build --packages-select com2009_teamXX_2026\n</code></pre> </li> <li> <p> You must ensure that a launch file exists for each of the programming tasks (Tasks 1, 2 &amp; 3) and that these are callable (after having run the above <code>colcon build</code> command) so that we are able to launch your work using <code>ros2 launch</code> as follows<sup>1</sup>:</p> <pre><code>ros2 launch com2009_teamXX_2026 taskY.launch.py\n</code></pre> <p>... where <code>XX</code> will be replaced by your team number, and <code>Y</code> will be replaced by the appropriate task number.</p> <p>Important</p> <p>You must ensure that your launch files are named correctly (as detailed in each of the task pages). We won't use any other method to launch your ROS nodes during the assessment of each programming task. </p> </li> <li> <p> Any nodes within your package that are executed by the above launch files must have been correctly defined as package executables (i.e. in your <code>CMakeLists.txt</code>) and must also have been assigned the appropriate execute permission (i.e. with <code>chmod</code>).  </p> <p>Important</p> <p>It's up to you to ensure that your code launches as intended for a given task. If it doesn't, then you'll be awarded zero marks, so make sure you test it all out prior to submission!</p> <p></p> </li> <li> <p> Your package must contain no build files (<code>build/</code>, <code>install/</code>, <code>log/</code>) that would be generated as a result of (incorrectly) running <code>colcon build</code> from inside your package.</p> <p>Remember</p> <p>Always run <code>colcon build</code> from the root of the ROS workspace (e.g. <code>~/ros2_ws/</code>), to ensure that all build files are generated in the right location in the filesystem (<code>~/ros2_ws/build/</code>, <code>~/ros2_ws/install/</code>, <code>~/ros2_ws/log/</code>).</p> </li> <li> <p> On each of the deadlines, we will pull your work from the <code>main</code> branch of your package repository. We will ONLY assess work on your <code>main</code> branch!</p> </li> <li> <p> Your package's <code>package.xml</code> file must contain a <code>&lt;maintainer&gt;</code> tag for each member of your team. Add these as necessary, e.g.:</p> <p></p><pre><code>&lt;maintainer email=\"member.1@sheffield.ac.uk\"&gt;Member 1's Name&lt;/maintainer&gt;\n&lt;maintainer email=\"member.2@sheffield.ac.uk\"&gt;Member 2's Name&lt;/maintainer&gt;\n...\n</code></pre> (providing each team member's full name and Sheffield email address.)<p></p> </li> </ul> <p>For the assessment of each Assignment #2 Task, your package will be built and deployed on one of the Robotics Laptops that you'll have been working with extensively during the lab sessions. We will use the standard <code>student</code> user account, and your package will be downloaded to the <code>~/ros2_ws/src/</code> directory. </p>"},{"location":"com/assignment2/assessment/#other-important-information","title":"Other Important Information","text":"<ul> <li> <p>The <code>tuos_ros</code> Course Repo will be installed and up-to-date on the Robotics Laptop that we use to assess your work with.</p> </li> <li> <p>The Robotics Laptop that we use for the assessment will be selected at random.</p> </li> <li> <p>This laptop will have been paired with a robot prior to us attempting to run your submission.</p> </li> <li> <p>The robot will also be selected at random.</p> </li> <li> <p>We will have already launched the bringup on the robot, so ROS will be up and running, and the robot will be ready to go in the arena.</p> </li> <li> <p>A Zenoh Session will already be running on the laptop to connect to the Robot's Zenoh Router, and communications will have been tested prior to us attempting to launch your work for each task.</p> </li> </ul> <ol> <li> <p>Make sure you have defined an appropriate <code>install</code> directory in your package's <code>CMakeLists.txt</code> \u21a9</p> </li> </ol>"},{"location":"com/assignment2/getting-started/","title":"Week 1: Getting Started","text":"<p>Before you get started on Assignment #2 (as detailed in the pages that follow), you should work through the following tasks in your teams during the first lab in Week 1. </p> <ul> <li> Set Up Your Team's ROS Package</li> <li> Get to Know the Robots</li> </ul> <p>The instructions below will guide you through these key first steps.</p>"},{"location":"com/assignment2/getting-started/#set-up-your-teams-ros-package","title":"Set Up Your Team's ROS Package","text":"<p>As discussed on the Assignment #2 Overview, everything that your team submit for this lab assignment must be contained within a single ROS package. Inside this you will develop all the necessary nodes to make a TurtleBot3 Waffle complete each of the assignment tasks, as well as some documentation to describe your approach. Each task will be assessed by the Teaching Team via launch files that you must also provide within your package. </p> <p>The first step however is to create your team's ROS Package.</p> <p>Your team's package will need to be hosted on GitHub, so each team member will need a GitHub account. Head to GitHub and set up an account if you don't already have one<sup>1</sup>.</p> <p>Working with Git and GitHub</p> <p>You'll be working with Git and GitHub quite extensively throughout Assignment #2. Hopefully a lot of you will already be quite familiar with these tools, but if not, we would strongly recommend that you have a look at This Course by the University of Sheffield's Research Software Engineering (RSE) Team.</p>"},{"location":"com/assignment2/getting-started/#creating-your-teams-package-repo-on-github","title":"Creating Your Team's Package Repo (on GitHub)","text":"<p>Nominate only one member of your team to do this bit.</p> <ol> <li>Ensure that you are signed in to your account on GitHub, then go to the <code>ros2_pkg_template</code> Repo. </li> <li> <p>Click on the green <code>Use this template</code> button, and then select <code>Create a new repository</code> from the dropdown menu. </p> <p></p> <p></p> <p>You should then be presented with a Create a new repository screen.</p> </li> <li> <p>The name for your repository must be as follows:</p> <pre><code>com2009_teamXX_2026\n</code></pre> <p>... where <code>XX</code> should be replaced with your COM2009 Assignment #2 Team Number. Enter this in the <code>Repository name</code> box.</p> <p>If your team number is less than 10: put a zero before the number, so that the team number is always 2 digits long, e.g.: </p> <ul> <li><code>com2009_team03_2026</code> for Team 3</li> <li><code>com2009_team08_2026</code> for Team 8</li> <li><code>com2009_team15_2026</code> for Team 15</li> </ul> <p>Important</p> <p>Your repository name should match the above format exactly:</p> <ul> <li>The name should be 19 characters long in total.</li> <li>All characters should be lower case (e.g. <code>com2009</code>, NOT <code>COM2009</code>)</li> </ul> </li> <li> <p>Under <code>Configuration</code>, select <code>Private</code> to make the repository private: </p> <p></p> <p></p> <p>Then click the green <code>Create repository</code> button.</p> </li> <li> <p>You'll then be directed to your main repository page. From here, click on <code>Settings</code>, then under <code>Access</code> click <code>Collaborators</code>:</p> <p></p> <p></p> <p>(You may be prompted for 2FA.)</p> </li> <li> <p>In the <code>Manage access</code> area, click the green <code>Add people</code> button and add <code>tom-howard</code> and <code>AlexandrLucas</code>: </p> <p></p> <p></p> </li> <li> <p>Finally, click on the <code>Add people</code> button and add the rest of your team members as collaborators to this repo too.</p> </li> </ol>"},{"location":"com/assignment2/getting-started/#pkg-reg","title":"Register Your ROS Package URL with the Teaching Team","text":"<p>Having created your package, you'll need to tell us your GitHub username and the URL to your team's GitHub repository, so that we can access it and pull download your work when the submissions are due.</p> <p>There is a form that you must complete (as a team), to register your ROS package with us for Assignment #2. The form is available via the link below (also on Blackboard). </p> <p></p>COM2009 Package Registration Form (2026)<p></p> <p>You must be signed in to your university email account (<code>...@sheffield.ac.uk</code>) to access this. </p> <p>The team member who created the Repo (in the step above) should fill in this form now.</p> <p>Warning</p> <p>Failure to do this (and do it properly) could result in you receiving 0 marks for the assignment tasks!</p> <p>At some point within the first few weeks of the course a <code>hello.md</code> file will be pushed to your repo (by Tom) to confirm that it has been registered correctly.</p>"},{"location":"com/assignment2/getting-started/#initialising-your-teams-ros-package-locally","title":"Initialising Your Team's ROS Package (Locally)","text":"<p>Nominate only one member of your team to do this bit too.</p> <p>You should do this from within your own ROS installation (or WSL-ROS2), rather than on the robotics laptop that you will use to work with the real robots in the lab. Select a team member who has access to their own ROS installation in the lab now (i.e. via a personal laptop), or access WSL-ROS2 using one of the \"WSL-ROS laptops\" that are also available in the lab.</p> <ol> <li> <p>On GitHub, go back to your repository's main page by clicking the <code>&lt;&gt; Code</code> tab at the top-left.</p> </li> <li> <p>Click the green <code>Code</code> button and then, from the dropdown menu, click the  button to copy the remote HTTPS URL of your repo. </p> <p></p> <p></p> </li> <li> <p>From your local ROS installation, open a terminal instance and navigate to the <code>src</code> directory of the ROS Workspace:</p> <pre><code>cd ~/ros2_ws/src\n</code></pre> </li> <li> <p>Clone your repo here using the remote HTTPS URL:</p> <pre><code>git clone REMOTE_HTTPS_URL\n</code></pre> <p>You'll then be asked to enter your GitHub username, followed by a password. This password is not your GitHub account password!  </p> <p>Warning</p> <p>Your GitHub account password won't work here! You'll need to generate a personal access token and use this instead!</p> </li> <li> <p>Navigate into the package directory using the <code>cd</code> command:</p> <pre><code>cd com2009_teamXX_2026\n</code></pre> <p>(...replacing <code>XX</code> with your COM2009 Assignment #2 Team Number.)</p> </li> <li> <p>Then, run an initialisation script to configure your ROS package appropriately:</p> <pre><code>./init_pkg.sh\n</code></pre> </li> </ol>"},{"location":"com/assignment2/getting-started/#git","title":"Configure Git","text":"<p>Next, you'll need to make sure Git is configured properly in your local ROS installation before you do anything else.</p> <ol> <li> <p>From the same terminal instance as above run the following commands to update your personal details in the global Git config file on your machine:</p> <p></p><pre><code>git config user.name \"your_name\"\n</code></pre> ...replacing <code>your_name</code> with your actual name! E.g.: <code>git config --global user.name \"John Smith\"</code><p></p> <p></p><pre><code>git config user.email \"your_email_address\"\n</code></pre> ...replacing <code>your_email_address</code> with your actual email address!<p></p> </li> <li> <p>If you're working in WSL-ROS2 on a University machine, don't forget to run <code>wsl_ros backup</code> to save these changes to your external WSL-ROS2 backup file, so that they will always be restored whenever you run <code>wsl_ros restore</code> in a fresh WSL-ROS2 instance on another machine. </p> <p>Note</p> <p>All team members will actually need to do this bit before interacting with Git!</p> <p>Regardless of which team member is setting up your team's ROS package to begin with, you'll all need to interact with Git for this assignment, and you should therefore each set up your own individual Git configurations (via the steps above) before working individually on your team's ROS package.</p> </li> </ol>"},{"location":"com/assignment2/getting-started/#git-push","title":"Push Your Local ROS Package Back to GitHub","text":"<p>Again, only one member of your team needs to do this bit.</p> <p>Having initialised your team's ROS package, it's now ready for you to start populating with code for the Assignment #2 Tasks! The first step though is to push the changes made in the initialisation step (above) back to GitHub, so that everyone in your team is working from the right starting point. </p> <ol> <li> <p>From the same terminal as above, use the <code>git status</code> command to show you all the changes that have been made to the repo in the initialisation process:</p> <pre><code>git status\n</code></pre> </li> <li> <p>Use <code>git add</code> to stage all these changes for an initial commit:</p> <pre><code>git add .\n</code></pre> <p>Warning</p> <p>Don't forget the <code>.</code> at the end there!</p> </li> <li> <p>Then commit them:</p> <pre><code>git commit -m \"Package initialisations complete.\"\n</code></pre> </li> <li> <p>Finally, push the local changes back up the \"remote\" repository on GitHub:</p> <pre><code>git push origin main\n</code></pre> <p>You'll then be asked to enter your GitHub username and password again. </p> <p>Remember</p> <p>This is not your GitHub account password... Use the personal access token that you created earlier.  </p> </li> <li> <p>All team members should then be able to pull the remote repo into their own ROS Workspaces (<code>cd ~/ros2_ws/src/ &amp;&amp; git clone REMOTE_HTTPS_URL</code>), make contributions and push these back to the remote repo as required (using their own GitHub account credentials and personal access tokens).</p> </li> </ol> <p>You'll need to copy your ROS package onto the Robot Laptops when working on the Real-Robot based tasks, which we'll cover in more detail later. </p>"},{"location":"com/assignment2/getting-started/#getting-to-know-the-real-robots","title":"Getting to Know the Real Robots","text":"<p>Assignment #2 involves extensive work with our real robots, and you'll therefore have access to the robots for every lab session so that you can work on these tasks as you wish. All the details on how the robots work, how to get them up and running and start programming them can be found in the \"Waffles\" section of this course site. You should proceed now as follows (in your teams):</p> <ol> <li>Each team has been assigned a specific robot (there's a list on Blackboard). When you're ready, speak to a member of the teaching team who will provide you with the robot that has been assigned to you.</li> <li> <p>Work through each page of the \"Waffles\" section of this site (in order):</p> <ul> <li> Read about the hardware.</li> <li> Learn how to launch ROS and get the robots up and running.</li> <li> Work through the Waffle (&amp; ROS) Basics, which will help to get you started and understand how ROS and the robots work.</li> <li> There is also some further Essential Information that you must all be aware of when working with the real robots. Work through the further exercises here now.</li> <li> Finally, review the Shutdown Procedures. Follow these steps to shut down the robot and power off the robotics laptop at the end of each lab session.</li> </ul> </li> </ol> <ol> <li> <p>As a University of Sheffield student, you can apply for the GitHub Student Developer Pack, which gives you access to a range of developer tools including GitHub Pro. GitHub Pro allows you to have unlimited collaborators on your repositories, which might help you to collaborate on your ROS package with your team.\u00a0\u21a9</p> </li> </ol>"},{"location":"com/assignment2/ros-pkg-tips/","title":"Working with your ROS Package (in the Lab)","text":"<p>Having followed the instructions on the Getting Started page (in Week 1), your team's ROS package will be hosted on GitHub, which makes it much easier to collaborate and transfer your work to the real hardware during the lab sessions<sup>1</sup>.</p> <p>You'll need to transfer your ROS package to a robot laptop whenever you want to work on a real robot during the labs. </p>"},{"location":"com/assignment2/ros-pkg-tips/#getting-started","title":"Getting Started in Each Lab Session","text":"<p>Your team should be provided with the same laptop for every lab session. Having completed all the steps to set up SSH Keys (as described in the sections below), you should be able to return to the laptop, re-clone your package and continue working with relative ease at the start of each and every lab session.</p> <ol> <li> <p>Check if you already have an SSH Key: You'll save a private SSH Key (private to you and the rest of your team) on the laptop that has been designated to you for each lab session. The first step is to check whether this exists on your laptop:</p> <pre><code>ls -al ~/.ssh | grep com2009_teamXX_2026\n</code></pre> <p>Replace <code>XX</code> with your team number.</p> <p>If your team's ssh key is presented then you're good to go, continue to Step 2. If not, go to the Setting up SSH Keys section below.</p> </li> <li> <p>If your team's SSH Key exists: start the laptop's ssh-agent and activate your key:</p> <pre><code>eval \"$(ssh-agent -s)\"\n</code></pre> <pre><code>ssh-add ~/.ssh/com2009_teamXX_2026\n</code></pre> <p>Replacing <code>XX</code> with your team number.</p> </li> <li> <p>Then, clone your package onto the laptop.</p> <p>You'll be asked for your secret passphrase, hopefully you remember it!</p> <p>Warning</p> <p>We strongly recommend that you delete your team's package from the laptop at the end of each lab session.</p> </li> </ol>"},{"location":"com/assignment2/ros-pkg-tips/#setting-up-ssh-keys","title":"Setting Up SSH Keys","text":"<p>Using SSH keys, you can clone your team's ROS package to the robot laptops, make commits and push these back up to GitHub during the labs, without needing to provide your GitHub username and a personal access token every time. This makes life a lot easier! The following steps describe the process you should follow to achieve this<sup>2</sup>.</p>"},{"location":"com/assignment2/ros-pkg-tips/#ssh-keygen","title":"Step 1: Generating an SSH key (on the Laptop)","text":"<ol> <li> <p>From a terminal instance on the laptop navigate to the <code>~/.ssh</code> folder:</p> <pre><code>cd ~/.ssh\n</code></pre> </li> <li> <p>Create a new SSH key on the laptop, using your GitHub email address:</p> <pre><code>ssh-keygen -t ed25519 -C \"your.email@sheffield.ac.uk\"\n</code></pre> <p>Replacing <code>your.email@sheffield.ac.uk</code> with your GitHub email address.</p> <p></p> </li> <li> <p>You'll then be asked to <code>Enter a file in which to save the key</code>. This needs to be unique, so enter the name of your ROS package, e.g.: <code>com2009_teamXX_2026</code> (where <code>XX</code> is replaced with your team number).</p> </li> <li> <p>You'll then be asked to <code>enter a passphrase</code>. This is how you make your SSH key secure, so that no other teams using the same laptop can access and make changes to your team's package/GitHub repo. You'll be asked to enter this whenever you try to commit/push new changes to your ROS package on GitHub. Decide on a passphrase and share this ONLY with your fellow team members. </p> </li> <li> <p>Next, start the laptop's ssh-agent:</p> <pre><code>eval \"$(ssh-agent -s)\"\n</code></pre> </li> <li> <p>Add your SSH private key to the laptop's ssh-agent. You'll need to enter the name of the SSH key file that you created in the earlier step (e.g.: <code>com2009_teamXX_2026</code>)</p> <pre><code>ssh-add ~/.ssh/com2009_teamXX_2026\n</code></pre> <p>Replacing <code>XX</code> with your team number of course!</p> </li> <li> <p>Then, you'll need to add the SSH key to your account on GitHub...</p> </li> </ol>"},{"location":"com/assignment2/ros-pkg-tips/#step-2-adding-an-ssh-key-to-your-github-account","title":"Step 2: Adding an SSH key to your GitHub account","text":"<p>These instructions are replicated from this GitHub Docs page.</p> <ol> <li> <p>On the laptop, copy the SSH public key that you created in the previous steps to your clipboard.</p> <p>Do this from a terminal on the laptop, using <code>cat</code>:</p> <pre><code>cat ~/.ssh/com2009_teamXX_2026.pub\n</code></pre> <p>Replacing <code>XX</code> with your team number once again.</p> <p>The content of the file will then be displayed in the terminal... copy it from here.</p> <p>Tips</p> <ol> <li>To copy text from inside a terminal window use Ctrl+Shift+C</li> <li> <p>You could also open the file in VS Code and copy it from there:</p> <pre><code>code ~/.ssh/com2009_teamXX_2026.pub\n</code></pre> </li> </ol> </li> <li> <p>Go to your GitHub account in a web browser. In the upper-right corner of any page, click your profile photo, then click Settings.</p> </li> <li> <p>In the \"Access\" section of the sidebar, click SSH and GPG keys.</p> </li> <li> <p>Click New SSH key.</p> </li> <li> <p>Enter a descriptive name for the key in the \"Title\" field, e.g. <code>com2009_dia_laptop1</code>.</p> </li> <li> <p>Select <code>Authentication Key</code> as the \"Key Type.\"</p> </li> <li> <p>Paste the text from your SSH Public Key file into the \"Key\" field.</p> </li> <li> <p>Finally, click the \"Add SSH Key\" button.</p> </li> </ol>"},{"location":"com/assignment2/ros-pkg-tips/#ssh-clone","title":"Cloning your ROS package onto the Laptop","text":"<p>With your SSH keys all set up, you'll be able to clone your ROS package onto the laptop. </p> <p>There's a ROS 2 Workspace on each of the robot laptops and (much the same as in your own local ROS environment) your package must reside within this workspace!</p> <ol> <li> <p>From a terminal on the laptop, navigate to the ROS 2 Workspace <code>src</code> directory:</p> <pre><code>cd ~/ros2_ws/src/\n</code></pre> </li> <li> <p>Go to your ROS package on GitHub. Click the Code button and then select the SSH option to reveal the SSH address of your repo. Copy this. </p> </li> <li> <p>Head back to the terminal instance on the laptop to then clone your package into the <code>ros2_ws/src/</code> directory using <code>git</code>:</p> <pre><code>git clone REMOTE_SSH_ADDRESS\n</code></pre> <p>Where <code>REMOTE_SSH_ADDRESS</code> is the SSH address that you have just copied from GitHub.</p> </li> <li> <p>Run Colcon to build your package, which is a three-step process:</p> <ol> <li> <p>Navigate into the root of the ROS Workspace:</p> <pre><code>cd ~/ros2_ws\n</code></pre> </li> <li> <p>Run the <code>colcon build</code> command, targetting your package only:</p> <p></p><pre><code>colcon build --packages-select com2009_teamXX_2026 --symlink-install\n</code></pre> (Again, replacing <code>XX</code> with your team number.)<p></p> </li> <li> <p>Then, re-source your environment:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Navigate into your package: </p> <pre><code>cd ~/ros2_ws/src/com2009_teamXX_2026/\n</code></pre> <p>... and then run the following commands to set your identity (which will allow you to make commits to your package repo):</p> <p></p><pre><code>git config user.name \"your name\"\n</code></pre> <pre><code>git config user.email \"your email address\"\n</code></pre><p></p> </li> </ol> <p>You should then be able to commit and push any updates that you make to your ROS package while working on the laptop, back to your remote repository using the secret passphrase that you defined earlier!</p>"},{"location":"com/assignment2/ros-pkg-tips/#deleting-your-ros-package-after-a-lab-session","title":"Deleting your ROS package after a lab session","text":"<p>Remember that the Robotics Laptops use an account that everyone in the class has access to. You might therefore want to delete your package from the laptop at the end of each lab session. It's very easy to clone it back onto the laptop again by following the steps above at the start of each lab session. Deleting your package (by following the instructions below) won't delete your SSH key from the laptop though, so you won't need to do all that again, and your SSH key will still be protected with the secret passphrase that you set up when generating the SSH Key to begin with (assuming that you are working on the same laptop, of course!) </p> <p>Warning</p> <p>Make sure you've pushed any changes to GitHub before deleting your package!</p> <p>Delete your package by simply running the following command from any terminal on the laptop:</p> <pre><code>rm -rf ~/ros2_ws/src/com2009_teamXX_2026\n</code></pre> <p>... replacing <code>XX</code> with your own team's number!</p> <ol> <li> <p>Remember: If you're not already familiar with how to use tools like Git and GitHub, we would strongly recommend that you have a look at This Course by the University of Sheffield's Research Software Engineering (RSE) Team.\u00a0\u21a9</p> </li> <li> <p>Adapted from GitHub Docs \u21a9</p> </li> </ol>"},{"location":"com/assignment2/part-a/","title":"Assignment #2 Part A","text":""},{"location":"com/assignment2/part-a/#assignment-2-part-a","title":"Assignment #2 Part A","text":"<p>Total Marks: 40/100</p> <p>Submission: Friday of Week 6 at 6pm (GMT) </p> <p>Tasks:</p> <ul> <li>Task 1: Velocity Control</li> <li>Task 2: Avoiding Obstacles</li> </ul> <p>Ensure that you have prepared appropriately for the Part A submission by following all the instructions here.</p>"},{"location":"com/assignment2/part-a/task1/","title":"Task 1: Velocity Control","text":"<p>Develop a working ROS application to making a real TurtleBot3 Waffle follow a prescribed motion profile, whilst printing key information to the terminal.</p> <p>Assignment #1 Checkpoints</p> <p>The following parts of The ROS 2 Course will support your work on this task: </p> <ul> <li> Part 1: in full.</li> <li> Part 2: up to (and including) Exercise 5.</li> <li> Part 3: Exercise 1.</li> </ul>"},{"location":"com/assignment2/part-a/task1/#summary","title":"Summary","text":"<p>The main objective of this task is to create a ROS node (or multiple nodes) that make your robot follow a figure-of-eight pattern on the robot arena floor. The figure-of-eight trajectory should be generated by following two loops, both 1 meter in diameter, as shown below. </p> <p> </p> The figure-of-eight path for Task 1. <p>The Robot Arena will be set up as follows for this task:</p> <p> </p> The DIA-CR5 Robot Arena layout for Task 1. <p>Whilst doing this, you will also need to present robot odometry data to the terminal at regular intervals (see below for the specifics). Your node must output this data as ROS 2 Log Messages with <code>INFO</code> severity, i.e. using <code>get_logger().info()</code> calls from within one of your nodes. Log messaging is used in lots of the exercises and example code throughout Assignment #1, and so you should refer back to this for guidance if you need it. </p> <p>Additionally, for this task, log messages must be formatted in a particular way, so you might want to have a look at the documentation on Python String Formatting, and read the details below thoroughly so that you know what is expected!</p>"},{"location":"com/assignment2/part-a/task1/#details","title":"Details","text":"<ol> <li>\"Loop 1\": The robot must start by moving anti-clockwise, following a circular motion path of 1 m diameter around the red beacon as shown above.</li> <li>\"Loop 2\": Once the robot has returned to the starting point, it must then turn clockwise to follow a second circular path, again of 1 m diameter, this time around the blue beacon.</li> <li>After Loop 2 the robot must stop on the start / finish line (10 cm, as denoted by the white tape lines on the floor).</li> <li> <p>The velocity of the robot should be defined to ensure that the whole sequence takes 60 seconds to complete (5 seconds).</p> <p>Note: The timer will start as soon as the robot starts moving.</p> </li> <li> <p>The robot's real-time pose should be printed to the terminal throughout, where messages should be of the following format (exactly): </p> <pre><code>x={x} [m], y={y} [m], yaw={yaw} [degrees].\n</code></pre> <p>Where <code>{x}</code>, <code>{y}</code> and <code>{yaw}</code> should be replaced with the correct real-time odometry data as follows:</p> <ol> <li><code>{x}</code>: the robot's linear position in the X axis, quoted in meters to two decimal places.</li> <li><code>{y}</code>: the robot's linear position in the Y axis, quoted in meters to two decimal places.</li> <li><code>{yaw}</code>: the robot's orientation about the Z axis, quoted in degrees to one decimal place.</li> </ol> <p>The data should be quoted relative to its starting position at the beginning of the task, e.g. at the start of the task (before the robot has moved) the terminal messages should read:</p> <pre><code>x=0.00 [m], y=0.00 [m], yaw=0.0 [degrees].\n</code></pre> <p>These message should be printed to the terminal at a rate of 1Hz. It doesn't matter if the messages continue to be printed to the terminal after the robot has stopped (i.e. after the figure-of-eight has been completed).</p> <p>Important</p> <p>You should use <code>get_logger().info()</code> method calls within your node to print these terminal messages.</p> </li> </ol>"},{"location":"com/assignment2/part-a/task1/#a-note-on-odometry","title":"A note on Odometry","text":"<p>When the robot is placed in the arena at the start of the task its odometry may not necessarily read zero, so you will need to compensate for this. You'll therefore need to grab the robot pose from the <code>/odom</code> topic before your robot starts moving, and then use that as the zero-reference to convert all the subsequent odometry readings that you obtain throughout the task.</p> <p>Odometry and keeping track of the robot's pose is discussed in detail in Assignment #1 Part 2.</p>"},{"location":"com/assignment2/part-a/task1/#launch","title":"Executing Your Code","text":"<p>When assessing your code for this task, the teaching team will use the following command to execute all the necessary functionality from within your package:</p> <pre><code>ros2 launch com2009_teamXX_2026 task1.launch.py\n</code></pre> <p>... where <code>XX</code> will be replaced with your team number.</p> <p>As such, your ROS 2 Node(s) for Task 1 MUST be executable via a launch file, and this launch file MUST be called <code>task1.launch.py</code>.</p> <p>Note</p> <p>ROS will already be running on the robot before we attempt to execute your launch file, and a Zenoh Session will be running on the laptop, to allow your nodes (running on the laptop) to communicate with it. You don't need to include any of this in your <code>task1.launch.py</code> launch description.</p>"},{"location":"com/assignment2/part-a/task1/#marking","title":"Marking","text":"<p>This task will be assessed by the teaching team as part of Part A (i.e. along with Task 2). This will be assessed during the Easter Holiday period, with feedback returned to you before the semester resumes.</p> <p>There are 20 marks available for this task in total, summarised as follows:</p> <p></p><p></p> Criteria Marks Details A: The Motion Path 10/20 How closely the real robot follows a true figure-of-eight path in the robot arena, based on the criteria table below. B: Terminal Messages 10/20 The correct formatting of your odometry messages, and the validity of the data that is presented in the terminal as the robot performs the task, based on the criteria table below. <p></p><p></p>"},{"location":"com/assignment2/part-a/task1/#criterion-a-the-motion-path","title":"Criterion A: The Motion Path","text":"<p>Marks: 10/20</p> <p></p><p></p> Criteria Details Marks A1: Direction of travel The robot must move anticlockwise for the first loop (\"Loop 1\") and then clockwise for the second (\"Loop 2\"). 2 A2: Loop 1 The loop must be 1 m in diameter, centred about the red beacon. 2 A3: Loop 2 The loop must be 1 m in diameter, centred about the blue beacon. 2 A4: Stopping Once the robot completes its figure of eight, it must stop with both wheels within 10 cm of the start line (as denoted by white tape lines on the floor). 2 A5: Timing The robot must complete the full figure of eight and stop in 55-65 seconds. 2 <p></p><p></p>"},{"location":"com/assignment2/part-a/task1/#criterion-b-terminal-messages","title":"Criterion B: Terminal Messages","text":"<p>Marks: 10/20</p> <p></p><p></p> Criteria Details Marks B1: Rate Messages should be printed to the terminal at a rate of 1 Hz. 2 B2: Format The messages printed to the terminal should be formatted exactly as detailed above, and must be presented as ROS 2 Log Messages with <code>INFO</code> severity (i.e. using <code>get_logger().info()</code> method calls). 2 B3: Data Each message value (<code>x</code>, <code>y</code> and <code>yaw</code>) should be plausible, that is: they represent the actual pose of the robot at all points throughout the figure-of-eight, based on all readings being set to zero at the start/finish line (as illustrated above). In addition, each value must be quoted in the correct units (meters / degrees, as appropriate). 6 <p></p><p></p>"},{"location":"com/assignment2/part-a/task1/#simulation-resources","title":"Simulation Resources","text":"<p>You might find it helpful to develop the core functionality for this task in simulation before getting things running on the real robot. </p> <p>Real World vs. Sim</p> <p>There is no substitute for real-world testing! </p> <p>While you might develop a ROS application that works perfectly in simulation, this doesn't mean that it will work equally well in the real world.</p> <p>Ultimately, this task (and indeed all the other Assignment #2 programming tasks) will be assessed on real robots, so make the most of the lab sessions and test things out on the real robots thoroughly.</p> <p>As shown above, for the assessment there will be cylindrical beacons placed at the centre of each of the figure-of-eight loops which the robot will need to move around as it completes the task. We have therefore also created a simulation environment that is representative of the real world environment. This is available in the <code>tuos_task_sims</code> package, which is part of the <code>tuos_ros</code> Course Repo. The instructions for downloading and installing this within your own local ROS installation are available here.</p> <p>If you've already installed this (as part of Assignment #1 perhaps), then it's worth making sure that you have the most up-to-date version (as discussed here).</p> <p>Once you've done all this, then you should be able to launch the simulation using <code>ros2 launch</code> as follows:</p> <pre><code>ros2 launch tuos_task_sims fig_of_eight.launch.py\n</code></pre> <p> </p> A simulation environment to represent the real DIA-CR5 arena layout for Task 1. <p>Note</p> <p>Loop markers are illustrative, there won't be any on the real robot arena floor during the assessment.</p>"},{"location":"com/assignment2/part-a/task2/","title":"Task 2: Avoiding Obstacles","text":"<p>Develop the ROS node(s) to allow a real TurtleBot3 Waffle to autonomously explore an environment containing various obstacles. The robot must explore as much of the environment as possible in 90 seconds without crashing into anything!</p> <p>Assignment #1 Checkpoints</p> <p>The following parts of The ROS 2 Course to support your work on this task (in addition to the suggestions for Task 1): </p> <ul> <li> Part 3: in full.</li> <li> Part 5: in full.</li> </ul> <p> </p>"},{"location":"com/assignment2/part-a/task2/#summary","title":"Summary","text":"<p>Assignment #1 Part 3 introduces the Waffle's LiDAR sensor. This sensor is very useful, as it tells us the distance to any objects that are present in the robot's environment. In Assignment #1 Part 5 we look at how this data, in combination with the ROS Action framework, can be used as the basis for a basic exploration strategy that would incorporate obstacle avoidance. Building on this in Part 5 Exercise 6, we discuss how this could be developed further by developing an action client that could make successive calls to the action server to keep the robot moving randomly, and indefinitely, around an arena whilst avoiding obstacles.</p> <p>This is one approach that you could use for this task, but there are other (and potentially simpler) ways that this could be achieved too. </p> <p>In COM2009 Lecture 3 (\"Sensing, Actuation &amp; Control\"), for instance, you are introduced to Cybernetic Control Principles and some of Braitenberg's \"Vehicles,\" which are discussed and implemented on a Lego robot during the lecture! In particular, \"Vehicle 3b\" might well be relevant to consider as a simple method to achieve an obstacle avoidance behaviour.</p> <p>Another aspect of this task is exploration: your robot will be awarded more marks for navigating around more of the environment. Consider the search strategies that are discussed in Lecture 8 (\"Local Guidance Strategies\"), such as \"Brownian Motion\" and \"Levy Walks.\" Could something along these lines be implemented on the  Waffle?</p>"},{"location":"com/assignment2/part-a/task2/#details","title":"Details","text":"<p>The Diamond Computer Room 5 Robot Arena is a square arena of 4x4m. For the task, the arena will contain a number of \"obstacles,\" i.e.: short wooden walls and coloured cylinders. Your robot will need to be able to detect these obstacles and navigate around them in order to fully explore the space.</p> <p>Exploration marks will be awarded when the robot enters each of the 12 outer arena zones (each a 1x1m square), as shown below.</p> <p></p> <p> </p> The DIA-CR5 Robot Arena layout for Task 2. <p></p> <p>Important</p> <p>This is an example of what the environment might look like:</p> <ul> <li>ALL objects (i.e. the four coloured cylinders and the four wall assemblies) could be in different positions entirely. </li> <li>The wooden walls may not be touching the outer edges of the arena!</li> <li>The coloured cylinders could be inside exploration zones. </li> <li>The only things that will remain the same are the arena size, the presence of the outer arena walls and the floor layout (i.e. the location of all the zones).</li> </ul> <ol> <li>The robot will start in the centre of the arena, perpendicular to one of the four outer walls.</li> <li> <p>It must explore the environment for 90 seconds without touching any of the arena walls or the obstacles within it.</p> <p>Note: The 90-second timer will start as soon as the robot starts moving within the arena.</p> </li> <li> <p>If the robot makes contact with anything before the time has elapsed then the attempt is over, and this time will be recorded to determine a \"Run Time\" mark (see below).</p> </li> <li>As shown above, the arena floor will be marked with 12 equal-sized (1x1m) zones and the robot must enter as many of these 12 exploration zones as possible during the attempt.</li> <li> <p>The robot must be moving for the entire duration of the task. Simply just turning on the spot for the whole time doesn't count!</p> <ul> <li>What we want to see here is that the robot is constantly making an effort to explore.</li> <li>It is however OK for the robot to stop moving and turn on the spot for a few seconds whenever required though.</li> <li>If the robot explores for a while and then stops and doesn't move again for the remainder of the 90-second run, then Run Time marks will be awarded up to the point at which the robot ceases to be active.</li> <li>Further details on the eligibility for Run Time marks are provided in the Marking Section below.</li> </ul> </li> </ol>"},{"location":"com/assignment2/part-a/task2/#launch","title":"Executing Your Code","text":"<p>When assessing your code for this task, the teaching team will use the following command to execute all the necessary functionality from within your team's ROS 2 package:</p> <pre><code>ros2 launch com2009_teamXX_2026 task2.launch.py\n</code></pre> <p>... where <code>XX</code> will be replaced with your team number.</p> <p>As such, the ROS 2 package that your team submit must contain a launch file called <code>task2.launch.py</code>, to execute all the necessary functionality from within your package for this task.</p> <p>Note</p> <p>ROS will already be running on the robot before we attempt to execute your launch file, and a Zenoh Session will be running on the laptop, to allow nodes running on the laptop to communicate with it.</p>"},{"location":"com/assignment2/part-a/task2/#dependencies","title":"Dependencies","text":"<p>You may draw upon pre-existing Python libraries or ROS 2 packages in your own work for Assignment #2, but there are restrictions that you must be aware of. See here for more details on this.</p> <p>Nav2</p> <p>In addition to the above, the use of Nav2 is not permitted for this task.</p>"},{"location":"com/assignment2/part-a/task2/#marking","title":"Marking","text":"<p>There are 20 marks available for Task 2 in total, awarded as follows:</p> <p></p><p></p> Criteria Marks Details A: Run Time 8/20 You will be awarded marks for the amount of time that your robot spends exploring the environment before 90 seconds has elapsed, or until the robot makes contact with anything in its environment for the first time (as per the table below). The robot must leave the centre zone (a 1x1m box, denoted in red in the figure above) in order to be eligible for any of these marks. If the robot does not explore beyond the \"partial exploration\" zone (denoted orange in the figure) then a \\(0.5\\times\\) multiplication factor will be applied to the run time marks. B: Exploration 12/20 You will be awarded 1 mark for each of the 12 exploration zones that the robot manages to enter. The robot only needs to enter each of the 12 zones once, but its full body must be inside the zone marking to be awarded the mark. <p></p><p></p>"},{"location":"com/assignment2/part-a/task2/#run-time","title":"Criterion A: Run Time","text":"<p>Marks: 8/20</p> <p>Marks will be awarded as follows:</p> <p></p><p></p> Time (Seconds) Marks 0-9 0 10-19 1 20-29 2 30-39 3 40-49 4 50-59 5 60-89 6 The full 90! 8 <p></p><p></p>"},{"location":"com/assignment2/part-a/task2/#simulation-resources","title":"Simulation Resources","text":"<p>Within the <code>tuos_task_sims</code> package there is an example arena that can be used to develop and test out your team's obstacle avoidance node(s) for this task<sup>1</sup>. As above however, this is just an example of what the real-world environment might look like. </p> <p>The simulation can be launched using the following <code>ros2 launch</code> command:</p> <pre><code>ros2 launch tuos_task_sims obstacle_avoidance.launch.py\n</code></pre> <p> </p> A simulation environment to represent the real DIA-CR5 arena layout for Task 2. <p>Real World vs. Sim</p> <p>Remember: just because it works in simulation DOESN'T mean it will work in the real world!</p> <p>Make sure you test things out thoroughly on the real robots during the lab sessions.</p> <ol> <li> <p>Make sure you check for updates to the Course Repo to ensure that you have the most up-to-date version of these simulations.\u00a0\u21a9</p> </li> </ol>"},{"location":"com/assignment2/part-b/","title":"Assignment #2 Part B","text":""},{"location":"com/assignment2/part-b/#assignment-2-part-a","title":"Assignment #2 Part A","text":"<p>Total Marks: 60/100</p> <p>Submission: Friday of Week 12 at 6pm (BST)</p> <p>Tasks:</p> <ul> <li>Task 3: Exploration &amp; Search</li> <li>Task 4: Documentation</li> </ul> <p>Ensure that you have prepared appropriately for the Part B submission by following all the instructions here.</p>"},{"location":"com/assignment2/part-b/task3/","title":"Task 3: Exploration & Search","text":"<p>Develop the ROS node(s) to allow a TurtleBot3 Waffle to autonomously explore the full Computer Room 5 robot arena, navigating through a series of rooms as quickly as possible whilst documenting its exploration with a photo of a beacon and a map of the environment as it goes! </p> <p>Course Checkpoints</p> <p>You should aim to have completed the following additional parts of the COM2009 ROS Course to support your work on this task: </p> <ul> <li>Assignment #1: All of it, in full!</li> <li>Real Waffle Essentials: Exercises 1-5 (i.e. all of them!)</li> </ul>"},{"location":"com/assignment2/part-b/task3/#summary","title":"Summary","text":"<p>For this task the robot arena will contain a series of \"rooms\" each with a coloured, cylindrical beacon in it. The main aim is to safely explore each of the rooms in the shortest time possible (emphasis on \"safely\" here, meaning you need to also try not to crash into anything in the process!) At the same time, you'll need to search for a beacon of a particular colour as well as documenting your exploration by building a map of the environment (with SLAM).</p>"},{"location":"com/assignment2/part-b/task3/#simulation-resources","title":"Simulation Resources","text":"<p>As with Tasks 1 &amp; 2, there's a simulation to help you develop your code for this task outside the lab sessions. This also helps to illustrate the nature of the task.</p> <p>Remember</p> <p>Just because it works in simulation DOESN'T mean it will work in the real world!</p> <p>... Make sure you test things out thoroughly during the lab sessions!</p> <p>You can launch the simulation from the <code>com2009_simulations</code> package with the following <code>ros2 launch</code> command:</p> With RobotWithout Robot <pre><code>ros2 launch com2009_simulations task3.launch.py\n</code></pre> <p></p> <p></p> <pre><code>ros2 launch com2009_simulations task3.launch.py with_robot:=false\n</code></pre> <p></p> <p></p> <p>Make sure you check for updates to the course repo to ensure that you have the most up-to-date version of this.</p> <p>Once again, this is just an example of what the real-world environment could look like:</p> <ul> <li>Like the simulation, \"rooms\" will be constructed of wooden walls 180 mm tall, 10 mm thick and either 440 mm or 880 mm in length</li> <li>Each room will contain a cylindrical beacon of 200 mm diameter and 250 mm height</li> <li>Rooms will be different shapes and sizes and in different locations (but there will always be four of them)</li> <li>The robot might not necessarily be located at the same starting point as in the simulation, it could start anywhere in the arena</li> <li> <p>Beacons will be the same shape, size and colour as those in the simulation (yellow, red, green and blue). </p> <p>But: detecting colours is a lot harder in the real-world than it is in simulation, so you'll need to do a lot of testing on a real robot if you want to get this working robustly (you will have access to all the beacons during the lab sessions).</p> </li> </ul>"},{"location":"com/assignment2/part-b/task3/#details","title":"Details","text":"<ol> <li>The robot will have 3 minutes (180 seconds) in total to complete this task. The timer will start as soon as the robot starts moving within the arena.</li> <li>The arena floor will be marked out into 9 equal-sized zones. You will be awarded marks for each of the zones that the robot enters within the time available (excluding the one it starts in).</li> <li>In addition to this, the robot will need to try to explore the four rooms that will also be present in the arena. There will be marks available not only for the number of rooms that the robot manages to explore, but also the speed at which it manages to explore them all (see the marking section below for more details).</li> <li> <p>Your robot will need to do this whilst avoiding contact with anything in the environment. </p> <p>Any contact with the environment is referred to as an \"incident.\" Once an incident has taken place, we'll move the robot away slightly so that it is free to move again, but after five incidents have occurred the assessment will be stopped.</p> </li> </ol> <p>Having developed the core functionality for the task, you'll then need to think about a couple of more advanced features...</p>"},{"location":"com/assignment2/part-b/task3/#advanced-feature-1-a-photo-of-a-beacon","title":"Advanced Feature 1: A photo of a beacon","text":"<p>As with the previous 2 tasks, we will launch the ROS node(s) from within your package for this task using <code>ros2 launch</code> (as discussed below). For this one however, we will also attempt to supply an additional argument when we make the command-line call:</p> <pre><code>ros2 launch com2009_teamXX_2026 task3.launch.py target_colour:=COLOUR\n</code></pre> <p>...where <code>COLOUR</code> will be replaced with either <code>yellow</code>, <code>red</code>, <code>green</code> or <code>blue</code> (the target colour will be selected randomly). Based on this input, your robot will need to capture an image of the beacon in the arena of the same colour!</p> <p>Warning</p> <ul> <li>You should know from Essential Exercise 3 that the camera image topic name is different on the real robot!</li> <li>You should also consider the fact that images are captured at a different resolution: consider how this might impact any image processing that you do. </li> </ul> <p>At the root of your package there must be a directory called <code>snaps</code>, and the image must be saved into this directory with the file name: <code>target_beacon.jpg</code>. The image that is saved must be the raw image from the robot's camera, and should not include any filtering that you may have applied in post-processing.  </p> <p>You will therefore need to define your launch file to accommodate the <code>target_colour</code> command-line argument. In addition to this, inside your launch file you'll also need to pass the value of this to a ROS node within your package, so that the node knows which beacon to actually look for (i.e. your node needs to know whether to look for a yellow, red, green or blue beacon). This kind of launch file functionality wasn't covered in Assignment #1, so there are some additional resources available here, to help you with this.</p> <p></p> <p>We will test whether your launch file has been correctly built to accept the <code>target_colour</code> command-line argument using the <code>-s</code> option with <code>ros2 launch</code>, which provides a list of all arguments that may be given to the launch file: </p> <pre><code>ros2 launch com2009_teamXX_2026 task3.launch.py -s\n</code></pre> <p>Having built your <code>task3.launch.py</code> file correctly, the <code>target_colour</code> argument should be listed in the output of the above command, e.g.:</p> <pre><code>$ ros2 launch com2009_teamXX_2026 task3.launch.py -s\nArguments (pass arguments as '&lt;name&gt;:=&lt;value&gt;'):\n\n    'target_colour':\n        The colour of the beacon to search for (yellow|red|green|blue). \n</code></pre> <p></p> <p>To illustrate that the value of the <code>target_colour</code> command-line argument has been correctly passed to a ROS Node within your package, you should configure your Node (or any one of your nodes, if you have multiple) to print a message to the terminal as soon as your launch file is called. </p> <p>The message should be formatted exactly as follows:</p> <pre><code>TARGET BEACON: Searching for COLOUR.\n</code></pre> <p>...where <code>COLOUR</code> must be replaced with the actual colour that was passed to your <code>task3.launch.py</code> file (either <code>yellow</code>, <code>red</code>, <code>green</code> or <code>blue</code>). You should use a <code>get_logger().info()</code> method call to print this terminal message.</p>"},{"location":"com/assignment2/part-b/task3/#advanced-feature-2-mapping-with-slam","title":"Advanced Feature 2: Mapping with SLAM","text":"<p>Marks are also available if, whilst your robot is completing this task, you can use SLAM to generate a map of the environment in the background.</p> <p>In Part 3 of Assignment #1 we launched SLAM using the following <code>ros2 launch</code> command:</p> <pre><code>ros2 launch tuos_simulations cartographer.launch.py\n</code></pre> <p>Note</p> <p>When using Cartographer with the real robots, you'll need to set <code>use_sim_time</code> to <code>false</code>:</p> <pre><code>ros2 launch tuos_simulations cartographer.launch.py use_sim_time:=false\n</code></pre> <p>See here for how to do this within a launch file.</p> <p>Also in Part 3 we discussed how to use launch files to launch other launch files! Consider how you could take a similar approach to run SLAM from your <code>task3.launch.py</code> file.</p> <p>When it comes to saving the map that has been generated by SLAM, recall how we did this from the command-line in Part 3 Exercise 5, using the following command: </p> <pre><code>ros2 run nav2_map_server map_saver_cli -f MAP_NAME\n</code></pre> <p>It is also possible however to do this programmatically using the ROS 2 Service framework. Consider Assignment #1 Part 4 Exercise 6 for how this could be done from within your Task 3 application.</p> <p>The root of your package directory must contain a directory called <code>maps</code>, and the map file that you obtain must be saved into this directory with the name: <code>arena_map.png</code>.</p>"},{"location":"com/assignment2/part-b/task3/#launch","title":"Executing Your Code","text":"<p>Your team's ROS package must contain a launch file named <code>task3.launch.py</code>, such that (for the assessment) we are able to launch all the nodes that you have developed for this task via the following command:</p> <p></p><pre><code>ros2 launch com2009_teamXX_2026 task3.launch.py target_colour:=COLOUR\n</code></pre> ... where <code>XX</code> will be replaced with your team number and <code>COLOUR</code> will be replaced with either <code>yellow</code>, <code>red</code>, <code>green</code> or <code>blue</code>.<p></p> <p>Note</p> <p>ROS will already be running on the robot before we attempt to execute your launch file, and a bridge between the robot and laptop will have already been established.</p>"},{"location":"com/assignment2/part-b/task3/#marking","title":"Marking","text":"<p>There are 40 marks available for this task in total, awarded based on the criteria outlined below.</p> <p></p><p></p> Criteria Marks Details A: Arena exploration 8/40 For this task, the arena will be divided into nine equal-sized zones. You will be awarded 1 mark for each zone that your robot manages to enter, excluding the one it starts within. The robot only needs to enter each zone once, but its full body must be inside the zone marking to be awarded the associated mark. B: Room exploration 12/40 Marks will be awarded based on the maximum number of rooms that your robot manages to explore within the 180-second time limit and the speed by which it does this. The marking details are outlined here. C: An 'incident-free run' 5/40 If your robot completes the task (or the 180 seconds elapses) without it making contact with anything in the arena then you will be awarded full marks here for an incident-free-run! You will however be deducted 1 mark per unique \"incident\" that occurs during the assessment. Your robot must at least leave the zone that it starts in to be eligible for these marks and once five incidents have been recorded in total then the assessment will be stopped. D1: A Photo of a Beacon 10/40 Further details below. D2: Mapping with SLAM 5/40 Further details below. <p></p><p></p>"},{"location":"com/assignment2/part-b/task3/#room-explore","title":"Criterion B: Room exploration","text":"<p>The marks available per room explored will be awarded as follows:</p> <p></p><p></p> Time (seconds) 1 room 2 rooms 3 rooms 4 rooms 150-180 1.0 4.0 7.0 12.0 120-149 1.5 4.5 7.5 12.0 90-129 2.0 5.0 8.0 12.0 60-89 2.5 5.5 9.0 12.0 &lt;60 3.0 6.0 9.0 12.0 <p></p><p></p> <p>The 180-second timer starts as soon as the robot starts moving within the arena.</p>"},{"location":"com/assignment2/part-b/task3/#crit-d1","title":"Criterion D1: A Photo of a Beacon","text":"Criteria Details Marks D1.a Your <code>task3.launch.py</code> file has been built to accept <code>target_colour</code> argument (assessed by running <code>ros2 launch -s</code> on your launch file) and a message is printed to the terminal (using a <code>get_logger().info()</code> method call) to indicate that the correct target colour has been passed to a node in your package (this must occur straight away on executing your launch file and the message format must be exactly as specified here). 2 D1.b At the end of the assessment a single image file, called <code>target_beacon.jpg</code>, has been obtained from the robot's camera (during the course of the assessment), and this is located in a folder called <code>snaps</code> at the root of your package directory i.e.: <code>com2009_teamXX_2026/snaps/target_beacon.jpg</code>. 2 D1.c Your <code>com2009_teamXX_2026/snaps/target_beacon.jpg</code> image file contains any part of the correct beacon. 3 D1.d Your <code>com2009_teamXX_2026/snaps/target_beacon.jpg</code> image file has captured the full width of the correct beacon. 3"},{"location":"com/assignment2/part-b/task3/#crit-d2","title":"Criterion D2: Mapping with SLAM","text":"Criteria Details Marks D2.a By the end of the assessment a map of the robot arena (or any part of it) must have been generated. Two files should exist: a <code>.png</code> and a <code>.yaml</code>, both of which must be called <code>arena_map</code>, and both must be located in a <code>maps</code> folder at the root of your package directory i.e. <code>com2009_teamXX_2026/maps/arena_map.png</code> and <code>com2009_teamXX_2026/maps/arena_map.yaml</code>. 2 D2.b Your <code>com2009_teamXX_2026/maps/arena_map.png</code> file that is created during the assessment is a map that depicts at least one of the rooms of the arena, in full. 3"},{"location":"com/assignment2/part-b/task4/","title":"Task 4: Documentation","text":""},{"location":"com/assignment2/part-b/task4/#summary","title":"Summary","text":"<p>For this task you will write some documentation to describe the application that you have developed for Task 3.</p> <p>Your documentation should be contained within a single <code>README.md</code> file which must sit in the root of your team's package directory, i.e.: </p> <pre><code>com2009_teamXX_2026/README.md\n</code></pre> <p>Important</p> <p>Your documentation file MUST be called <code>README.md</code>, it MUST be in the root of your package directory, and it must be created with Markdown Formatted Text (see below).</p>"},{"location":"com/assignment2/part-b/task4/#content-of-your-readme","title":"Content of your README","text":"<p>Your documentation should contain the following information.</p>"},{"location":"com/assignment2/part-b/task4/#overview","title":"Overview","text":"<p>A brief explanation of what your application does (no more than 100 words).</p>"},{"location":"com/assignment2/part-b/task4/#installation-and-execution","title":"Installation and Execution","text":"<p>Explain (to someone who may not already be familiar) how to install and execute your package on one of the Robotics Laptops in the lab.</p>"},{"location":"com/assignment2/part-b/task4/#exceptions","title":"Exceptions","text":"<ol> <li>In your documentation you can assume that Steps 1-4 of the Robot/Laptop Setup process have already been carried out, so you don't need to discuss any of this.</li> <li>You can also assume (for the purposes of this documentation) that your package is public, so there's no need to include any information about creating ssh keys etc (assume anyone could download your package through a simple <code>git clone ...</code>).</li> </ol>"},{"location":"com/assignment2/part-b/task4/#dependencies","title":"Dependencies","text":"<p>This section should also provide details on all the external packages that your application depends upon in order to function (i.e. any Python/ROS 2 libraries that you are using that exist outside your own package). </p>"},{"location":"com/assignment2/part-b/task4/#functional-description","title":"Functional Description","text":"<p>Explain how your application works. This will form the bulk of the documentation, and should include a Functional Block Diagram (FBD) (or multiple FBDs if you wish) to aid the explanation and illustrate the control logic. This should be more than simply a ROS node/topic graph. </p> <p>See here for information on how to include images in your README.md to ensure that they are rendered correctly (see the information on using relative links to images that exist within your repository).</p>"},{"location":"com/assignment2/part-b/task4/#contributors","title":"Contributors","text":"<p>List all contributing team members and provide links to their GitHub profiles.</p>"},{"location":"com/assignment2/part-b/task4/#word-count","title":"Word Count","text":"<p>Your documentation should be 600-800 words in length, any content after the 800-word limit won't be read (and therefore won't be considered in the assessment either).</p>"},{"location":"com/assignment2/part-b/task4/#formatting","title":"Formatting","text":"<p>Your <code>README.md</code> file should be formatted using GitHub Flavoured Markdown. You can find the basic formatting syntax here, which should be sufficient for the purposes of this task. If you really want to do more however, then the full GitHub Flavoured Markdown Specification can be found here.</p> <p>When marking this task we will view your <code>README.md</code> file on GitHub, so it's important that you check the formatting yourself prior to the Part B deadline. You will lose marks for formatting errors!</p>"},{"location":"com/assignment2/part-b/task4/#marking","title":"Marking","text":"<p>There are 20 marks available for this task in total, distributed as follows.</p> <p></p><p></p> Criteria Marks Details A: Overview 5/20 A clear and concise summary of the application, and a full and correct explanation of how to install and execute it. B: Functional Description 10/20 A detailed and accurate description of how the application works (or was intended to work<sup>1</sup>). A clear yet detailed Functional Block Diagram should be included to support the discussion. C: Formatting and Writing Standard 5/20 Clear, concise and professional writing throughout, which uses technical language appropriately but which is accessible to a none experienced reader. The <code>README.md</code> file must be correctly formatted, i.e. headings, styling, code blocks, other text formatting, figures etc. should all be rendered correctly when viewed directly on GitHub. <p></p><p></p> <ol> <li> <p>The marking of this task is not dependent on your team's performance in Task 3, so even if you score 0/40 marks in Task 3, you could still score 20/20 marks for this task if you document your work well.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/","title":"The ROS 2 Course","text":""},{"location":"course/#overview","title":"Overview","text":"<p>This is a 6-part course, designed to teaching you all about ROS 2 and how to use it. The course is designed to be completed in simulation, so you will therefore need access to a ROS 2 installation which can either be installed on your own machine, or accessed on a range of managed computers across the University of Sheffield campus. See here for more information on how to access or install ROS 2.</p> <p>Each part of the course comprises a series of step-by-step instructions and exercises to teach you how ROS works, and introduces you to the core principles of the framework. The exercises give you the opportunity to see how to apply these principles to practical robotic applications. Completing this course (either in part or in full) will provide you with the necessary skills for working with our real TurtleBot3 Waffle robots in the Diamond.  </p>"},{"location":"course/#the-course","title":"The Course","text":"<ul> <li> <p>Part 1: Getting Started with ROS 2</p> <p>Learn the basics of ROS 2 and become familiar with some key tools and principles, allowing you to program robots and work with ROS 2 applications effectively.</p> </li> <li> <p>Part 2: Odometry &amp; Navigation</p> <p>Learn about Odometry, which informs us of a robot's position and orientation in an environment. Apply both open and closed-loop velocity control methods to a Waffle.</p> </li> <li> <p>Part 3: Beyond the Basics</p> <p>Execute ROS applications more efficiently using launch files, and learn how to affect the behaviour of nodes during run-time using parameters. Learn about the LiDAR sensor, the data that it generates, and see the benefits of this for tools like \"SLAM\".</p> </li> <li> <p>Part 4: Services</p> <p>Learn about an alternative way that ROS nodes can communicate across a ROS network, and the situations where this might be useful.</p> </li> <li> <p>Part 5: Actions</p> <p>Learn about another key ROS communication method which is similar to a ROS Service, but with a few key benefits and alternative use-cases.</p> </li> <li> <p>Part 6: Cameras, Machine Vision &amp; OpenCV</p> <p>Learn how to work with images from a robot's camera. Learn techniques to detect features within these images, and use this to inform robot decision-making.</p> </li> </ul>"},{"location":"course/part1/","title":"Part 1: Getting Started with ROS 2","text":""},{"location":"course/part1/#introduction","title":"Introduction","text":"<p> Exercises: 8 Estimated Completion Time: 2 hours Difficulty Level: Beginner </p>"},{"location":"course/part1/#aims","title":"Aims","text":"<p>In the first part of this lab course you will learn the basics of ROS 2 and become familiar with some key tools and principles of the framework, which will allow you to program robots and work with ROS 2 applications effectively.</p> <p>Throughout this course, and from herein, we'll refer to ROS 2 as just \"ROS\" to make things easier!</p> <p>For the most part, you will interact with ROS using the Linux command line and so you will also become familiar with some key Linux command line tools that will help you.  Finally, you will learn how to create some basic ROS Nodes using Python and get a taste of how communications work via ROS Topics and Interfaces.</p>"},{"location":"course/part1/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:  </p> <ol> <li>Control a TurtleBot3 Robot, in simulation, using ROS.</li> <li>Launch ROS applications using <code>ros2 launch</code> and <code>ros2 run</code>.</li> <li>Interrogate running ROS applications using key ROS command line tools.</li> <li>Create a ROS package comprised of multiple nodes and program these nodes (in Python) to communicate with one another using ROS Communication Methods.</li> <li>Create a custom ROS message interface and create Python Nodes to use this.</li> <li>Navigate a Linux filesystem and learn how to do various filesystem operations from within a Linux Terminal.</li> </ol>"},{"location":"course/part1/#quick-links","title":"Quick Links","text":""},{"location":"course/part1/#exercises","title":"Exercises","text":"<ul> <li>Exercise 1: Launching a simulation and making a robot move</li> <li>Exercise 2: Visualising the ROS Network</li> <li>Exercise 3: Exploring ROS Topics and Messages</li> <li>Exercise 4: Creating your own ROS Package</li> <li>Exercise 5: Creating a publisher node</li> <li>Exercise 6: Creating a subscriber node</li> <li>Exercise 7: Defining our own message</li> <li>Exercise 8: Using a custom ROS Message</li> </ul>"},{"location":"course/part1/#additional-resources","title":"Additional Resources","text":"<ul> <li>A Simple Python Publisher (for Exercise 5)</li> <li>A Simple Python Subscriber (for Exercise 6)</li> <li>The <code>Example</code> Message Publisher (for Exercise 8)</li> </ul>"},{"location":"course/part1/#first-steps","title":"First Steps","text":""},{"location":"course/part1/#step-1-accessing-a-ros-2-environment-for-this-course","title":"Step 1: Accessing a ROS 2 Environment for this Course","text":"<p>If you haven't done so already, see here for all the details on how to install or access a ROS environment for this course.</p>"},{"location":"course/part1/#step-2-launch-ros","title":"Step 2: Launch ROS","text":"<p>Launch your ROS environment.</p> <ol> <li>If you're using WSL-ROS2 on a university managed desktop machine then follow the instructions here to launch it.</li> <li>If you're running WSL-ROS2 on your own machine, then you'll need to launch the Windows Terminal to access a WSL-ROS2 terminal instance.</li> <li>If you're using Docker, then you can find further instructions here. </li> </ol> <p>Either way, you should now have access to ROS 2 via a Linux terminal instance, and we'll refer to this terminal instance as TERMINAL 1.</p>"},{"location":"course/part1/#step-3-download-the-course-repo","title":"Step 3: Download The Course Repo","text":"<p>We've put together a few ROS packages specifically for this course. These all live within this GitHub repo, and you'll need to download and install this into your ROS environment now, before going any further.</p> <ol> <li> <p>In TERMINAL 1, Navigate into the \"ROS Workspace\" using the <code>cd</code> command<sup>1</sup>:</p> <pre><code>cd ~/ros2_ws/src/\n</code></pre> </li> <li> <p>Then, run the following command to clone the Course Repo from GitHub:</p> <pre><code>git clone https://github.com/tom-howard/tuos_ros.git -b jazzy\n</code></pre> </li> <li> <p>Once this is done, you'll need to build this using a tool called \"Colcon\"<sup>2</sup>:</p> <pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build --packages-up-to tuos_ros &amp;&amp; source ~/.bashrc\n</code></pre> </li> </ol> <p>Don't worry too much about what you just did right now. We'll cover this in more detail throughout the course. That's it for now though, we'll start using some of the packages that we've just installed a bit later on.</p>"},{"location":"course/part1/#ex1","title":"Exercise 1: Launching a simulation and making a robot move","text":"<p>Now that you're all up and running, let's launch ROS and fire up a simulation of our TurtleBot3 Waffle robot... </p> <ol> <li> <p>In TERMINAL 1 enter the following command to launch a simulation of a TurtleBot3 Waffle in an empty world:  </p> <pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre> </li> <li> <p>A Gazebo Sim window should open:</p> <p></p> <p></p> <ol> <li>Zoom in and out using the scroll wheel on your mouse.  </li> <li>Rotate the camera view by simultaneously pressing and holding the left mouse button and the Shift key on your keyboard, and then moving your mouse around.</li> </ol> <p>Using both of these methods you should be able to get a better view of the robot, which is an approximate representation of our real ones.</p> <p></p> <p></p> </li> <li> <p>With the Gazebo simulation up and running, return to your terminal and open up a second terminal instance (TERMINAL 2)</p> </li> <li> <p>In this new terminal instance enter the following command:</p> <pre><code>ros2 run turtlebot3_teleop teleop_keyboard\n</code></pre> </li> <li> <p>Follow the instructions provided in the terminal to drive the robot around using specific buttons on your keyboard:</p> <p></p> <p></p> </li> </ol>"},{"location":"course/part1/#summary","title":"Summary","text":"<p>You just launched a number of different applications on a ROS Network using two different ROS commands - <code>ros2 launch</code> and <code>ros2 run</code>: </p> <ol> <li><code>ros2 launch turtlebot3_gazebo empty_world.launch.py</code></li> <li><code>ros2 run turtlebot3_teleop teleop_keyboard</code></li> </ol> <p>These two commands have a similar structure, but work slightly differently. </p> <p>The first command you used was a <code>launch</code> command, which has the following two parts to it (after the <code>launch</code> bit):</p> <pre><code>ros2 launch {[1] Package name} {[2] Launch file}\n</code></pre> <p>Part [1] is the name of the ROS package containing the functionality that we want to execute. Part [2] is a file within that package that tells ROS exactly what scripts ('nodes') that we want to launch. We can launch multiple nodes at the same time from a single launch file.</p> <p>The second command was a <code>run</code> command, which has a structure similar to <code>launch</code>:</p> <pre><code>ros2 run {[1] Package name} {[2] Node name}\n</code></pre> <p>Here, Part [1] is the same as the <code>launch</code> command, but Part [2] is slightly different: <code>{[2] Node name}</code>. Here we are directly specifying a single script that we want to execute. We therefore use <code>ros2 run</code> if we only want to launch a single node on the ROS network: the <code>teleop_keyboard</code> node (a Python script), in this case.</p>"},{"location":"course/part1/#ros-packages-nodes","title":"ROS Packages &amp; Nodes","text":""},{"location":"course/part1/#packages","title":"Packages","text":"<p>ROS applications are organised into packages. Packages are basically collections containing scripts, configurations and launch files (ways to launch those scripts and configurations), all of which relate to some common robot functionality. ROS uses packages as a way to organise all the programs running on a robot. </p> <p>Info</p> <p>The package system is a fundamental concept in ROS and all ROS programs are organised in this way.</p> <p>You will create a number of packages throughout this course, each containing different nodes, launch files and other things too. We'll start to explore this later on.</p>"},{"location":"course/part1/#nodes","title":"Nodes","text":"<p>ROS Nodes are executables that perform specific robot tasks and operations. Earlier on (for example) we used <code>ros2 run</code> to execute a node called <code>teleop_keyboard</code>, which allowed us to remotely control (or \"teleoperate\") the robot. </p> <p>Question</p> <p>What was the name of the ROS package that contained the <code>teleop_keyboard</code> node? (Remember: <code>ros2 run {[1] Package name} {[2] Node name}</code>)</p> <p>A ROS robot might have hundreds of individual nodes running simultaneously to carry out all its necessary operations and actions. Each node runs independently, but uses ROS communication methods to share data with the other nodes on the ROS Network.</p>"},{"location":"course/part1/#the-ros-network","title":"The ROS Network","text":"<p>We can use the <code>ros2 node</code> command to view all the nodes that are currently active on a ROS Network.</p>"},{"location":"course/part1/#ex2","title":"Exercise 2: Visualising the ROS Network","text":"<p>You should currently have two terminal instances active: the first in which you launched the Gazebo simulation (TERMINAL 1) and the second with your <code>teleop_keyboard</code> node active (TERMINAL 2).</p> <ol> <li>Open up a new terminal instance now (TERMINAL 3).</li> <li> <p>Use the following command to have a look at which nodes are currently active on the network:</p> <pre><code>ros2 node list\n</code></pre> <p>Only a handful of nodes should be listed:</p> <pre><code>/robot_state_publisher\n/ros_gz_bridge\n/ros_gz_image\n/teleop_keyboard\n</code></pre> </li> <li> <p>We can visualise the connections between the active nodes by using an application called RQT. RQT is a collection of graphical tools that allow us to interact with and interrogate the ROS network. Launch the main RQT application by entering <code>rqt</code> in TERMINAL 3 (you might see some warnings in the terminal when you do this, but don't worry about them):</p> <pre><code>rqt\n</code></pre> <p>A window should then open:</p> <p></p> <p></p> </li> <li> <p>From here, we then want to load the Node Graph plugin. From the top menu select <code>Plugins</code> &gt; <code>Introspection</code> &gt; <code>Node Graph</code>.</p> </li> <li> <p>Select <code>Nodes/Topics (all)</code> from the top-left most dropdown, and in the <code>Hide</code> section uncheck everything except <code>Debug</code> and <code>Params</code> (you may then need to hit the refresh button):</p> <p></p> (Click on the image to enlarge it.) <p></p> <p>Here, nodes are represented by ellipses and topics by rectangles (hover over a region of the graph to enable colour highlighting).</p> <p>This tool shows us that (amongst other things) the <code>/teleop_keyboard</code> node is communicating with another node called <code>/ros_gz_bridge</code>. The direction of the arrow tells us that <code>/teleop_keyboard</code> is a Publisher and <code>/ros_gz_bridge</code> is a Subscriber. The two nodes communicate via a ROS Topic called <code>/cmd_vel</code>. </p> </li> </ol>"},{"location":"course/part1/#publishers-and-subscribers-a-ros-communication-method","title":"Publishers and Subscribers: A ROS Communication Method","text":"<p>ROS Topics are key to making things happen on a robot. Nodes can publish (write) and/or subscribe to (read) ROS Topics in order to share data around the ROS network. Data is published to topics using ROS Messages. As we've just learnt, the <code>teleop_keyboard</code> node was publishing messages to a topic (<code>/cmd_vel</code>) to make the robot move.</p> <p>Let's have a look at this in a bit more detail...</p>"},{"location":"course/part1/#ex3","title":"Exercise 3: Exploring ROS Topics and Messages","text":"<p>We can find out more about the <code>/cmd_vel</code> topic by using the <code>ros2 topic</code> command.</p> <ol> <li> <p>Open up yet another new terminal instance (TERMINAL 4) and type the following:</p> <pre><code>ros2 topic list\n</code></pre> <p>This shows us all the topics that are currently available on the ROS network (a lot of which we saw in the RQT Node Graph above):</p> <pre><code>/camera/camera_info\n/camera/image_raw\n/camera/image_raw/compressed\n/camera/image_raw/compressedDepth\n/camera/image_raw/theora\n/camera/image_raw/zstd\n/clock\n/cmd_vel\n/imu\n/joint_states\n/odom\n/parameter_events\n/robot_description\n/rosout\n/scan\n/tf\n/tf_static\n</code></pre> <p>Let's find out a bit more about <code>/cmd_vel</code>...</p> </li> <li> <p>Use the <code>topic info</code> command now:</p> <pre><code>ros2 topic info /cmd_vel\n</code></pre> <p>This should provide the following output:</p> <pre><code>Type: geometry_msgs/msg/TwistStamped\nPublisher count: 1\nSubscription count: 1\n</code></pre> <p>We've now established the following information about <code>/cmd_vel</code>: </p> <ol> <li>The topic has 1 publisher writing data to it (the <code>/teleop_keyboard</code> node, as established from the RQT Graph)</li> <li>The topic also has 1 subscriber reading this data (the <code>ros_gz_bridge</code> node)</li> <li> <p>Data is transmitted on the <code>/cmd_vel</code> topic using an Interface. This particular interface is defined as: <code>geometry_msgs/msg/TwistStamped</code>. </p> <p>Interface Definitions</p> <p>Interfaces are standardised data structures that are used to broadcast data across the ROS network. The interface definition above (and, indeed, every interface definition) has three parts to it:</p> <ol> <li><code>geometry_msgs</code>: the name of the ROS package that this interface belongs to.</li> <li><code>msg</code>: that this is a topic message rather than another type of interface (there are three types of interface, and we'll learn about the other two later in this course).</li> <li><code>TwistStamped</code>: the actual interface name</li> </ol> <p>In summary then, we've established that if we want to make the robot move we need to publish <code>TwistStamped</code> messages to the <code>/cmd_vel</code> topic.</p> </li> </ol> </li> <li> <p>Still in TERMINAL 4, use the <code>ros2 interface</code> command to show us the (standardised) data structure used by the <code>TwistStamped</code> Interface:</p> <pre><code>ros2 interface show geometry_msgs/msg/TwistStamped\n</code></pre> <p>From this, we obtain the following:</p> <pre><code>std_msgs/Header header\n    builtin_interfaces/Time stamp\n            int32 sec\n            uint32 nanosec\n    string frame_id\nTwist twist\n    Vector3  linear\n            float64 x\n            float64 y\n            float64 z\n    Vector3  angular\n            float64 x\n            float64 y\n            float64 z\n</code></pre> <p>We'll learn more about what this means in Part 2.</p> </li> <li> <p>To finish, enter Ctrl+C in each of the three terminals that should currently have ROS processes running (Terminals 1, 2 and 3). The associated Gazebo and RQT Node Graph windows should close as a result of this too.</p> <p>Tip</p> <p>Whenever you need to stop any ROS process use Ctrl+C in the terminal it's running in. </p> </li> </ol>"},{"location":"course/part1/#creating-your-first-ros-applications","title":"Creating Your First ROS Applications","text":"<p>Shortly we'll create some simple publisher and subscriber nodes in Python and send simple data between them. As we learnt earlier though, ROS nodes must always live within packages, and so we need to create a package first in order to start creating our own ROS nodes. </p> <p>It's important to work in a specific filesystem location when we create and work on our own ROS packages. These are called \"Workspaces\" and you should already have one ready to go within your local ROS environment called <code>ros2_ws</code><sup>3</sup>, with a subdirectory within it called <code>src</code>:</p> <pre><code>~/ros2_ws/src/\n</code></pre> <p>All new packages MUST be located inside the <code>src</code> folder of the workspace!!</p> <p>Note</p> <p><code>~</code> is an alias for your home directory. So <code>cd ~/ros2_ws/src/</code> is the same as typing <code>cd /home/{your username}/ros2_ws/src/</code>.</p>"},{"location":"course/part1/#ex4","title":"Exercise 4: Creating your own ROS Package","text":"<p>The <code>ros2</code> Command Line Interface (CLI) that we've been using so far includes a tool to create new ROS packages: <code>ros2 pkg create</code><sup>4</sup>. We'll actually take a slightly different approach to package creation for this course however, to provide us with a little more flexibility and ease of use (particularly for things we'll do later on)<sup>5</sup>. We've therefore created our own ROS 2 Package Template (on GitHub), and we'll walk through how to use this to create new packages now...</p> <ol> <li> <p>Navigate into the <code>ros2_ws/src</code> directory using the Linux <code>cd</code> command (change directory). In TERMINAL 1 enter the following:</p> <pre><code>cd ~/ros2_ws/src/\n</code></pre> </li> <li> <p>From here, use <code>git</code> to clone our ROS 2 Package Template from GitHub:</p> <pre><code>git clone https://github.com/tom-howard/ros2_pkg_template.git\n</code></pre> </li> <li> <p>This package template contains a script called <code>init_pkg.sh</code>, which we'll use to turn the template into our first ROS 2 package. Run the script as follows, to convert the template into a ROS 2 package called <code>part1_pubsub</code>:</p> <pre><code>./ros2_pkg_template/init_pkg.sh part1_pubsub\n</code></pre> <p>As a result of doing this, the <code>ros2_pkg_template</code> directory has now been renamed to <code>part1_pubsub</code>, and various other things within the package have been updated too, to initialise the package with the name that we specified. </p> </li> <li> <p>Navigate into the package directory (using <code>cd</code>):</p> <pre><code>cd part1_pubsub/\n</code></pre> </li> <li> <p><code>tree</code> is a Linux command which shows us the content of the current directory in a nice tree-like format. Use <code>tree</code> now to show the current content of the <code>part1_pubsub</code> directory:</p> <pre><code>tree\n</code></pre> <p>...which should yield:</p> <pre><code>.\n\u251c\u2500\u2500 CMakeLists.txt\n\u251c\u2500\u2500 package.xml\n\u251c\u2500\u2500 part1_pubsub_modules\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tb3_tools.py\n\u2514\u2500\u2500 scripts\n    \u251c\u2500\u2500 basic_velocity_control.py\n    \u2514\u2500\u2500 stop_me.py\n\n3 directories, 6 files\n</code></pre> <ul> <li><code>scripts</code>: is a directory that will contain all the Python Nodes that we'll create (you'll notice a couple in there already).</li> <li> <p><code>part1_pubsub_modules</code>: is a directory that we can use to store Python modules, that we can then import into our main Python nodes</p> <p>(<code>from part1_pubsub_modules.tb3_tools import ...</code>, for example)</p> </li> <li> <p><code>package.xml</code> and <code>CMakeLists.txt</code>: are both files that define our package, and how it must be built (using <code>colcon build</code>). We'll explore these more shortly... </p> </li> </ul> </li> </ol>"},{"location":"course/part1/#ex5","title":"Exercise 5: Creating a publisher node","text":"<ol> <li> <p>From the root of your <code>part1_pubsub</code> package, navigate to the <code>scripts</code> folder using the <code>cd</code> command.</p> <pre><code>cd scripts\n</code></pre> </li> <li> <p><code>touch</code> is a Linux command that we can use to create an empty file. Use this to create an empty file called <code>publisher.py</code>, which we will add content to shortly:</p> <pre><code>touch publisher.py\n</code></pre> </li> <li> <p>Use <code>ls</code> to verify that the file has been created, but use the <code>-l</code> option with this, so that the command provides its output in \"a long listing format\":</p> <pre><code>ls -l\n</code></pre> <p>This should output something similar to the following: </p> <pre><code>-rwxr-xr-x 1 student student 1500 MMM DD HH:MM minimal_node.py\n-rw-r--r-- 1 student student    0 MMM DD HH:MM publisher.py\n-rwxrwxr-x 1 student student  816 MMM DD HH:MM stop_me.py\n</code></pre> <p>This confirms that the <code>publisher.py</code> file exists, and the <code>0</code> on that line indicates that the file is empty (i.e. its current size is 0 bytes), which is what we'd expect.</p> </li> <li> <p>We therefore now need to open the file and add content to it. We'd recommend using Visual Studio Code (VS Code) as an IDE for this course. Launch VS Code and access your ROS 2 environment (how you do this will vary based on how you have ROS installed on your machine).</p> </li> <li> <p>Using the VS Code File Explorer, locate the empty <code>publisher.py</code> file that you have just created (<code>~/ros2_ws/src/part1_pubsub/scripts/</code>) and click on the file to open it in the main editor. </p> </li> <li> <p>The <code>publisher.py</code> code is provided here:</p> <p></p> The <code>publisher.py</code> code<p></p> <p>Take a look at this and be aware of the following additional content on this page too:</p> <ul> <li>Click on the  icons to expand the annotations in the code. It's important that you understand how the code works, so make sure you read these annotations!</li> <li>There's a further section underneath the code called \"Defining Package Dependencies\". Make sure you follow the steps outlined here too!</li> </ul> </li> <li> <p>Once you've reviewed the code take a copy of it, paste it into your <code>publisher.py</code> file and save it. </p> </li> <li> <p>Now, we need to add our <code>publisher.py</code> file as an executable to our package's <code>CMakeLists.txt</code>. This will ensure that it then gets built when we run <code>colcon build</code> (in the next step).</p> <p>In VS Code, open the <code>CMakeLists.txt</code> file that is at the root of your <code>part1_pubsub</code> package directory (<code>ros2_ws/src/part1_pubsub/CMakeLists.txt</code>). Locate the lines (near the bottom of the file) that read:</p> <pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/basic_velocity_control.py\n  scripts/stop_me.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> <p>Add the <code>publisher.py</code> Node as follows:</p> <pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/basic_velocity_control.py\n  scripts/stop_me.py\n  scripts/publisher.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Now, use <code>colcon</code> to build your package.</p> <ol> <li> <p>You MUST run this from the root of your Colcon Workspace (i.e.: <code>~/ros2_ws/</code>), NOT the <code>src</code> directory (<code>~/ros2_ws/src/</code>), so navigate there now using <code>cd</code>:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Then, use the following <code>colcon</code> command to build your package:</p> <pre><code>colcon build --packages-select part1_pubsub --symlink-install\n</code></pre> <p>What do the additional arguments above do?</p> <ul> <li><code>--packages-select</code>: Build only the <code>part1_pubsub</code> package, nothing else (without this <code>colcon</code> would attempt to build every package in the workspace).</li> <li><code>--symlink-install</code>: Ensures that you don't have to re-run <code>colcon build</code> every time you make a change to your package's executables (i.e. your Python nodes in the <code>scripts</code> directory).</li> </ul> </li> <li> <p>Finally, \"re-source\" your <code>bashrc</code><sup>6</sup>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>We should now be able to run this node using the <code>ros2 run</code> command. </p> <p>Remember: <code>ros2 run {package name} {script name}</code>, so:</p> <pre><code>ros2 run part1_pubsub publisher.py\n</code></pre> <p>... Hmm, something not quite right? If you typed the command exactly as above and then tried to run it, you probably just received the following error:</p> <pre><code>No executable found\n</code></pre> <p></p> <p>When we create a file using <code>touch</code> it is given certain permissions by default. Recall the output of the <code>ls -l</code> command that we ran before (click here to go back to this for a reminder):</p> <pre><code>-rw-r--r-- 1 student student   0 MMM DD HH:MM publisher.py\n</code></pre> <p>The first bit tells us about the permissions that are currently assigned to the <code>publisher.py</code> file:  </p> <p></p><code>-rw-r--r--</code> <p></p> <p>This tells us who has permission to do what with this file and (currently) the first bit: <code>-rw-</code>, tells us that we have permission to read or write to it. There is a third option we can set too though, which is the execute permission, and we can set this using the <code>chmod</code> Linux command...</p> </li> <li> <p>Use <code>cd</code> to navigate back to our package's <code>scripts</code> directory (where the <code>publisher.py</code> file is located):</p> <pre><code>cd ~/ros2_ws/src/part1_pubsub/scripts/\n</code></pre> <p>Then run the <code>chmod</code> command as follows to give the <code>publisher.py</code> file execute permissions:</p> <pre><code>chmod +x publisher.py\n</code></pre> </li> <li> <p>Now, run <code>ls -l</code> again to see what has changed:</p> <pre><code>ls -l\n</code></pre> <p>We have now granted permission for the file to be executed too:</p> <pre><code>-rwxr-xr-x 1 student student 1195 MMM DD HH:MM publisher.py\n</code></pre> </li> <li> <p>OK, now use <code>ros2 run</code> again to (hopefully!) run the <code>publisher.py</code> node (remember: <code>ros2 run {package name} {script name}</code>).</p> <p>If you see a message in the terminal similar to the following then the node has been launched successfully:</p> <pre><code>[INFO] [#####] [simple_publisher]: The 'simple_publisher' node is initialised.\n</code></pre> <p>Phew!</p> </li> <li> <p>We can further verify that our publisher node is running using a number of different tools. Try running the following commands in TERMINAL 2:</p> <ol> <li><code>ros2 node list</code>: This will provide a list of all the nodes that are currently active on the system. Verify that the name of our publisher node is visible in this list (it's probably the only item in the list at the moment!)</li> <li><code>ros2 topic list</code>: This will provide a list of the topics that are currently being used by nodes on the system. Verify that the name of the topic that our publisher is publishing messages to (<code>/my_topic</code>) is present within this list.</li> </ol> </li> </ol>"},{"location":"course/part1/#rostopic","title":"Interrogating ROS Topics","text":"<p>So far we have used the <code>ros2 topic</code> ROS command with two additional arguments:</p> <ul> <li><code>list</code>: to provide us with a list of all the topics that are active on our ROS system, and</li> <li><code>info</code>: to provide us with information on a particular topic of interest.</li> </ul> <p>We can find out what other sub-commands are available for us to use with <code>ros2 topic</code> by calling for help! Run the following in TERMINAL 2:</p> <pre><code>ros2 topic --help\n</code></pre> <p>Which should provide us with a list of all the options:</p> <pre><code>Commands:\n  bw     Display bandwidth used by topic\n  delay  Display delay of topic from timestamp in header\n  echo   Output messages from a topic\n  find   Output a list of available topics of a given type\n  hz     Print the average publishing rate to screen\n  info   Print information about a topic\n  list   Output a list of available topics\n  pub    Publish a message to a topic\n  type   Print a topic's type\n\n  Call `ros2 topic &lt;command&gt; -h` for more detailed usage.\n</code></pre> <p>Let's talk about a few of these:</p> <ul> <li> <p><code>ros2 topic hz {topic name}</code> provides information on the frequency (in Hz) at which messages are being published to a topic:</p> <pre><code>ros2 topic hz /my_topic\n</code></pre> <p>This should tell us that our publisher node is publishing messages to the <code>/my_topic</code> topic at (or close to) 1 Hz, which is exactly what we ask for in the <code>publisher.py</code> file (in the <code>__init__</code> part of our <code>Publisher</code> class). Enter Ctrl+C to stop this command.</p> </li> <li> <p><code>ros2 topic echo {topic name}</code> shows the messages being published to a topic:</p> <pre><code>ros2 topic echo /my_topic\n</code></pre> <p>This will provide a live stream of the messages that our <code>publisher.py</code> node is publishing to the <code>/my_topic</code> topic. Enter Ctrl+C to stop this.</p> </li> <li> <p>We can see some additional options for the <code>echo</code> command by viewing the help documentation for this too:</p> <pre><code>ros2 topic echo --help\n</code></pre> <p>From here, for instance, we can learn that if we just wanted to print the first message that was received we could use the <code>--once</code> option, for example:</p> <pre><code>ros2 topic echo /my_topic --once\n</code></pre> </li> </ul>"},{"location":"course/part1/#ex6","title":"Exercise 6: Creating a subscriber node","text":"<p>To illustrate how information can be passed from one node to another (via topics and messages) we'll now create another node to subscribe to the topic that our publisher node is broadcasting messages to.</p> <ol> <li> <p>In TERMINAL 2 use the filesystem commands that were introduced earlier (<code>cd</code>, <code>ls</code>, etc.) to navigate to the <code>scripts</code> folder of your <code>part1_pubsub</code> package.</p> </li> <li> <p>Use the same procedure as before to create a new empty Python file called <code>subscriber.py</code> and remember to make it executable! </p> </li> <li> <p>Then, open this newly created <code>subscriber.py</code> file in VS Code.</p> </li> <li> <p>The code for the <code>subscriber.py</code> file is provided here:</p> <p></p> The <code>subscriber.py</code> code<p></p> <p>Once again, it's important that you understand how this code works, so make sure you read the code annotations! </p> <p>Fill in the <code>{BLANK}</code>!</p> <p>This code won't work out-of-the-box! Look out for a <code>{BLANK}</code>, which is a prompt for you to replace this text with something else! </p> </li> <li> <p>We now need to add this as an additional package executable. </p> <p>Open up the <code>CMakeLists.txt</code> file at the root of your <code>part1_pubsub</code> package directory again, head back to the <code># Install Python executables</code> section and add the <code>subscriber.py</code> file as illustrated below:</p> <pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/basic_velocity_control.py\n  scripts/stop_me.py\n  scripts/publisher.py\n  scripts/subscriber.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Now we need to <code>colcon build</code> again.</p> <ol> <li> <p>Make sure you're at the root of the Colcon Workspace:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Run <code>colcon build</code> on only the <code>part1_pubsub</code> package:</p> <pre><code>colcon build --packages-select part1_pubsub --symlink-install\n</code></pre> </li> <li> <p>And then re-source the <code>bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Use <code>ros2 run</code> (in TERMINAL 2) to execute your newly created <code>subscriber.py</code> node (remember: <code>ros2 run {package name} {script name}</code>). If your publisher and subscriber nodes are working correctly you should see an output like this:</p> <p></p> <p></p> </li> <li> <p>Interrogate your ROS network:</p> <ol> <li> <p>As before, we can find out what nodes are running on our system by using the <code>ros2 node list</code> command. Run this in TERMINAL 3, you should see both your publisher and subscriber nodes listed there.</p> </li> <li> <p>Use the <code>ros2 topic</code> command to list all the topics that are available on the network. You should see <code>/my_topic</code> listed there.</p> </li> <li> <p>Use the <code>ros2 topic</code> command again to find more info on <code>my_topic</code>. </p> </li> <li> <p>Use the <code>ros2 interface</code> command to show you what type of data is being sent between the two nodes.</p> </li> </ol> </li> <li> <p>Finally, close down your publisher and subscriber nodes by entering Ctrl+C in the terminals where they are running (should be 1 &amp; 2).</p> </li> </ol>"},{"location":"course/part1/#ex7","title":"Exercise 7: Defining our own message","text":"<p>We've just created a publisher and subscriber that were able to communicate with one another via a topic. </p> <p></p> <p>The data that the publisher was sending to the topic was very simple: a <code>example_interfaces/msg/String</code> type message.</p> <pre><code>ros2 topic info /my_topic\n\nType: example_interfaces/msg/String\nPublisher count: 1\nSubscription count: 1\n</code></pre> <p>This message just has one field called <code>data</code> of the type <code>string</code>:</p> <pre><code>ros2 interface show ros2 topic info -t /my_topic\n\nstring data\n</code></pre> <p>ROS messages will generally be more complex than this, typically containing several fields in a single message. We'll define our own custom message now, this time with two fields, so you can see how things work with slightly more complex data structures. </p> <ol> <li> <p>Message interfaces must be defined within a <code>msg</code> folder at the root of our package directory, so let's create this folder now in TERMINAL 1:</p> <ol> <li> <p>First, navigate into your package:</p> <pre><code>cd ~/ros2_ws/src/part1_pubsub\n</code></pre> </li> <li> <p>Then use <code>mkdir</code> to make a new directory:</p> <pre><code>mkdir msg\n</code></pre> </li> </ol> </li> <li> <p>We'll create a message called <code>Example</code>, and to do this we'll need to create a new file called <code>Example.msg</code> inside the <code>msg</code> folder:</p> <pre><code>touch msg/Example.msg\n</code></pre> </li> <li> <p>To define the data structure of this message, we now need to open up the file and add the following content:</p> Example.msg<pre><code>string info\nint32 time\n</code></pre> <p>The message will therefore have two fields:</p> <p></p><p></p> # Field Name Data Type 1 <code>info</code> <code>string</code> 2 <code>time</code> <code>int32</code> <p></p><p></p> <p>We can give our fields any names that we want, but the data types must be either built-in-types or other pre-existing ROS interfaces.</p> </li> <li> <p>We now need to declare this message in our package's <code>CMakeLists.txt</code> file, so that the necessary Python code can be created (by <code>colcon build</code>) to allow us to import this message into our own Python files.</p> <p>Add the following lines to your <code>part1_pubsub/CMakeLists.txt</code> file, above the <code>ament_package()</code> line:</p> CMakeLists.txt<pre><code>find_package(rosidl_default_generators REQUIRED)\nrosidl_generate_interfaces(${PROJECT_NAME}\n  \"msg/Example.msg\" \n)\n</code></pre> </li> <li> <p>We also need to modify our <code>package.xml</code> file. Add the following lines to this one, just above the <code>&lt;export&gt;</code> line:</p> package.xml<pre><code>&lt;buildtool_depend&gt;rosidl_default_generators&lt;/buildtool_depend&gt;\n&lt;exec_depend&gt;rosidl_default_runtime&lt;/exec_depend&gt;\n&lt;member_of_group&gt;rosidl_interface_packages&lt;/member_of_group&gt;\n</code></pre> </li> <li> <p>We can now use Colcon to generate the necessary source code for the message:</p> <ol> <li> <p>First, make sure you're in the root of the ROS 2 Workspace:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Then run <code>colcon build</code>:</p> <pre><code>colcon build --packages-select part1_pubsub --symlink-install \n</code></pre> </li> <li> <p>And finally re-source the <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>We can now verify that this worked with some more <code>ros2</code> command line tools:</p> <ol> <li> <p>First, list all the ROS messages that are available to us on our system:</p> <pre><code>ros2 interface list -m\n</code></pre> <p>Scroll through this list and see if you can find our message in there (it'll be listed as <code>part1_pubsub/msg/Example</code>)</p> </li> <li> <p>Next, show the data structure of the interface:</p> <pre><code>ros2 interface show part1_pubsub/msg/Example\n</code></pre> <p>This should match with how we defined it in our <code>part1_pubsub/msg/Example.msg</code> file.</p> </li> </ol> </li> </ol>"},{"location":"course/part1/#ex8","title":"Exercise 8: Using a custom ROS Message","text":"<ol> <li> <p>Create a copy of the <code>publisher.py</code> file from Exercise 5. Let's do this from the command line too (in TERMINAL 1):</p> <ol> <li> <p>Navigate into your package's <code>scripts</code> folder:</p> <pre><code>cd ~/ros2_ws/src/part1_pubsub/scripts\n</code></pre> </li> <li> <p>And use the <code>cp</code> command to make a copy of the <code>publisher.py</code> file and call this new file <code>custom_msg_publisher.py</code>:</p> <pre><code>cp publisher.py custom_msg_publisher.py\n</code></pre> </li> <li> <p>Let's create a copy of the <code>subscriber.py</code> file too, while we're here:</p> <pre><code>cp subscriber.py custom_msg_subscriber.py\n</code></pre> </li> </ol> </li> <li> <p>Declare these two new files as additional executables in our <code>CMakeLists.txt</code>:</p> CMakeLists.txt<pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/basic_velocity_control.py\n  scripts/stop_me.py\n  scripts/publisher.py\n  scripts/subscriber.py\n  scripts/custom_msg_publisher.py  # ADD THIS \n  scripts/custom_msg_subscriber.py # AND THIS\nDESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Run Colcon again (last time now!):</p> <ol> <li>First:     <pre><code>cd ~/ros2_ws\n</code></pre></li> <li>Then:     <pre><code>colcon build --packages-select part1_pubsub --symlink-install\n</code></pre></li> <li>And finally:     <pre><code>source ~/.bashrc\n</code></pre></li> </ol> </li> <li> <p>Now modify your <code>custom_msg_publisher.py</code> file according to the code provided below:</p> <p></p> The <code>custom_msg_publisher.py</code> code<p></p> </li> <li> <p>Final Task:</p> <p>Modify the <code>custom_msg_subscriber.py</code> node now to accommodate the new interface messages that are being published to <code>/my_topic</code>. </p> </li> </ol>"},{"location":"course/part1/#wrapping-up","title":"Wrapping Up","text":"<p>In this session we've covered the basics of ROS, and learnt about some key concepts such as Packages; Nodes; and how to send data across a ROS Network using Topics, Messages, and the Publisher-Subscriber Communication Method.</p> <p>We've learnt how to use some key <code>ros2</code> commands:  </p> <ul> <li><code>launch</code>: to launch multiple ROS Nodes via launch files.</li> <li><code>run</code>: to run executables within a ROS package.</li> <li><code>node</code>: to display information about active ROS Nodes.</li> <li><code>topic</code>: to display information about active ROS topics.</li> <li><code>interface</code>: to display information about all ROS Interfaces that are available to use in a ROS application.</li> </ul> <p>We have also learnt how to work in the Linux Terminal and navigate a Linux filesystem using key commands such as:</p> <ul> <li><code>ls</code>: lists the files in the current directory.</li> <li><code>cd</code>: change directory to move around the file system.</li> <li><code>mkdir</code>: make a new directory (<code>mkdir {new_folder}</code>).</li> <li><code>chmod</code>: modify file permissions (i.e. to add execute permissions to a file for all users: <code>chmod +x {file}</code>).</li> <li><code>touch</code>: create a file without any content.</li> </ul> <p>In addition to this we've also learnt how to create a ROS 2 package, and how to create simple Python nodes that can publish and subscribe to topics on a ROS network. </p> <p>We've worked with pre-made ROS messages to do this and also created our own custom message interface to offer more advanced functionality.</p>"},{"location":"course/part1/#backup","title":"WSL-ROS2 Managed Desktop Users: Save your work!","text":"<p>Remember, the work you have done in the WSL-ROS2 environment during this session will not be preserved for future sessions or across different University machines automatically! To save the work you have done here today you should now run the following script in any idle WSL-ROS2 Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will export your home directory to your University <code>U:\\</code> Drive, allowing you to restore it on another managed desktop machine the next time you fire up WSL-ROS2.  </p> <ol> <li> <p>What is a ROS 2 Workspace? You can find out more here.\u00a0\u21a9</p> </li> <li> <p>What is Colcon? Find out more here.\u00a0\u21a9</p> </li> <li> <p><code>ros2_ws</code> is a common name used for a ROS 2 workspace in many online tutorials, the name doesn't really matter, it could be called anything. You can learn more about ROS 2 Workspaces here.\u00a0\u21a9</p> </li> <li> <p>You can learn more about all this from the Official ROS 2 Tutorials (if you're interested).\u00a0\u21a9</p> </li> <li> <p>The approach we take is based on this tutorial (courtesy of the Robotics Backend), so feel free to look at this if you'd like to find out more.\u00a0\u21a9</p> </li> <li> <p>What does <code>source ~/.bashrc</code> do? See here for an explanation.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/part2/","title":"Part 2: Odometry & Navigation","text":""},{"location":"course/part2/#introduction","title":"Introduction","text":"<p> Exercises: 6 Estimated Completion Time: 3 hours Difficulty Level: Intermediate  </p>"},{"location":"course/part2/#aims","title":"Aims","text":"<p>In Part 2 we'll learn how to control a ROS robot's position and velocity from both the command line and through ROS Nodes. We'll also learn how to interpret the data that allows us to monitor a robot's position in its physical environment (odometry).  The things covered here form the basis of robot navigation in ROS, from simple open-loop methods to more advanced closed-loop control (both of which we will explore).</p>"},{"location":"course/part2/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Interpret the Odometry data published by a ROS Robot and identify the parts of these interface messages that are relevant to a 2-wheeled differential drive robot (such as the TurtleBot3 Waffle).</li> <li>Develop Python nodes to obtain Odometry data from an active ROS network and translate this into useful information about a robot's pose in a convenient, human-readable way.</li> <li>Implement open-loop velocity control of a robot using ROS command-line tools.</li> <li>Develop Python nodes that use open-loop velocity control methods to make a robot follow a pre-defined motion path.</li> <li>Combine both publisher &amp; subscriber communication methods into a single Python node to implement closed-loop (odometry-based) velocity control of a robot.</li> <li>Explain the limitations of Odometry-based motion control methods. </li> </ol>"},{"location":"course/part2/#quick-links","title":"Quick Links","text":""},{"location":"course/part2/#exercises","title":"Exercises","text":"<ul> <li>Exercise 1: Exploring Odometry Data</li> <li>Exercise 2: Creating a Python Node to Process Odometry Data</li> <li>Exercise 3: Controlling Velocity with the ROS 2 CLI</li> <li>Exercise 4: Creating a Python Node to Make a Robot Move in a circle</li> <li>Exercise 5: Implementing a Shutdown Procedure</li> <li>Exercise 6: Making our Robot Follow a Square Motion Path</li> </ul>"},{"location":"course/part2/#additional-resources","title":"Additional Resources","text":"<ul> <li>An Odometry Subscriber Node</li> <li>A Simple Velocity Control Node (Move Circle)</li> <li>Odometry-based Navigation (Move Square)</li> </ul>"},{"location":"course/part2/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch your ROS 2 Environment</p> <p>If you haven't done so already, launch your ROS environment now:</p> <ol> <li>WSL-ROS2 on a university managed desktop: follow the instructions here to launch it.</li> <li>Running WSL-ROS2 on your own machine: launch the Windows Terminal to access a WSL-ROS2 terminal instance.</li> <li>Docker Users: follow the instructions.</li> </ol> <p>You should now have access to ROS 2 via a Linux terminal instance. We'll refer to this terminal instance as TERMINAL 1.</p> <p>Step 2: Restore your work (WSL-ROS2 Managed Desktop Users ONLY)</p> <p>Remember that any work that you do within the WSL-ROS2 Environment will not be preserved between sessions or across different University computers. At the end of Part 1 you should have run the <code>wsl_ros</code> tool to back up your home directory to your University <code>U:\\</code> Drive. Once WSL-ROS2 is up and running, you should be prompted to restore this:</p> <pre><code>It looks like you already have a backup from a previous session:\n  U:\\wsl-ros\\ros2-backup-XXX.tar.gz\nDo you want to restore this now? [y/n]\n</code></pre> <p>Enter Y+Enter to restore your work from last time. You can also restore your work at any time using the following command:</p> <pre><code>wsl_ros restore\n</code></pre> <p>Step 3: Launch VS Code </p> <p>It's also worth launching VS Code now, so that it's ready to go for when you need it later on. </p> WSL Users... <p>It's important to launch VS Code within your WSL environment using the \"WSL\" extension. Always remember to check for this:</p> <p></p> <p></p> <p>Step 4: Make Sure The Course Repo is Up-To-Date</p> <p></p> <p>In Part 1 you should have downloaded and installed The Course Repo into your ROS environment. If you haven't done this yet then go back and do it now. If you have already done it, then it's worth just making sure it's all up-to-date, so run the following command in TERMINAL 1 now to do so:</p> <pre><code>cd ~/ros2_ws/src/tuos_ros/ &amp;&amp; git pull\n</code></pre> <p>Then build with Colcon: </p> <pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build --packages-up-to tuos_ros\n</code></pre> <p>And finally, re-source your environment:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Warning</p> <p>If you have any other terminal instances open, then you'll need run <code>source ~/.bashrc</code> in these too, in order for any changes made by the Colcon build process to propagate through to these as well.</p> <p>Step 5: Launch a Waffle Simulation</p> <p>In TERMINAL 1 enter the following command to launch a simulation of a TurtleBot3 Waffle in an empty world:  </p> <pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre> <p>A Gazebo simulation window should open and within this you should see a TurtleBot3 Waffle in empty space:</p> <p></p>"},{"location":"course/part2/#ros-topics-and-interfaces-from-part-1","title":"ROS Topics and Interfaces (from Part 1)","text":"<p>In Part 1 we learnt about ROS Topics, and about how the <code>teleop_keyboard</code> node could be used to publish messages to a particular topic in order to control the velocity of the robot (and thus change its position).</p> <p>Questions</p> <ol> <li>Which topic is used to control the velocity of the robot?</li> <li>What interface does this topic use?</li> </ol> <p>Return here if you need a reminder on how to find the answers to these questions.</p> <p>Recall that Topics are key to making things happen on a robot: data is passed between the Nodes on a ROS network via Topics using standardised data structures called Message Interfaces, allowing each of these nodes to make decisions and perform necessary tasks to bring the robot to life.</p> <p>Having investigated the <code>/cmd_vel</code> topic in Part 1, let's have a look at another topic now: <code>/odom</code>, and consider what the information here means, and what it's used for.</p>"},{"location":"course/part2/#odometry","title":"Odometry","text":"<p>Odometry is a process of monitoring a robot's position and orientation in an environment, which (as we'll learn) is essential for robot navigation. The position and orientation of a robot is referred to as its pose. A robot's pose is 3-dimensional, and is therefore defined in terms of three \"Principal Axes\": <code>X</code>, <code>Y</code> and <code>Z</code>. In the context of our TurtleBot3 Waffles, these axes and the motion about them are defined as follows:</p> <p></p> <p></p> <p>Not all the above positions and orientations apply to our Waffles, and we'll explore this further below.</p>"},{"location":"course/part2/#odometry-in-action","title":"Odometry In Action","text":"<p>Recall from Part 1 the command that we can use to list all the topics that are available on our robot:</p> <pre><code>ros2 topic list\n</code></pre> <p>You should see <code>/odom</code> in this list, which is where our robot's Odometry data is published. Run the <code>ros2 topic</code> command again, but this time with an additional <code>-t</code> option:</p> <pre><code>ros2 topic list -t\n</code></pre> <p>Look for <code>/odom</code> again in the list, and you will now notice that the interface definition is provided in square brackets alongside the topic name:</p> <pre><code>/odom [nav_msgs/msg/Odometry]\n</code></pre> <p>Questions</p> <ol> <li>What package does this interface belong to?</li> <li>What type of interface is it?</li> <li>What is its name?</li> </ol> <p>See here for a reminder on how to interpret an interface definition.</p> <p>Having established the data structure, let's explore the actual data now, using <code>rqt</code>.</p>"},{"location":"course/part2/#ex1","title":"Exercise 1: Exploring Odometry Data","text":"<ol> <li> <p>In a new terminal instance (TERMINAL 2) use the following command to launch the RQT Topic Monitor:</p> <pre><code>ros2 run rqt_topic rqt_topic \n</code></pre> <p>Topic Monitor should launch with a list of active topics matching the topic list from the <code>ros2 topic list</code> command that you ran earlier.</p> </li> <li> <p>Check the box next to <code>/odom</code> and click the arrow next to it to expand the topic and reveal four base fields.</p> </li> <li> <p>Expand the <code>pose</code> field, and then the further <code>pose</code> field within that. This should reveal two further fields: <code>position</code> and <code>orientation</code>. </p> <p>Expand both of these to reveal the data being published to the three position (<code>x</code>, <code>y</code> and <code>z</code>) and four orientation (<code>x</code>, <code>y</code>, <code>z</code> and <code>w</code>) values.</p> <p></p> <p></p> </li> <li> <p>Next, launch a new terminal instance, we'll call this one TERMINAL 3. Arrange this next to the <code>rqt</code> window, so that you can see them both side-by-side.</p> </li> <li> <p>In TERMINAL 3 launch the <code>teleop_keyboard</code> node as you did in Part 1: </p> <pre><code>ros2 run turtlebot3_teleop teleop_keyboard\n</code></pre> </li> <li> <p>Enter A a couple of times to make the robot rotate on the spot. Observe how the odometry data changes in Topic Monitor.</p> <p>Question</p> <p>Which <code>pose</code> fields are changing?</p> </li> <li> <p>Now press the S key to halt the robot, then press W a couple of times to make the robot drive forwards.</p> <p>Question</p> <p>Which <code>pose</code> fields are changing now? How does this relate to the position of the robot in the simulated world?</p> </li> <li> <p>Now press D a couple of times and your robot should start to move in a circle.</p> <p>Question</p> <p>What's happening with the <code>pose</code> data now? How is this data changing as your robot moves in a circular path.</p> </li> <li> <p>Press S in TERMINAL 3 to stop the robot (but leave the <code>teleop_keyboard</code> node running).  Then, press Ctrl+C in TERMINAL 2 to close down <code>rqt</code>. </p> </li> <li> <p>Let's look at the Odometry data differently now. With the robot stationary, use <code>ros2 run</code> (in TERMINAL 2) to run a Python node from the <code>tuos_examples</code> package: </p> <pre><code>ros2 run tuos_examples robot_pose.py\n</code></pre> </li> <li> <p>Now (using the <code>teleop_keyboard</code> node in TERMINAL 3) drive your robot around again, keeping an eye on the outputs that are being printed by the <code>robot_pose.py</code> node in TERMINAL 2 as you do so.</p> <p>The output of the <code>robot_pose.py</code> node shows you how the robot's position and orientation (i.e. \"pose\") are changing in real-time as you move the robot around. The <code>\"initial\"</code> column tells us the robot's pose when the node was first launched, and the <code>\"current\"</code> column show us what its pose currently is. The <code>\"delta\"</code> column then shows the difference between the two.</p> <p>Question</p> <p>Which pose parameters haven't changed, and is this what you would expect (considering the robot's principal axes, as illustrated above)?</p> </li> <li> <p>Press Ctrl+C in TERMINAL 2 and TERMINAL 3, to stop the <code>robot_pose.py</code> and <code>teleop_keyboard</code> nodes. </p> </li> </ol>"},{"location":"course/part2/#odometry-explained","title":"Odometry Explained","text":"<p>Hopefully you're starting to understand what Odometry is now, but let's dig a little deeper using some key ROS command line tools again. IN TERMINAL 2:</p> <pre><code>ros2 topic info /odom\n</code></pre> <p>This provides information about the interface used by this topic:</p> <pre><code>Type: nav_msgs/msg/Odometry\n</code></pre> <p>We can find out more about this interface using the <code>ros2 interface show</code> command:</p> <pre><code>ros2 interface show nav_msgs/msg/Odometry\n</code></pre> <p>Look down the far left-hand side to identify the four base fields of the interface (the fields that are not indented):</p> <p></p> <p></p><p></p> # Field Name Field Type 1 <code>header</code> <code>std_msgs/Header</code> 2 <code>child_frame_id</code> <code>string</code> 3 <code>pose</code> <code>geometry_msgs/PoseWithCovariance</code> 4 <code>twist</code> <code>geometry_msgs/TwistWithCovariance</code> <p></p><p></p> <p>We saw all these in <code>rqt</code> earlier. As before, its item 3 that's of most interest to us...</p>"},{"location":"course/part2/#pose","title":"Pose","text":"<pre><code># Estimated pose that is typically relative to a fixed world frame.\ngeometry_msgs/PoseWithCovariance pose\n        Pose pose\n                Point position\n                        float64 x\n                        float64 y\n                        float64 z\n                Quaternion orientation\n                        float64 x\n                        float64 y\n                        float64 z\n                        float64 w\n        float64[36] covariance\n</code></pre> <p>Within the <code>pose</code> base field we have two subfields: <code>pose</code> and <code>covariance</code>:</p> <p></p><p></p> # Field Name Field Type 1 <code>pose</code> <code>Pose</code> 2 <code>covariance</code> <code>float64[36]</code> <p></p><p></p> <p>It's the <code>pose</code> subfield that we're most interested in here, which contains two further subfields called <code>position</code> and <code>orientation</code>: </p> <p></p><p></p> # Field Name Field Type 1 <code>position</code> <code>Point</code> 2 <code>orientation</code> <code>Quaternion</code> <p></p><p></p> <ol> <li> <p><code>position</code></p> <p>Tells us where our robot is located in 3-dimensional space. This is expressed in units of meters.</p> </li> <li> <p><code>orientation</code></p> <p>Tells us which way our robot is pointing in its environment. This is expressed in units of Quaternions, which is a mathematically convenient way to store data related to a robot's orientation (it's a bit hard for us humans to understand and visualise this though, so we'll talk about how to convert it to a different format later).</p> </li> </ol> <p>Pose is defined relative to an arbitrary reference point, typically where the robot was when it was turned on, or the origin of a simulated world. On our real TurtleBot3 Waffle robots, it is determined from:</p> <ul> <li>Data from the Inertial Measurement Unit (IMU) on the OpenCR board</li> <li>Data from both the left and right wheel encoders</li> <li>A kinematic model of the robot</li> </ul> <p>All the above information can then be used to calculate (and keep track of) the distance travelled by the robot from its pre-defined reference point using a process called \"dead-reckoning.\"</p>"},{"location":"course/part2/#what-are-quaternions","title":"What are Quaternions?","text":"<p>Quaternions represent the orientation of something in 3 dimensional space<sup>1</sup>, as we can observe from the structure of the <code>nav_msgs/msg/Odometry</code> ROS interface, there are four fields associated with this:</p> <pre><code>Quaternion orientation\n        float64 x\n        float64 y\n        float64 z\n        float64 w\n</code></pre> <p>For us, it's easier to think about the orientation of our robot in a \"Euler Angle\" representation, which tell us the degree of rotation about the three principal axes (as discussed above):</p> <ul> <li>\\(\\theta_{x}\\): The angular position about the X-axis, aka \"Roll\"</li> <li>\\(\\theta_{y}\\): The angular position about the Y-axis, aka \"Pitch\"</li> <li>\\(\\theta_{z}\\): The angular position about the Z-axis, aka \"Yaw\"</li> </ul> <p>Fortunately, the maths involved in converting between these two orientation formats is fairly straight forward (see here).</p>"},{"location":"course/part2/#which-pose-values-apply-to-our-waffles","title":"Which Pose Values Apply to our Waffles?","text":"<p>Referring back to the three principal axes from earlier: </p> <p></p> <p>You can also see here that our TurtleBot3 has two motors that allow it to move. As a result of this, it can only move in a 2D plane and so its pose can be fully represented by just 3 odometry terms in total: </p> <ul> <li>\\(x\\) &amp; \\(y\\): the 2D coordinates of the robot in the X-Y plane</li> <li>\\(\\theta_{z}\\): the angle of the robot about the Z-axis (yaw)</li> </ul> <p>(unfortunately, it can't fly!)</p>"},{"location":"course/part2/#odometry-data-as-a-feedback-signal","title":"Odometry Data as a Feedback Signal","text":"<p>Odometry data can be really useful for robot navigation, allowing us to keep track of where a robot is, how it's moving and how to get back to where we started. We therefore need to know how to use this data effectively within our Python nodes, and we'll explore this now.</p>"},{"location":"course/part2/#ex2","title":"Exercise 2: Creating a Python Node to Process Odometry Data","text":"<p>In Part 1 we learnt how to create a package and build simple Python nodes to publish and subscribe to messages on a topic. In this exercise we'll build a new subscriber node, much like we did previously, but this one will subscribe to the <code>/odom</code> topic that we've been talking about above. We'll also create a new package called <code>part2_navigation</code> for this node to live in!</p> <ol> <li> <p>First, head to the <code>src</code> directory of your ROS 2 workspace in TERMINAL 2:</p> <pre><code>cd ~/ros2_ws/src/\n</code></pre> </li> <li> <p>Clone the ROS 2 Package Template:</p> <pre><code>git clone https://github.com/tom-howard/ros2_pkg_template.git\n</code></pre> </li> <li> <p>Run the <code>init_pkg.sh</code> script within this to initalise that package with the name \"part2_navigation\":</p> <pre><code>./ros2_pkg_template/init_pkg.sh part2_navigation\n</code></pre> </li> <li> <p>Then navigate into the new package using <code>cd</code>:</p> <pre><code>cd ./part2_navigation/\n</code></pre> </li> <li> <p>The subscriber that we will build here will have a similar structure to the subscriber that we built in Part 1. As a starting point, copy across the <code>subscriber.py</code> file from your <code>part1_pubsub</code> package using the <code>cp</code> command (i.e. copy):</p> <pre><code>cp ../part1_pubsub/scripts/subscriber.py ./scripts/odom_subscriber.py\n</code></pre> Info: Copying files in a terminal <p>When using the <code>cp</code> command to copy things, we need to provide two key bits of information (at least): </p> <pre><code>cp SOURCE DEST\n</code></pre> <p>i.e.: copy the file <code>SOURCE</code> to the destination <code>DEST</code>.</p> <p>Remember that we are located in our <code>part2_navigation</code> package root folder when we run this, and the file paths that we are using here are all relative to that location.</p> <p>As such, <code>..</code> means \"go back one directory,\" so when defining the <code>SOURCE</code> file that we want to copy, we're telling <code>cp</code> to go out of the <code>part2_navigation</code> directory (back to <code>~/ros2_ws/src/</code>), and then go into the <code>part1_pubsub</code> directory from there (and onwards into <code>scripts</code>).</p> <p><code>.</code> means \"this current directory,\" so when defining where we want the <code>subscriber.py</code> to be copied to (<code>DEST</code>), we're telling <code>cp</code> to start from where we currently are in the filesystem (i.e. <code>~/ros2_ws/src/part2_navigation/</code>) and copy it into the <code>scripts</code> directory from there (whilst also renaming it to <code>odom_subscriber.py</code>).</p> </li> <li> <p>Next, head to the following page for step-by-step instructions on how to build the odometry subscriber:</p> <p></p> Building the <code>odom_subscriber.py</code> node <p></p> </li> <li> <p>Now, declare the <code>odom_subscriber.py</code> node as an executable in the <code>CMakeLists.txt</code>:</p> CMakeLists.txt<pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/basic_velocity_control.py\n  scripts/stop_me.py\n  scripts/odom_subscriber.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Head back to the terminal and use Colcon to build the package (including the new <code>odom_subscriber.py</code> node). This is a three-step process, that you must always follow:</p> <ol> <li> <p>Navigate to the root of the ROS 2 workspace:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Build your package using <code>colcon</code>:</p> <pre><code>colcon build --packages-select part2_navigation --symlink-install\n</code></pre> </li> <li> <p>And finally, re-source the <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Now we're ready to run this! Do so using <code>ros2 run</code> and see what it does:</p> <pre><code>ros2 run part2_navigation odom_subscriber.py\n</code></pre> </li> <li> <p>Having followed all the steps, the output from your node should be similar to that shown below:</p> <p></p> <p></p> </li> <li> <p>Observe how the output (the formatted odometry data) changes while you move the robot around using the <code>teleop_keyboard</code> node in TERMINAL 3.</p> </li> <li>Stop your <code>odom_subscriber.py</code> node in TERMINAL 2 and the <code>teleop_keyboard</code> node in TERMINAL 3 by entering Ctrl+C in each of the terminals.</li> </ol>"},{"location":"course/part2/#velocity","title":"Basic Navigation: Open-loop Velocity Control","text":"<p>In order to change our robot's pose, we need to apply velocity to make it move. We learnt about this in Part 1, but let's look at it all in a bit more detail now.</p> <p>We know that we can use the <code>/cmd_vel</code> topic to publish velocity commands to our robot. Let's remind ourselves how these velocity commands must be structured:</p> <pre><code>ros2 topic info /cmd_vel\n</code></pre> <p>This tells us that the data that is transmitted on the <code>/cmd_vel</code> topic is of the <code>geometry_msgs/msg/TwistStamped</code> interface type.  </p> <p>We also learnt how to find out more about this particular interface (using the <code>ros2 interface show</code> command): </p> <pre><code>ros2 interface show geometry_msgs/msg/TwistStamped\n</code></pre> <p></p> <pre><code>std_msgs/Header header\n        builtin_interfaces/Time stamp\n                int32 sec\n                uint32 nanosec\n        string frame_id\nTwist twist\n        Vector3  linear\n                float64 x\n                float64 y\n                float64 z\n        Vector3  angular\n                float64 x\n                float64 y\n                float64 z\n</code></pre> <p>There are two base fields in this data structure:</p> <p></p><p></p> # Field Name Data Type 1 <code>header</code> <code>std_msgs/Header</code> 2 <code>twist</code> <code>Twist</code> <p></p><p></p> <p>Each of these base fields are comprised of further subfields. It's the <code>twist</code> field that's of most interest to us, and this comprises two further subfields:</p> <p></p><p></p> # Field Name Data Type 1 <code>linear</code> <code>Vector3</code> 2 <code>angular</code> <code>Vector3</code> <p></p><p></p> <p>Each of these contains 3 further subfields: <code>x</code>, <code>y</code> and <code>z</code>:</p> <p></p><p></p> # Field Name Data Type 1 <code>x</code> <code>float64</code> 2 <code>y</code> <code>float64</code> 3 <code>z</code> <code>float64</code> <p></p><p></p>"},{"location":"course/part2/#velocity-commands","title":"Velocity Commands","text":"<p>There are therefore six velocity fields that we can assign values to when sending velocity commands to a ROS robot: two velocity types, each with three velocity components: </p> <p></p><p></p> Velocity Type Component 1 Component 2 Component 3 <code>linear</code> <code>x</code> <code>y</code> <code>z</code> <code>angular</code> <code>x</code> <code>y</code> <code>z</code> <p></p><p></p> <p>These relate to a robot's six degrees of freedom (DOFs), and velocity commands are therefore formatted to give a ROS Programmer the ability to ask a robot to move in any one of its six DOFs. </p> <p></p><p></p> Component (Axis) Linear Velocity Angular Velocity X \"Forwards/Backwards\" \"Roll\" Y \"Left/Right\" \"Pitch\" Z \"Up/Down\" \"Yaw\" <p></p><p></p>"},{"location":"course/part2/#the-degrees-of-freedom-of-our-waffles","title":"The Degrees of Freedom of our Waffles","text":"<p>Recall (again) our robot's \"Principal Axes\" and the motion about them:</p> <p></p> <p>As discussed above, our Waffles only have two motors. These two motors can be controlled independently (in what is known as a \"differential drive\" configuration), which ultimately provides it with a total of two degrees of freedom overall, as highlighted below.</p> <p></p> <p>When issuing velocity commands to our Waffles therefore, only two (of the six) velocity command fields are applicable: linear velocity in the x-axis (Forwards/Backwards) and angular velocity about the z-axis (Yaw).</p> <p></p><p></p> Principal Axis Linear Velocity Angular Velocity X \"Forwards/Backwards\" \"Roll\" Y \"Left/Right\" \"Pitch\" Z \"Up/Down\" \"Yaw\" <p></p><p></p> <p></p> <p>Maximum Velocity Limits</p> <p>Keep in mind (while we're on the subject of velocity) that our TurtleBot3 Waffles have maximum velocity limits:</p> <p></p><p></p> Velocity Component Upper Limit Units Linear (in the X axis) 0.26 m/s Angular (about the Z axis) 1.82 rad/s <p></p><p></p>"},{"location":"course/part2/#ex3","title":"Exercise 3: Controlling Velocity with the ROS 2 CLI","text":"<p>Warning</p> <p>Make sure that you've stopped the <code>teleop_keyboard</code> node before starting this exercise!</p> <p>We can use the <code>ros2 topic pub</code> command to publish data to a topic from a terminal by using the command in the following way:</p> <pre><code>ros2 topic pub {topic_name} {interface_type} {data}\n</code></pre> <p>As discussed above, the <code>/cmd_vel</code> topic is expecting interface messages containing linear and angular velocity data, each with <code>x</code>, <code>y</code> and <code>z</code> values associated with them. We can compose these messages in a terminal and publish them with the <code>ros2 topic pub</code> command, provided we take care to format the messages correctly to conform with the <code>geometry_msgs/msg/TwistStamped</code> data structure.</p> <ol> <li> <p>In TERMINAL 3 enter the following, this will end up being quite a long command, so let's break it down a little:</p> <ol> <li> <p>Start with the desired subcommand of <code>ros2 topic</code>:</p> <pre><code>ros2 topic pub\n</code></pre> </li> <li> <p>Next comes the name of the topic that we want to publish to: </p> <pre><code>ros2 topic pub /cmd_vel\n</code></pre> </li> <li> <p>Next, we define the interface type that this topic uses. Start typing this and then enter the Tab key where shown, and it should autocomplete for you:</p> <pre><code>ros2 topic pub /cmd_vel geom[TAB]\n</code></pre> <p>Which should autocomplete the interface type for you:</p> <pre><code>ros2 topic pub /cmd_vel geometry_msgs/msg/TwistStamped\n</code></pre> <p>Tip</p> <p>You can use Tab to autocomplete lots of terminal commands, experiment with it - it'll save you lots of time! </p> </li> <li> <p>Finally, provide the values to be published, which must be formatted according to the interface definition (as we established earlier):</p> <pre><code>ros2 topic pub /cmd_vel geometry_msgs/msg/TwistStamped \\\n\"{header: auto, \\\n  twist: { \\\n    linear: {x: 0.0, y: 0.0, z: 0.0}, \\\n    angular: {x: 0.0, y: 0.0, z: 0.0} \\\n  } \\\n}\"\n</code></pre> </li> </ol> </li> <li> <p>Scroll back through the message using the Left key on your keyboard and then edit the values of the various fields, as appropriate.</p> <p>First, define some values that would make the robot rotate on the spot.  </p> </li> <li> <p>Enter Ctrl+C in TERMINAL 3 to stop the message from being published.</p> <p>Notice that the robot carries on moving even when you stop the <code>ros2 topic pub</code> process...</p> <p>In order to make the robot actually stop, we need to publish a new message containing alternative velocity commands.</p> </li> <li> <p>In TERMINAL 3 press the Up key on your keyboard to recall the previous command, but don't press Enter just yet! Now press the Left key to track back through the message and change the velocity field values back to <code>0.0</code> in order to now make the robot stop.</p> </li> <li> <p>Once again, enter Ctrl+C in TERMINAL 3 to stop the publisher from actively publishing new messages, and then follow the same steps as above to compose another new message to now make the robot move in a circle.</p> </li> <li> <p>Enter Ctrl+C to again stop the message from being published, publish a further new message to stop the robot, and then compose (and publish) a message that would make the robot drive in a straight line.</p> </li> <li> <p>Finally, stop the robot again!</p> </li> </ol>"},{"location":"course/part2/#ex4","title":"Exercise 4: Creating a Python Node to Make a Robot Move in a circle","text":"<p>Controlling a robot from the terminal (or by using the <code>teleop_keyboard</code> node) is all well and good, but what about if we need to implement some more advanced control or autonomy?</p> <p>We'll now learn how to control the velocity of our robot programmatically, from a Python Node. We'll start out with a simple example to achieve a simple velocity profile (a circle), but this will provide us with the basis on which we can build more complex velocity control algorithms (which we'll look at in the following exercise).</p> <p>In Part 1 we built a simple publisher node, and this one will work in much the same way, but this time however, we need to publish <code>Twist</code> type messages to the <code>/cmd_vel</code> topic instead... </p> <ol> <li> <p>In TERMINAL 2, ensure that you're located within the <code>scripts</code> folder of your <code>part2_navigation</code> package (you could use <code>pwd</code> to check your current working directory).</p> <p>If you aren't located here then navigate to this directory using <code>cd</code>.</p> </li> <li> <p>Create a new file called <code>move_circle.py</code>:</p> <p></p><pre><code>touch move_circle.py\n</code></pre> ... and make this file executable using the <code>chmod</code> command (as we did in Part 1).<p></p> </li> <li> <p>The task is to make the robot move in a circle with a path radius of approximately 0.5 meters.</p> <p>Follow the instructions on the following page for building this:</p> <p></p> Building the <code>move_circle.py</code> node<p></p> </li> <li> <p>Next (hopefully you're getting the idea by now!), declare the <code>move_circle.py</code> node as an executable in the <code>part2_navigation/CMakeLists.txt</code> file:</p> CMakeLists.txt<pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/basic_velocity_control.py\n  scripts/stop_me.py\n  scripts/odom_subscriber.py\n  scripts/move_circle.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Finally, head back to TERMINAL 2 and use Colcon to build the new node alongside everything else in the package, using the same three-step process as before:</p> <ol> <li> <p>First: </p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Then:</p> <pre><code>colcon build --packages-select part2_navigation --symlink-install\n</code></pre> </li> <li> <p>And finally, re-source again:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Run this node now, using <code>ros2 run</code> and see what happens:</p> <pre><code>ros2 run part2_navigation move_circle.py\n</code></pre> <p>Head back to the Gazebo simulation and watch as the robot moves around in a circle of 0.5-meter radius!</p> </li> <li> <p>Once you're done, enter Ctrl+C in TERMINAL 2 to stop the <code>move_circle.py</code> node. </p> <p>Question</p> <p>What happens to the robot when you hit Ctrl+C to stop the <code>move_circle.py</code> node?</p> <p>Answer: It carries on moving !</p> <p>You can run another node from within your package to actually stop it:</p> <pre><code>ros2 run part2_navigation stop_me.py\n</code></pre> </li> </ol>"},{"location":"course/part2/#ex5","title":"Exercise 5: Implementing a Shutdown Procedure","text":"<p>Clearly, our work on the <code>move_circle.py</code> node isn't quite done. When we terminate our node we'd expect the robot to stop moving, but this (currently) isn't the case. </p> <p>You may have also noticed (with all the nodes that we have created so far) an error traceback in the terminal, every time we hit Ctrl+C. </p> <p>None of this is very good, and we'll address this now by modifying the <code>move_circle.py</code> file to incorporate a proper (and safe) shutdown procedure.</p> <ol> <li> <p>Return to the <code>move_circle.py</code> file in VS Code. </p> </li> <li> <p>First, we need to add an import to our Node:</p> <pre><code>from rclpy.signals import SignalHandlerOptions\n</code></pre> <p>You'll see what this is for shortly...</p> </li> <li> <p>Then move on to the <code>__init__()</code> method of your <code>Circle()</code> class.</p> <p>Add in a boolean flag here called <code>shutdown</code>:</p> <pre><code>self.shutdown = False\n</code></pre> <p>... to begin with, we want this to be set to <code>False</code>.</p> </li> <li> <p>Next, add a new method to your <code>Circle()</code> class, called <code>on_shutdown()</code>:</p> <pre><code>def on_shutdown(self):\n    self.get_logger().info(\n        \"Stopping the robot...\"\n    )\n    self.my_publisher.publish(TwistStamped()) # (1)!\n    self.shutdown = True # (2)!\n</code></pre> <ol> <li>All velocities within the <code>Twist(Stamped)</code> message class are set to zero by default, so we can just publish this as-is, in order to ask the robot to stop.</li> <li>Set the <code>shutdown</code> flag to true to indicate that a stop message has now been published.</li> </ol> </li> <li> <p>Finally, head to the <code>main()</code> function of the script. This is where most of the changes need to be made...</p> <pre><code>def main(args=None):\n    rclpy.init(\n        args=args,\n        signal_handler_options=SignalHandlerOptions.NO\n    ) # (1)!\n    move_circle = Circle()\n    try:\n        rclpy.spin(move_circle) # (2)!\n    except KeyboardInterrupt: # (3)!\n        print(\n            f\"{move_circle.get_name()} received a shutdown request (Ctrl+C).\"\n        )\n    finally: \n        move_circle.on_shutdown() # (4)!\n        while not move_circle.shutdown: # (5)!\n            continue\n        move_circle.destroy_node() # (6)!\n        rclpy.shutdown()\n</code></pre> <ol> <li> <p>When initialising <code>rclpy</code>, we're requesting for our <code>move_circle.py</code> node to handle \"signals\" (i.e. events like a Ctrl+C), rather than letting <code>rclpy</code> handle these for us. Here we're using the <code>SignalHandlerOptions</code> object that we imported from <code>rclpy.signals</code> earlier.</p> </li> <li> <p>We set our node to spin inside a Try-Except block now, so that we can catch a <code>KeyboardInterrupt</code> (i.e. a Ctrl+C) and act accordingly when this happens.</p> </li> <li> <p>On detection of the <code>KeyboardInterrupt</code> we print a message to the terminal. After this, the code will move on to the <code>finally</code> block...</p> </li> <li> <p>Call the <code>on_shutdown()</code> method that we defined earlier. This will ensure that a STOP command is published to the robot (via <code>/cmd_vel</code>).</p> </li> <li> <p>This <code>while</code> loop will continue to iterate until our boolean <code>shutdown</code> flag has turned <code>True</code>, to indicate that the STOP message has been published.</p> </li> <li> <p>The rest is the same as before...</p> <p>... destroy the node and then shutdown <code>rclpy</code>.</p> </li> </ol> </li> <li> <p>With all this in place, run the node again now (<code>ros2 run ...</code>).</p> <p>Now, when you hit Ctrl+C you should find that the robot actually stops moving. Ah, much better!</p> </li> </ol>"},{"location":"course/part2/#odometry-based-navigation","title":"Odometry-based Navigation","text":"<p>Over the course of the previous two exercises we've created a Python node to make our robot move using open-loop control. To achieve this we published velocity commands to the <code>/cmd_vel</code> topic to make the robot follow a circular motion path.</p> <p>Questions</p> <ol> <li>How do we know if our robot actually achieved the motion path that we asked for?</li> <li>In a real-world environment, what external factors might result in the robot not achieving its desired trajectory?</li> </ol> <p>Earlier on we also learnt about Robot Odometry, which is used by the robot to keep track of its position and orientation (aka Pose) in the environment.  As explained earlier, this is determined by a process called \"dead-reckoning,\" which is only really an approximation, but it's a fairly good one in any case, and we can use this as a feedback signal to understand if our robot is moving in the way that we expect it to.</p> <p>We can therefore build on the techniques that we used in the <code>move_circle.py</code> exercise, and now also build in the ability to subscribe to a topic too and obtain some real-time feedback. To do this, we'll need to subscribe to the <code>/odom</code> topic, and use this to implement some basic closed-loop control.</p>"},{"location":"course/part2/#ex6","title":"Exercise 6: Making our Robot Follow a Square Motion Path","text":"<ol> <li> <p>Make sure your <code>move_circle.py</code> node is no longer running in TERMINAL 2, stopping it with Ctrl+C if necessary.</p> </li> <li> <p>Make sure TERMINAL 2 is still located inside your <code>part2_navigation</code> package.</p> </li> <li> <p>Navigate to the package <code>scripts</code> directory and use the Linux <code>touch</code> command to create a new file called <code>move_square.py</code>:</p> <pre><code>touch move_square.py\n</code></pre> </li> <li> <p>Then make this file executable using <code>chmod</code>:</p> <pre><code>chmod +x move_square.py\n</code></pre> </li> <li> <p>Define <code>move_square.py</code> as a package executable in your <code>CMakeLists.txt</code> file (you should know how to do this by now, but if not, refer back to either Exercise 2 or Exercise 4). </p> </li> <li> <p>Use the VS Code File Explorer to navigate to this <code>move_square.py</code> file and open it up, ready for editing.</p> </li> <li> <p>There's a template below to help you with this exercise. </p> <p></p> Access the <code>move_square.py</code> template here<p></p> <p>Copy and paste the template code into your new <code>move_square.py</code> file to get you started. </p> </li> <li> <p>Re-build your <code>part2_navigation</code> package, to include your new <code>move_square.py</code> node, following that three-step build process once again:</p> <p>Step 1:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> <p>Step 2:</p> <pre><code>colcon build --packages-select part2_navigation --symlink-install\n</code></pre> <p>Step 3:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> <li> <p>Run the code as it is to see what happens... </p> <p>Fill in the Blank!</p> <p>Something not quite working as expected? Did we forget something very crucial on the very first line of the code template?!</p> </li> <li> <p>Fill in the blank as required and then adapt the code to make your robot follow a square motion path of 1 x 1 meter dimensions.</p> <p>After following a square motion path a few times, your robot should return to the same location that it started from.</p> <p>Advanced feature</p> <p>Adapt the node to make the robot automatically stop once it has performed two complete loops.</p> </li> </ol>"},{"location":"course/part2/#wrapping-up","title":"Wrapping Up","text":"<p>In this session we've learnt how to control the velocity and position of a robot from both the command-line (using ROS command-line tools) and from ROS Nodes by publishing correctly formatted messages to the <code>/cmd_vel</code> topic.  </p> <p>We've also learnt about Odometry, which is published by our robot to the <code>/odom</code> topic.  The odometry data tells us the current linear and angular velocities of our robot in relation to its 3 principal axes.  In addition to this though, it also tells us where in physical space our robot is located and oriented, which is determined based on dead-reckoning. </p> <p>Questions</p> <ol> <li>What information (sensor/actuator data) is used to do this?</li> <li>Do you see any potential limitations of this?</li> </ol> <p>In the final exercise we explored the development of odometry-based control to make a robot follow a square motion path. You will likely have observed some degree of error in this which could be due to the fact that Odometry data is determined by dead-reckoning and is therefore subject to drift and accumulated error. Consider how other factors may impact the accuracy of control too.</p> <p>Questions</p> <ol> <li>How might the rate at which the odometry data is sampled play a role?</li> <li>How quickly can your robot receive new velocity commands, and how quickly can it respond?</li> </ol> <p>Be aware that we did all this in simulation here too. In fact, in a real world environment, this type of navigation might be less effective, since things such as measurement noise and calibration errors can also have considerable impact. You will have the opportunity to experience this first hand in the labs.</p> <p>Ultimately then, we've seen a requirement here for additional information to provide more confidence of a robot's location in its environment, in order to enhance its ability to navigate effectively and avoid crashing into things! We'll explore this further later in this course.</p>"},{"location":"course/part2/#backup","title":"WSL-ROS2 Managed Desktop Users: Save your work!","text":"<p>Remember, the work you have done in the WSL-ROS2 environment during this session will not be preserved for future sessions or across different University machines automatically! To save the work you have done here today you should now run the following script in any idle WSL-ROS2 Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will export your home directory to your University <code>U:\\</code> Drive, allowing you to restore it on another managed desktop machine the next time you fire up WSL-ROS2.  </p> <ol> <li> <p>Quaternions are explained very nicely here, if you'd like to learn more.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/part3/","title":"Part 3: Beyond the Basics","text":""},{"location":"course/part3/#introduction","title":"Introduction","text":"<p> Exercises: 5 Estimated Completion Time: 3 hours Difficulty Level: Intermediate  </p>"},{"location":"course/part3/#aims","title":"Aims","text":"<p>In this part of the course we'll look at some more advanced ROS concepts, and explore another of our robot's on-board sensors: the LiDAR sensor. From the work you did in Part 2 you may have started to appreciate the limitations associated with using odometry data alone as a feedback signal when trying to control a robot's position in its environment. The LiDAR sensor can provide further information about an environment, thus enhancing a robot's knowledge and capabilities. To begin with however, we'll look at how to launch ROS applications more efficiently with launch files, and how the behaviour of nodes can be changed dynamically using parameters. </p>"},{"location":"course/part3/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Create launch files to allow the execution of multiple ROS Nodes simultaneously with <code>ros2 launch</code>.</li> <li>Use parameters to influence the behaviours of nodes in real-time, without having to re-programme them.</li> <li>Learn about the robot's LiDAR sensor and the measurements obtained from this.</li> <li>Interpret the <code>LaserScan</code> data that is published to the <code>/scan</code> topic and use existing ROS tools to visualise this.</li> <li>Perform numeric analysis on data arrays (using the <code>numpy</code> Python library) to process <code>LaserScan</code> data for use in ROS applications.</li> <li>Use existing ROS tools to implement SLAM and build a map of an environment. </li> </ol>"},{"location":"course/part3/#quick-links","title":"Quick Links","text":""},{"location":"course/part3/#exercises","title":"Exercises","text":"<ul> <li>Exercise 1: Creating a Launch File</li> <li>Exercise 2: Using parameters to change robot behaviour in real-time</li> <li>Exercise 3: Using RViz to Visualise LaserScan Data</li> <li>Exercise 4: Building a LaserScan Callback Function</li> <li>Exercise 5: Building a map of an environment with SLAM</li> </ul>"},{"location":"course/part3/#additional-resources","title":"Additional Resources","text":"<ul> <li>A Basic <code>LaserScan</code> Subscriber Node (for Exercise 4)</li> </ul>"},{"location":"course/part3/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch your ROS Environment</p> <p>If you haven't done so already, launch your ROS environment now:</p> <ol> <li>Using WSL-ROS2 on a university managed desktop machine: follow the instructions here to launch it.</li> <li>Running WSL-ROS2 on your own machine: launch the Windows Terminal to access a WSL-ROS2 terminal instance.</li> <li>Docker Users: Follow the relevant steps to launch a terminal instance into your local ROS installation.</li> </ol> <p>You should now have access to a Linux terminal instance, and we'll refer to this terminal instance as TERMINAL 1.</p> <p>Step 2: Restore your work (WSL-ROS2 Managed Desktop Users ONLY)</p> <p>Remember that any work that you do within the WSL-ROS2 Environment will not be preserved between sessions or across different University computers. At the end of Part 2 you should have run the <code>wsl_ros</code> tool to back up your home directory to your University <code>U:\\</code> Drive. Once WSL-ROS2 is up and running, you should be prompted to restore this:</p> <pre><code>It looks like you already have a backup from a previous session:\n  U:\\wsl-ros\\ros2-backup-XXX.tar.gz\nDo you want to restore this now? [y/n]\n</code></pre> <p>Enter Y+Enter to restore your work from last time. You can also restore your work at any time using the following command:</p> <pre><code>wsl_ros restore\n</code></pre> <p>Step 3: Launch VS Code </p> <p>It's also worth launching VS Code now, so that it's ready to go for when you need it later on. </p> WSL Users... <p>It's important to launch VS Code within your ROS environment using the \"WSL\" extension. Always remember to check for this: </p> <p></p> <p></p> <p>Step 4: Make Sure The Course Repo is Up-To-Date</p> <p>In Part 1 you should have downloaded and installed The Course Repo into your ROS environment. Hopefully you've done this by now, but if you haven't then go back and do it now (you'll need it for some exercises here). If you have already done it, then (once again) it's worth just making sure it's all up-to-date, so run the following command now to do so:</p> <pre><code>cd ~/ros2_ws/src/tuos_ros/ &amp;&amp; git pull\n</code></pre> <p>Then run <code>colcon build</code> </p> <pre><code>cd ~/ros2_ws/ &amp;&amp; colcon build --packages-up-to tuos_ros\n</code></pre> <p>And finally, re-source your environment:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Remember</p> <p>If you have any other terminal instances open, then you'll need run <code>source ~/.bashrc</code> in these too, in order for the changes to propagate through to these as well!</p>"},{"location":"course/part3/#launch-files","title":"Launch Files","text":"<p>So far (in Parts 1 &amp; 2) we've used the <code>ros2 run</code> command to execute a variety of ROS nodes, such as <code>teleop_keyboard</code>, as well as a number of nodes that we've created of our own. You may also have noticed that we've used a <code>ros2 launch</code> command now and again too, mainly to launch Gazebo Simulations of our robot, but why do we have these two commands, and what's the difference between them?</p> <p>Complex ROS applications typically require the execution of multiple nodes at the same time. The <code>ros2 run</code> command only allows us to execute a single node, and so this isn't that convenient for such complex applications, where we'd have to open multiple terminals, use <code>ros2 run</code> multiple times and make sure that we ran everything in the correct order without forgetting anything! <code>ros2 launch</code>, on the other hand, provides a means to launch multiple ROS nodes simultaneously by defining exactly what we want to launch within launch files. This makes the execution of complex applications more reliable, repeatable and easier for others to launch these applications correctly. </p>"},{"location":"course/part3/#ex1","title":"Exercise 1: Creating a Launch File","text":"<p>In order to see how launch files work, let's create some of our own!</p> <p>In Part 1 we created <code>publisher.py</code> and <code>subscriber.py</code> nodes that could talk to one another via a topic called <code>/my_topic</code>. We launched these independently using the <code>ros2 run</code> command in two separate terminals. Wouldn't it be nice if we could have launched them both at the same time, from the same terminal instead?</p> <p>To start with, let's create another new package, this time called <code>part3_beyond_basics</code>. </p> <ol> <li> <p>In TERMINAL 1:</p> <ol> <li> <p>Head to the <code>src</code> folder of the ROS 2 workspace:</p> <pre><code>cd ~/ros2_ws/src/\n</code></pre> </li> <li> <p>Clone the ROS 2 Package Template:</p> <pre><code>git clone https://github.com/tom-howard/ros2_pkg_template.git\n</code></pre> </li> <li> <p>Run the <code>init_pkg.sh</code> script from within this template to initalise a package with the name \"part3_beyond_basics\":</p> <pre><code>./ros2_pkg_template/init_pkg.sh part3_beyond_basics\n</code></pre> </li> <li> <p>And navigate into the root of this new package, using <code>cd</code>:</p> <pre><code>cd ./part3_beyond_basics/\n</code></pre> </li> </ol> </li> <li> <p>Launch files should be located in a <code>launch</code> directory at the root of the package directory, so use <code>mkdir</code> to do this:</p> <pre><code>mkdir launch\n</code></pre> </li> <li> <p>Use the <code>cd</code> command to enter the <code>launch</code> folder that you just created:</p> <pre><code>cd launch\n</code></pre> <p>...and then use the <code>touch</code> command to create a new empty file called <code>pubsub.launch.py</code>:</p> <pre><code>touch pubsub.launch.py\n</code></pre> </li> <li> <p>Open this launch file in VS Code and enter the following:</p> pubsub.launch.py<pre><code>from launch import LaunchDescription # (1)!\nfrom launch_ros.actions import Node # (2)!\n\ndef generate_launch_description(): # (3)!\n    return LaunchDescription([ # (4)!\n        Node( # (5)!\n            package='part1_pubsub', # (6)!\n            executable='publisher.py', # (7)!\n            name='my_publisher' # (8)!\n        )\n    ])\n</code></pre> <ol> <li>Everything that we want to execute with a launch file must be encapsulated within a <code>LaunchDescription</code>, which is imported here from the <code>launch</code> module.</li> <li>In order to execute a node from a launch file we need to define it using the <code>Node</code> class from <code>launch_ros.actions</code> (not to be confused with the ROS Action communication method covered in Part 5!)</li> <li>We encapsulate a Launch Description inside a <code>generate_launch_description()</code> function.</li> <li>Here we define everything that we want this launch file to execute: in this case a Python list <code>[]</code> containing a single <code>Node()</code> item (for now).</li> <li>Here we describe the node that we want to launch.</li> <li>The name of the package that the node is part of.</li> <li>The name of the actual node that we want to launch from the above package.</li> <li> <p>A name to register this node as on the ROS network. While this is also defined in the node itself: </p> <pre><code>super().__init__(\"simple_publisher\")\n</code></pre> <p>...we can override this here with something else. </p> </li> </ol> </li> <li> <p>We need to make sure we tell <code>colcon</code> about our new <code>launch</code> directory, so that it can build the launch files within it when we run <code>colcon build</code>. To do this, we need to add a directory install instruction to our package's <code>CMakeLists.txt</code>:</p> <p>Open up the <code>CMakeLists.txt</code> file and add the following text just above the <code>ament_package()</code> line at the very bottom:</p> part3_beyond_basics/CMakeLists.txt<pre><code>install(DIRECTORY\n  launch\n  DESTINATION share/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Now, let's build the package using the three-step process that you'll be becoming familiar with by now... </p> <ol> <li> <p>Navigate back to the root of the ROS 2 workspace:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Run <code>colcon build</code> on your new package only:</p> <pre><code>colcon build --packages-select part3_beyond_basics --symlink-install\n</code></pre> </li> <li> <p>And finally, re-source the <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Use <code>ros2 launch</code> to launch this file and test it out as it is:</p> <pre><code>ros2 launch part3_beyond_basics pubsub.launch.py\n</code></pre> </li> <li> <p>The code (as it is) will launch the <code>publisher.py</code> node from the <code>part1_pubsub</code> package, but not the <code>subscriber.py</code> node.  We therefore need to add another <code>Node()</code> object to our <code>LaunchDescription</code>:</p> pubsub.launch.py<pre><code>from launch import LaunchDescription \nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(\n            package='part1_pubsub',\n            executable='publisher.py',\n            name='my_publisher'\n        ),\n        Node(\n            # TODO: define the subscriber.py node...\n        )\n    ])\n</code></pre> <p>Using the same methods as above, add the necessary definitions for the <code>subscriber.py</code> node into your launch file.</p> </li> <li> <p>Once you've made these changes you'll need to run <code>colcon build</code> again.</p> <p>Warning</p> <p>You'll need to run <code>colcon build</code> every time you make changes to a launch file, even if you use the <code>--symlink-install</code> option (as this only applies to nodes in the <code>scripts</code> directory)</p> </li> <li> <p>Once you've completed this, it should be possible to launch both the publisher and subscriber nodes (from your <code>part1_pubsub</code> package) with <code>ros2 launch</code> and the <code>pubsub.launch.py</code> file. Verify this in TERMINAL 1 by executing the launch file:</p> <p></p> <p></p> </li> <li> <p>We can further verify this in a new terminal (TERMINAL 2), using commands that we've use in Parts 1 &amp; 2 to list all nodes and topics that are active on our ROS network:</p> <p></p><pre><code>ros2 node list\n</code></pre> <pre><code>ros2 topic list\n</code></pre><p></p> <p>Do you see what you'd expect to see in the output of these two commands?</p> </li> </ol> <p>Launch Files: Advanced</p> <p>For more advanced launch file techniques, see here!</p>"},{"location":"course/part3/#parameters","title":"Parameters","text":"<p>Parameters are used to configure nodes, and can be further used to change their behaviour dynamically during run time. </p>"},{"location":"course/part3/#ex2","title":"Exercise 2: Using parameters to change robot behaviour in real-time","text":"<p>Let's think back to our <code>move_circle.py</code> node from Part 2 in order to see what this could be used for. The node that we built originally would make a robot move in a circle of 0.5-meter radius, forever! Wouldn't it be nice if we could actually change the radius of the circle while the node was running?</p> <ol> <li> <p>Shutdown your <code>pubsub.launch.py</code> file in TERMINAL 1 if it's still running.</p> </li> <li> <p>Now, we'll create a new version of the <code>move_circle.py</code> node from Part 2, that we can play around with. First, head to the <code>scripts</code> directory of your <code>part3_beyond_basics</code> package in TERMINAL 1:</p> <pre><code>cd ~/ros2_ws/src/part3_beyond_basics/scripts/\n</code></pre> </li> <li> <p>Create a new node called <code>param_circle.py</code>:</p> <pre><code>touch param_circle.py\n</code></pre> <p>and give this execute permissions:</p> <pre><code>chmod +x param_circle.py\n</code></pre> </li> <li> <p>Declare this as a package executable by opening up your package's <code>CMakeLists.txt</code> in VS Code and adding <code>param_circle.py</code> as shown below:</p> CMakeLists.txt<pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/basic_velocity_control.py\n  scripts/stop_me.py\n  scripts/param_circle.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Re-build your package with <code>colcon</code> (even though <code>param_circle.py</code> is still just an empty file at this stage):</p> <ol> <li> <p>First:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Then:</p> <pre><code>colcon build --packages-select part3_beyond_basics --symlink-install\n</code></pre> </li> <li> <p>And finally:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>After having completed Part 2 Exercises 4 &amp; 5, you should have a working <code>move_circle.py</code> node complete with a proper shutdown procedure. Take a copy of the code and paste it into your new <code>param_circle.py</code> file. Alternatively, if you didn't manage to finish these exercises previously, you can access a worked example here.</p> </li> <li> <p>Now, let's modify the code:</p> <ol> <li> <p>First, in the <code>__init__()</code> method, change the name of the node:</p> <pre><code>super().__init__(\"param_circle\")\n</code></pre> <p>When launched, the node will now be registered on the ROS network with the name <code>\"param_circle\"</code>.</p> </li> <li> <p>Then, directly under this (still within the <code>__init__()</code> method), add the following new lines:</p> <pre><code>self.declare_parameter(\n    name=\"radius\", \n    value=0.5 # meters\n)\n</code></pre> <p>Here, we're declaring a ROS parameter called <code>radius</code> and assigning a default value of <code>0.5</code> to it, to represent the desired radius of the circle (in meters).</p> </li> <li> <p>Somewhere in the <code>timer_callback()</code> method, you should be defining the desired radius of the circle. Modify this as follows:</p> <pre><code>radius = self.get_parameter(\"radius\").get_parameter_value().double_value\n</code></pre> <p>The radius of the circle that the robot will move through is now based on the value of the <code>radius</code> parameter, rather than a static value. </p> </li> <li> <p>Finally, modify your <code>get_logger().info()</code> statement (at the end of the <code>timer_callback()</code> method) to include information about the target radius:</p> <pre><code>self.get_logger().info( \n    f\"Moving with radius: {radius:.2f} [m]\\n\"\n    f\"Linear Velocity: {topic_msg.twist.linear.x:.2f} [m/s], \"\n    f\"Angular Velocity: {topic_msg.twist.angular.z:.2f} [rad/s].\",\n    throttle_duration_sec=1, \n)\n</code></pre> </li> </ol> </li> <li> <p>In order to test this, first fire up the Empty World simulation in TERMINAL 1:</p> <pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py \n</code></pre> </li> <li> <p>Next, in TERMINAL 2, run the <code>param_circle.py</code> node:</p> <pre><code>ros2 run part3_beyond_basics param_circle.py\n</code></pre> <p>The robot should start to move in a circle. To begin with, the radius of this circle should be 0.5 meters, based on the default value that we assigned to the parameter in our code.</p> </li> <li> <p>Next, open up another terminal instance (TERMINAL 3), and run the following command to list all the parameters that are currently active/available on our ROS network:</p> <pre><code>ros2 param list\n</code></pre> <p>This may be quite a big list. Parameters are listed under the nodes that define them, so to filter the list, provide the name of the node as well:</p> <pre><code>ros2 param list /param_circle\n</code></pre> <p>There should only be a handful of parameters listed now, including <code>radius</code>. </p> </li> <li> <p>We can now change the value of this parameter while our <code>param_circle.py</code> node is running, and therefore change the size of the circle that the robot is currently following. We can do this from the command line, without having to stop our <code>param_circle.py</code> node or make any changes to it. Again, in TERMINAL 3: </p> <pre><code>ros2 param set /param_circle radius 1.2\n</code></pre> <p>Try setting a range of different values and observe how the robot's behaviour changes in the simulated world!</p> </li> <li> <p>Once you're done, close down the <code>param_circle.py</code> node and the Gazebo Simulation by entering Ctrl+C in terminals 1 and 2 respectively.</p> </li> </ol>"},{"location":"course/part3/#summary","title":"Summary","text":"<p>Parameters allow us to change the behaviour of things during runtime. We'll also explore some other uses for them in the next part of this course (Part 4).</p> <p>We've also learnt some key launch file techniques now too, so let's move on to another more advanced and very important topic...</p>"},{"location":"course/part3/#lidar","title":"Laser Displacement Data and The LiDAR Sensor","text":"<p>As you'll recall from Part 2, odometry is really important for robot navigation, but it can be subject to drift and accumulated error over time. You may have observed this in simulation during Part 2 Exercise 5, and you would most certainly notice it if you were to do the same on a real robot. Fortunately, The Waffles have another sensor on-board which provides even richer information about the environment, and we can use this to supplement the odometry information and enhance the robot's navigation capabilities.</p>"},{"location":"course/part3/#introducing-the-laserscan-interface","title":"Introducing the LaserScan Interface","text":""},{"location":"course/part3/#ex3","title":"Exercise 3: Using RViz to Visualise LaserScan Data","text":"<p>We're now going to place the robot in a more interesting environment than the \"empty world\" that we've been working with so far...</p> <ol> <li> <p>In TERMINAL 1 enter the following command to launch this:</p> <pre><code>ros2 launch turtlebot3_gazebo turtlebot3_world.launch.py\n</code></pre> <p>A Gazebo simulation should now be launched with a TurtleBot3 Waffle in a new environment:</p> <p></p> <p></p> </li> <li> <p>In TERMINAL 2 enter the following:</p> <pre><code>ros2 launch tuos_tb3_tools rviz.launch.py environment:=sim\n</code></pre> <p>On running the command a new window should open:</p> <p></p> <p></p> <p>This is RViz, which is a ROS tool that allows us to visualise the data being measured by a robot in real-time. </p> <p>The green dots scattered around the robot represent laser displacement data which is measured by the LiDAR sensor located on the top of the robot, allowing it to measure the distance to any obstacles in its surroundings. </p> <p>The LiDAR sensor spins continuously, sending out laser pulses as it does so. These laser pulses are reflected from any objects and sent back to the sensor. Distance can then be determined based on the time it takes for the pulses to complete the full journey (from the sensor, to the object, and back again), by a process called \"time of flight\". Because the LiDAR sensor spins and performs this process continuously, a full 360\u00b0 scan of the environment can be generated.</p> <p>In this case (because we are working in simulation here) the data represents the objects surrounding the robot in its simulated environment, so you should notice that the green dots produce an outline that resembles the objects in the world that is being simulated in Gazebo (or partially at least).</p> </li> <li> <p>Laser displacement data from the LiDAR sensor is published by the robot to the <code>/scan</code> topic. We can use the <code>ros2 topic info</code> command to find out more about the nodes that are publishing and subscribing to this topic, as well as the type of interface used to transmit this topic data. In TERMINAL 3 enter the following:</p> <p></p><pre><code>ros2 topic info /scan\n</code></pre> <pre><code>Type: sensor_msgs/msg/LaserScan\nPublisher count: 1\nSubscription count: 0\n</code></pre><p></p> </li> <li> <p>As we can see from above, <code>/scan</code> data is of the <code>sensor_msgs/msg/LaserScan</code> type, and we can find out more about this interface using the <code>ros2 interface show</code> command:</p> <p></p><pre><code>ros2 interface show sensor_msgs/msg/LaserScan\n</code></pre> <pre><code># Single scan from a planar laser range-finder\n\nstd_msgs/Header header # timestamp in the header is the acquisition time of\n        builtin_interfaces/Time stamp\n                int32 sec\n                uint32 nanosec\n        string frame_id\n                             # the first ray in the scan.\n                             #\n                             # in frame frame_id, angles are measured around\n                             # the positive Z axis (counterclockwise, if Z is up)\n                             # with zero angle being forward along the x axis\n\nfloat32 angle_min            # start angle of the scan [rad]\nfloat32 angle_max            # end angle of the scan [rad]\nfloat32 angle_increment      # angular distance between measurements [rad]\n\nfloat32 time_increment       # time between measurements [seconds] - if your scanner\n                             # is moving, this will be used in interpolating position\n                             # of 3d points\nfloat32 scan_time            # time between scans [seconds]\n\nfloat32 range_min            # minimum range value [m]\nfloat32 range_max            # maximum range value [m]\n\nfloat32[] ranges             # range data [m]\n                             # (Note: values &lt; range_min or &gt; range_max should be discarded)\nfloat32[] intensities        # intensity data [device-specific units].  If your\n                             # device does not provide intensities, please leave\n                             # the array empty.\n</code></pre><p></p> </li> </ol>"},{"location":"course/part3/#interpreting-laserscan-data","title":"Interpreting LaserScan Data","text":"<p>The <code>LaserScan</code> interface is a standardised ROS message interface (from the <code>sensor_msgs</code> package) that any ROS Robot can use to publish data that it obtains from a Laser Displacement Sensor such as the LiDAR on the TurtleBot3.  </p> <p><code>ranges</code> is an array of <code>float32</code> values (array data-types are suffixed with <code>[]</code>). This is the part of the message containing all the actual distance measurements that are being obtained by the LiDAR sensor (in meters).</p> <p>Consider a simplified example here, taken from a TurtleBot3 in a different environment:</p> <p></p> <p>As illustrated in the figure, we can associate each data-point of the <code>ranges</code> array to an angular position by using the <code>angle_min</code>, <code>angle_max</code> and <code>angle_increment</code> values that are also provided within the <code>LaserScan</code> message.  We can use the <code>ros2 topic echo</code> command to find out what their values are:</p> <p></p><pre><code>$ ros2 topic echo /scan --field angle_min --once\n0.0\n---\n</code></pre> <pre><code>$ ros2 topic echo /scan --field angle_max --once\n6.28000020980835\n---\n</code></pre> <pre><code>$ ros2 topic echo /scan --field angle_increment --once\n0.01749303564429283\n---\n</code></pre><p></p> <p>Question</p> <p>What do these values represent? (Compare them with the figure above)</p> <p>Tip</p> <p>Notice how we were able to access specific variables within the <code>/scan</code> data using the <code>--field</code> flag, and ask the command to only provide us with a single message by using <code>--once</code>?</p> <p>The <code>ranges</code> array contains 360 values in total, i.e. a distance measurement at every 1\u00b0 (an <code>angle_increment</code> of 0.0175 radians) around the robot. The first value in the <code>ranges</code> array (<code>ranges[0]</code>) is the distance to the nearest object directly in front of the robot (i.e. at \u03b8 = 0 radians, or <code>angle_min</code>). The last value in the <code>ranges</code> array (<code>ranges[359]</code>) is the distance to the nearest object at 359\u00b0 (i.e. \u03b8 = 6.283 radians, or <code>angle_max</code>) from the front of the robot, i.e.: 1 degree to the right of the X-axis. <code>ranges[65]</code>, for example, would represent the distance to the closest object at an angle of 65\u00b0 (1.138 radians) from the front of the robot (anti-clockwise), as shown in the figure.</p> <p>The <code>LaserScan</code> message also contains the parameters <code>range_min</code> and <code>range_max</code>, which represent the minimum and maximum distances (again, in meters) that the LiDAR sensor can detect, respectively. Use the <code>ros2 topic echo</code> command to report these directly too.  </p> <p>Questions</p> <ol> <li>What is the maximum and minimum range of the LiDAR sensor? Use the same technique as we used above to find out.</li> <li> <p>Consider the note against <code>ranges</code> in the <code>ros2 interface show</code> output earlier:</p> <pre><code>float32[] ranges    # range data [m]\n                    # (Note: values &lt; range_min or &gt; range_max should be discarded)\n</code></pre> <p>(this might be worth thinking about).</p> </li> </ol> <p>Finally, use the <code>ros2 topic echo</code> command again to display the <code>ranges</code> portion of the <code>LaserScan</code> data. There's a lot of data here (360 data points per message in fact, as you know from above!):</p> <pre><code>ros2 topic echo /scan --field ranges\n</code></pre> <p>We're dropping the <code>--once</code> option now, so that we can see the data as it comes in, in real-time.  You might need to expand the terminal window so that you can see all the data points; data will be bound by square brackets <code>[]</code>, and there should be a <code>---</code> at the end of each message too, to help you confirm that you are viewing the whole thing.</p> <p>The main thing you'll notice here is that there's lots of information, and it changes rapidly! As you have already seen though, it is the numbers that are flying by here that are represented by green dots in RViz.  Head back to the RViz screen to have another look at this now. As you'll no doubt agree, this is a much more useful way to visualise the <code>ranges</code> data, and illustrates how useful RViz can be for interpreting what your robot can see in real-time.</p> <p>What you may also notice is several <code>inf</code> values scattered around the array.  These represent sensor readings that are outside the sensor's measurement range (i.e. greater than <code>range_max</code> or less than <code>range_min</code>), so the sensor can't report a distance measurement in such cases. Remember from above: </p> <pre><code>(Note: values &lt; range_min or &gt; range_max should be discarded)\n</code></pre> <p>Note</p> <p>This behaviour is different on the real robots! Be aware of this when developing code for real robots!!</p> <p>Stop the <code>ros2 topic echo</code> command from running in the terminal window by entering Ctrl+C in TERMINAL 3. Also close down the RViz process running in TERMINAL 2 now as well, but leave the simulation (in TERMINAL 1 running). </p>"},{"location":"course/part3/#ex4","title":"Exercise 4: Building a LaserScan Callback Function","text":"<p>LaserScan data presents us with a new challenge: processing large datasets. In this exercise we'll look at some basic approaches that can be taken to deal with this data, and get something meaningful out of it that can be used in your robot applications.</p> <ol> <li> <p>In TERMINAL 2, navigate into the <code>scripts</code> folder of your <code>part3_beyond_basics</code> package:</p> <pre><code>cd ~/ros2_ws/src/part3_beyond_basics/scripts/\n</code></pre> </li> <li> <p>Create a new file called <code>lidar_subscriber.py</code> (using <code>touch</code>), make this executable (using <code>chmod</code>) and then define this as a package executable in your <code>CMakeLists.txt</code> file (if you need help with any of this, refer back to the earlier exercise).</p> </li> <li> <p>Open up the file in VS Code, then see below for the instructions on how to build it:</p> <p></p> Building a <code>LaserScan</code> Subscriber <p></p> </li> <li> <p>Head back to the terminal and build with <code>colcon</code>:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> <pre><code>colcon build --packages-select part3_beyond_basics --symlink-install\n</code></pre> <pre><code>source ~/.bashrc\n</code></pre> </li> <li> <p>With all of that done, you're ready to go. Run the node using <code>ros2 run</code>:</p> <pre><code>ros2 run part3_beyond_basics lidar_subscriber.py\n</code></pre> </li> <li> <p>Open another terminal (so you can still see the outputs from your <code>lidar_subscriber.py</code> node). Launch the <code>teleop_keyboard</code> node, and drive the robot around, noting how the outputs from your <code>lidar_subscriber.py</code> node change as you do so.</p> </li> <li> <p>Close everything down now (including the simulation running in TERMINAL 1). Then launch the \"empty world\" simulation again:</p> <pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre> </li> <li> <p>Go back to TERMINAL 2 and launch your <code>lidar_subscriber.py</code> node again:</p> <pre><code>ros2 run part3_beyond_basics lidar_subscriber.py\n</code></pre> <p>What output do you see from this now?</p> <p>You should notice that your <code>lidar_subscriber.py</code> node reports <code>nan meters</code> now. That's because there's nothing in the environment for the LiDAR sensor to detect, so all readings are out of range and hence our analysis of the 40\u00b0 arc of LiDAR readings at the front of the robot has filtered out everything and therefore returned <code>nan</code> (not a number).</p> </li> <li> <p>Use the Box tool in Gazebo to place a box in the environment. </p> <p></p> <p></p> </li> <li> <p>Click the Translate tool to move the box around until the <code>lidar_subscriber.py</code> node returns some reading that aren't <code>nan</code> again.</p> </li> <li> <p>Move the box around some more to observe what our analysis of the <code>LaserScan</code> data can detect, and where the box falls out of the detectable range.</p> </li> <li> <p>Think about how you could adapt the callback function of the <code>lidar_subscriber.py</code> node so that it picks up on more than one <code>LaserScan</code> subset, so that it could detect situations such as this (for example):</p> <p></p> <p></p> </li> </ol>"},{"location":"course/part3/#slam","title":"Simultaneous Localisation and Mapping (SLAM)","text":"<p>In combination, the data from the LiDAR sensor and the robot's odometry (the robot pose specifically) are really powerful, and allow some very useful conclusions to be made about the environment a robot is operating within.  One of the key applications of this data is \"Simultaneous Localisation and Mapping\", or SLAM.  This is a tool that's built into ROS, and allows a robot to build up a map of its environment and locate itself within that map at the same time!  We'll now look at how easy it is to leverage this in ROS.</p>"},{"location":"course/part3/#ex5","title":"Exercise 5: Building a map of an environment with SLAM","text":"<ol> <li> <p>Close down all ROS processes that are running now by entering Ctrl+C in each terminal. </p> </li> <li> <p>We're going to launch our robot into another new simulated environment now, which we'll be creating a map of using SLAM! To launch the simulation enter the following command in TERMINAL 1:</p> <pre><code>ros2 launch tuos_simulations nav_world.launch.py\n</code></pre> <p>The environment that launches should look like this:</p> <p></p> <p></p> </li> <li> <p>Now launch SLAM to start building a map of this environment. In TERMINAL 2, launch SLAM as follows:</p> <pre><code>ros2 launch tuos_tb3_tools slam.launch.py environment:=sim\n</code></pre> <p>This will launch RViz again, where you should see a top-down view of an environment with a model of the robot, surrounded by some dots representing the real-time LiDAR data. </p> <p></p> <p></p> <p>SLAM has already started building a map of the boundaries that are currently visible to the robot, based on its starting position in the environment. </p> <p>If you leave this for a minute the walls of the arena will start to become visible in RViz, and the floor will start to turn a lighter grey. As time passes the SLAM algorithms become more certain of what is being observed in the environment, allowing the boundaries and the free space to be defined.</p> <p></p> <p></p> </li> <li> <p>In TERMINAL 3 launch the <code>teleop_keyboard</code> node (you should know how to do this by now). Re-arrange and re-size your windows so that you can see Gazebo, RViz and the <code>teleop_keyboard</code> terminal instances all at the same time.</p> </li> <li> <p>Drive the robot around the arena slowly, using the <code>teleop_keyboard</code> node, and observe how the map is constantly being updated and expanded in the RViz window as you do so. </p> </li> <li> <p>As you're doing this open up another terminal instance (TERMINAL 4) and run the <code>odom_subscriber.py</code> node that you created back in Part 2:</p> <pre><code>ros2 run part2_navigation odom_subscriber.py\n</code></pre> <p>This will provide you with the robot's <code>X</code> and <code>Y</code> coordinates (in meters) within the environment, as you are driving it around, and you can use this to determine the centre coordinates of the four circles (A, B, C &amp; D) that are printed on the arena floor. </p> <p>Drive your robot into each of these circular zones and stop the robot inside them.    </p> <p>Record the coordinates of each zone marker  in a table such as the one below.</p> <p></p><p></p> Zone X Position (m) Y Position (m) START 0.5 -0.04 A B C D <p></p><p></p> </li> <li> <p>Continue to drive the robot around until a full map of the environment has been generated.</p> <p></p> <p></p> </li> <li> <p>Once you have built a complete map of the environment (and you've got the coordinates of all the circles), you can then save your map for later use. We do this using a ROS <code>map_server</code> package.  First, stop the robot by pressing S in TERMINAL 3 and then enter Ctrl+C to shut down the <code>teleop_keyboard</code> node.</p> </li> <li> <p>Then, remaining in TERMINAL 3, navigate to the root of your <code>part3_beyond_basics</code> package directory and create a new folder in it called <code>maps</code>:</p> <p></p><pre><code>cd ~/ros2_ws/src/part3_beyond_basics/\n</code></pre> <pre><code>mkdir maps\n</code></pre><p></p> </li> <li> <p>Navigate into this new directory:</p> <pre><code>cd maps/\n</code></pre> </li> <li> <p>Then, run the <code>map_saver_cli</code> node from the <code>map_server</code> package to save a copy of your map: </p> <p></p><pre><code>ros2 run nav2_map_server map_saver_cli -f MAP_NAME\n</code></pre> Replacing <code>MAP_NAME</code> with a name of your choosing. <p></p> <p>This will create two files: a <code>MAP_NAME.pgm</code> and a <code>MAP_NAME.yaml</code> file, both of which contain data related to the map that you have just created.  The <code>.pgm</code> file contains an Occupancy Grid Map (OGM), which is used for autonomous navigation in ROS.  Have a look at the map by launching it in an Image Viewer Application called <code>eog</code>:</p> <pre><code>eog MAP_NAME.pgm\n</code></pre> <p>A new window should launch containing the map you have just created with SLAM and the <code>map_saver_cli</code> node: </p> <p></p> <p></p> <p>White regions represent the area that your robot has determined is open space and that it can freely move within.  Black regions, on the other hand, represent boundaries or objects that have been detected.  Any grey area on the map represents regions that remain unexplored, or that were inaccessible to the robot.</p> </li> <li> <p>Compare the map generated by SLAM to the actual simulated environment (in Gazebo). </p> <p>Questions</p> <ul> <li>How accurately did your robot map the environment?</li> <li>What might impact this when working in a real-world environment?</li> </ul> </li> <li> <p>Close the image using the  button on the right-hand-side of the eog window.</p> </li> </ol>"},{"location":"course/part3/#summary-of-slam","title":"Summary of SLAM","text":"<p>See how easy it was to map an environment in the previous exercise? This works just as well on a real robot in a real environment too (as you'll observe in the lab). </p> <p>This illustrates the power of ROS: having access to tools such as SLAM, which are built into the ROS framework, makes it really quick and easy for a robotics engineer to start developing robotic applications on top of this. Our job was made even easier here since we used some packages that had been pre-made by the manufacturers of our TurtleBot3 Robots to help us launch SLAM with the right configurations for our particular robot. If you were developing a robot yourself, or working with a different type of robot, then you might need to do a bit more work in setting up and tuning the SLAM tools to make them work for your own application.</p>"},{"location":"course/part3/#wrapping-up","title":"Wrapping Up","text":"<p>As we learnt in Part 2, a robot's odometry is determined by dead-reckoning and control algorithms based on this alone (like the <code>move_square.py</code> node) may be subject to drift and accumulated error. </p> <p>Ultimately then, a robot needs additional information to pinpoint its precise location within an environment, and thus enhance its ability to navigate effectively and avoid crashing into things!</p> <p>This additional information can come from a LiDAR sensor, which was discussed earlier. We explored where this data is published to, how we access it, and what it tells us about a robot's immediate environment.  We then looked at some ways odometry and laser displacement data can be combined to perform advanced robotic functions such as the mapping of an environment. This is all complicated stuff but, using ROS, we can leverage these tools with relative ease, which illustrates just how powerful ROS can be for developing robotic applications quickly and effectively without having to re-invent the wheel!</p>"},{"location":"course/part3/#backup","title":"WSL-ROS2 Managed Desktop Users: Save your work!","text":"<p>Remember, the work you have done in the WSL-ROS2 environment during this session will not be preserved for future sessions or across different University machines automatically! To save the work you have done here today you should now run the following script in any idle WSL-ROS2 Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will export your home directory to your University <code>U:\\</code> Drive, allowing you to restore it on another managed desktop machine the next time you fire up WSL-ROS2.  </p>"},{"location":"course/part4/","title":"Part 4: Services","text":""},{"location":"course/part4/#introduction","title":"Introduction","text":"<p> Exercises: 6 Estimated Completion Time: 2 hours Difficulty Level: Intermediate  </p>"},{"location":"course/part4/#aims","title":"Aims","text":"<p>In this part you'll learn about Services: an alternative communication method that can be used to transmit data/information or invoke actions on a ROS Network. You'll learn how this works, and why it might be useful. You'll also look at some practical applications of this.</p>"},{"location":"course/part4/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Recognise how ROS Services differ from the standard topic-based publisher-subscriber approach, and identify appropriate use-cases for this type of messaging system.</li> <li>Identify the services that are available on a ROS network, and use ROS command-line tools to interrogate and call them.</li> <li>Develop Python Service Client Nodes.</li> <li>Invoke different services using various service-type interfaces.</li> </ol>"},{"location":"course/part4/#quick-links","title":"Quick Links","text":""},{"location":"course/part4/#exercises","title":"Exercises","text":"<ul> <li>Exercise 1: Using Command-line Tools to Interrogate a Service and its Interface</li> <li>Exercise 2: Playing the Number Game (from the Command-line)</li> <li>Exercise 3: Creating a Service Interface</li> <li>Exercise 4: Adapting the Number Game Server</li> <li>Exercise 5: Creating a Python Service Client</li> <li>Exercise 6: Developing A Map Saver Service Client</li> </ul>"},{"location":"course/part4/#additional-resources","title":"Additional Resources","text":"<ul> <li>The <code>number_game_client.py</code> Node (for Exercise 5)</li> </ul>"},{"location":"course/part4/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch your ROS2 Environment</p> <p>If you haven't done so already, launch your ROS environment now. Having done this, you should now have access to a Linux terminal instance (aka TERMINAL 1).</p> <p>Step 2: Restore your work (WSL-ROS2 Managed Desktop Users ONLY)</p> <p>Remember that any work you do within the WSL-ROS2 Environment will not be preserved between sessions or across different University computers, so you should be backing up your work to your <code>U:\\</code> drive regularly. When prompted (on first launch of WSL-ROS2 in TERMINAL 1) enter Y+Enter to restore your data<sup>1</sup>.</p> <pre><code>It looks like you already have a backup from a previous session:\n  U:\\wsl-ros\\ros2-backup-XXX.tar.gz\nDo you want to restore this now? [y/n]\n</code></pre> <p>Step 3: Launch VS Code </p> <p>It's also worth launching VS Code now. </p> WSL Users... <p>It's important to launch VS Code within your ROS environment using the \"WSL\" extension. Always remember to check for this: </p> <p></p> <p></p> <p>Step 4: Make Sure The Course Repo is Up-To-Date</p> <p>Once again, it's worth quickly checking that the Course Repo is up-to-date before you start on the Part 4 exercises. Go back to Part 1 if you haven't installed it yet (really?!) or - alternatively - see here for how to update.</p>"},{"location":"course/part4/#an-introduction-to-services","title":"An Introduction to Services","text":"<p>So far, we've learnt about ROS topics and the message-type interfaces that we use to transmit data on them. We've also learnt how individual nodes can access data on a robot by simply subscribing to topics that another node on the ROS network is publishing messages to. In addition to this, we know that any node can publish messages to any topic, which broadcasts data across the ROS network, making it available to any other node on the network that may wish to access it.</p> <p>Another way to pass data between ROS Nodes is by using Services. These are based on a call and response type of communication:</p> <ul> <li>A Service Client sends a Request to a Service Server.</li> <li>The Service Server processes that request and sends back a Response.</li> </ul> <p></p> <p>This is a bit like a transaction: one node requests something, and another node fulfils that request and responds. This is good for quick, short duration tasks, e.g.:</p> <ol> <li>Turning a device on or off.</li> <li>Grabbing some data and saving it to a file (a map for example).</li> <li>Performing a calculation and returning a result.</li> <li>Making a sound<sup>2</sup>.</li> </ol> <p>A single service can have many clients, but you can only have a single Server providing that particular service at any one time.</p> <p> </p> Multiple Clients to a single Service Server <p>Let's see how this all works in practice now, by playing a number game! We don't need a simulation up and running for this one, so in TERMINAL 1 use the following command to launch the Guess the Number Service: </p> <pre><code>ros2 run tuos_examples number_game.py\n</code></pre> <p>Having launched the service successfully, you should be presented with the following:</p> <pre><code>[INFO] [#####] [number_game_service]: The '/guess_the_number' service is active.\n[INFO] [#####] [number_game_service]: A secret number has been set... Game on!\n</code></pre> <p>We need to interrogate this now, in order to work out how to play the game...</p>"},{"location":"course/part4/#interrogating-a-service","title":"Interrogating a Service","text":""},{"location":"course/part4/#ex1","title":"Exercise 1: Using Command-line Tools to Interrogate a Service and its Interface","text":"<ol> <li> <p>Open up a new terminal instance (TERMINAL 2) and use the <code>ros2 service</code> command to list all active ROS services:</p> <pre><code>ros2 service list\n</code></pre> <p>There'll be a few items in this list, most of them with the prefix: <code>/number_game_service</code>. This is the name of the node that is providing the service (i.e. the Server) and these items are all automatically generated by ROS. What we're really interested in is the service itself, which should be listed as <code>/guess_the_number</code>. </p> </li> <li> <p>Next, we need to find the interface type used by this service, which we can do a couple of ways:</p> <ol> <li> <p>Use the <code>type</code> sub-command:</p> <pre><code>ros2 service type /guess_the_number\n</code></pre> </li> <li> <p>Use the <code>list</code> sub-command again, but with the <code>-t</code> flag:</p> <pre><code>ros2 service list -t\n</code></pre> <p>The latter will provide the same list of services as before, but each one will now have its interface type provided alongside it.</p> </li> </ol> </li> <li> <p>Regardless of the method that you used above, you should have discovered that the interface type used by the <code>/guess_the_number</code> service is:</p> <pre><code>tuos_interfaces/srv/NumberGame\n</code></pre> <p>Notice how (much like with message interfaces used by topics), there are three fields to this type definition:</p> <ol> <li><code>tuos_interfaces</code>: the name of the ROS package that this interface belongs to.</li> <li><code>srv</code>: that this is a service interface, the second interface type we've covered now (we'll learn about the third and final one in Part 5).</li> <li><code>NumberGame</code>: the data structure.</li> </ol> <p>We need to know the data structure in order to make a call to the service, so let's identify this next.</p> </li> <li> <p>We can use the <code>ros2 interface list</code> command to list all interface types available to us on our ROS system, but this will provide us with a long list!</p> <ol> <li> <p>We can use the <code>-m</code> flag to filter for message interfaces, or the <code>-s</code> flag to filter for service interfaces. Try the latter:</p> <pre><code>ros2 interface list -s\n</code></pre> <p></p> </li> <li> <p>Still quite a lot there, right!? Let's filter this further with Grep to identify only interfaces that belong to the <code>tuos_interfaces</code> package:</p> <pre><code>ros2 interface list -s | grep tuos_interfaces\n</code></pre> <p>Hopefully, the <code>srv/NumberGame</code> interface is now listed.</p> </li> <li> <p>Use the <code>show</code> sub-command to show the message structure:</p> <pre><code>ros2 interface show tuos_interfaces/srv/NumberGame\n</code></pre> </li> </ol> <p>The interface structure should be shown as follows:</p> <pre><code>int32 guess\n---\nint32 guesses\nstring hint\nbool success\n</code></pre> </li> </ol>"},{"location":"course/part4/#the-format-of-service-interfaces","title":"The Format of Service Interfaces","text":"<p>Service interfaces have two parts to them, separated by three hyphens (<code>---</code>). Above the separator is the Service Request, and below it is the Service Response:</p> <pre><code>int32 guess      &lt;-- Request\n---\nint32 guesses    &lt;-- Response (1 of 3)\nstring hint      &lt;-- Response (2 of 3)\nbool success     &lt;-- Response (3 of 3)\n</code></pre> <p>In order to Call a service, we need to provide data to it in the format specified in the Request section of the interface. A service Server will then send data back to the caller in the format specified in the Response section of the interface.</p> <p>The <code>tuos_interfaces/srv/NumberGame</code> service interface has only one request parameter:</p> <ol> <li><code>guess</code>: a <code>int32</code> (32-bit integer)     ...which is the only thing we need to send to the <code>/number_game_service</code> Service Server in order to call it.</li> </ol> <p>There are then three response parameters:</p> <ol> <li>A 32-bit integer called <code>guesses</code></li> <li>A text string called <code>hint</code> </li> <li> <p>A boolean flag called <code>success</code></p> <p>...all of which will be returned by the server, once it has processed our request.</p> </li> </ol>"},{"location":"course/part4/#ex2","title":"Exercise 2: Playing the Number Game (from the Command-line)","text":"<p>We're now ready to make a call to the service, and we can do this using the <code>ros2 service</code> command again (from TERMINAL 2):</p> <ol> <li> <p>To start, let's send an initial guess of <code>0</code> and see what happens:</p> <pre><code>ros2 service call /guess_the_number tuos_interfaces/srv/NumberGame \"{guess: 0}\"\n</code></pre> <p>The request will be echoed back to us, followed by a response, which will likely look something like this, and which shows us the value of the three response parameters that we identified above:</p> <pre><code>response:\ntuos_interfaces.srv.NumberGame_Response(guesses=1, hint='Higher', success=False)\n</code></pre> <ol> <li><code>guesses</code>: tells us how many times we've tried to guess the number in total (just once so far)</li> <li><code>hint</code>: tells us if we should go \"higher\" or \"lower\" on our next guess in order to get closer to the secret number</li> <li><code>success</code>: tells us if we guessed the right number or not (unlikely on the first attempt!)</li> </ol> </li> <li> <p>Make another service call, this time changing the value of your <code>guess</code>, e.g.:</p> <pre><code>ros2 service call /guess_the_number tuos_interfaces/srv/NumberGame \"{guess: 10}\"\n</code></pre> </li> <li> <p>Try making a guess of 500 next.</p> <p>The service should respond with the hint <code>'Error'</code> now. Have a look back in TERMINAL 1 (where the Server is running) to get more information on this.</p> </li> <li> <p>Keep going until you guess the magic number, how many guesses does it take you?! </p> </li> <li> <p>Stop the server, by entering Ctrl+C in TERMINAL 1.</p> </li> </ol>"},{"location":"course/part4/#creating-our-own-services","title":"Creating Our Own Services","text":"<p>Over the next three exercises, we'll learn how to create a service interface of our own, and build a Server and Client (in Python) that use this.</p> <p>First though, we need to create a new package, so follow the same procedure as you have in the previous parts of this course to create another new package called <code>part4_services</code>.</p> <p>In TERMINAL 1:</p> <ol> <li> <p>Head to the ROS 2 workspace <code>src</code> folder:</p> <pre><code>cd ~/ros2_ws/src/\n</code></pre> </li> <li> <p>Clone the package template:</p> <pre><code>git clone https://github.com/tom-howard/ros2_pkg_template.git\n</code></pre> </li> <li> <p>Call the <code>init_pkg.sh</code> script and specify your package name:</p> <pre><code>./ros2_pkg_template/init_pkg.sh part4_services\n</code></pre> </li> </ol>"},{"location":"course/part4/#ex3","title":"Exercise 3: Creating a Service Interface","text":"<p>Let's create a service interface now which has a similar structure to the one used by the <code>/guess_the_number</code> service, but this time with two request parameters, rather than just one...</p> <ol> <li> <p>In TERMINAL 1, navigate into the root of your <code>part4_services</code> package directory:</p> <pre><code>cd ~/ros2_ws/src/part4_services/\n</code></pre> </li> <li> <p>Create a new directory there called <code>srv</code>:</p> <pre><code>mkdir srv\n</code></pre> </li> <li> <p>Create a new file in this directory called <code>MyNumberGame.srv</code>:</p> <pre><code>touch srv/MyNumberGame.srv\n</code></pre> <p>In here is where we will define the structure of our own <code>MyNumberGame</code> service interface.</p> </li> <li> <p>Open up this file in VS Code, enter the following content and save the file:</p> MyNumberGame.srv<pre><code>int32 guess\nbool cheat\n---\nint32 num_guesses\nstring hint\nbool correct\n</code></pre> <p>The Request will therefore have two fields now:</p> <p></p><p></p> # Field Name Data Type 1 <code>guess</code> <code>int32</code> 2 <code>cheat</code> <code>bool</code> <p></p><p></p> </li> <li> <p>The rest of the process now is very similar to creating a message interface, like we did in Part 1. </p> <p>First, we need to declare the interface in our <code>CMakeLists.txt</code> file, by adding the following above the <code>ament_package()</code> line:</p> part4_services/CMakeLists.txt<pre><code>find_package(rosidl_default_generators REQUIRED)\nrosidl_generate_interfaces(${PROJECT_NAME}\n  \"srv/MyNumberGame.srv\" \n)\n</code></pre> </li> <li> <p>Next, we need to modify the <code>package.xml</code> file. Add the following lines to this one, just above the <code>&lt;export&gt;</code> line:</p> package.xml<pre><code>&lt;buildtool_depend&gt;rosidl_default_generators&lt;/buildtool_depend&gt;\n&lt;exec_depend&gt;rosidl_default_runtime&lt;/exec_depend&gt;\n&lt;member_of_group&gt;rosidl_interface_packages&lt;/member_of_group&gt;\n</code></pre> </li> <li> <p>And finally, we use Colcon to generate the necessary source code for the service interface:</p> <ol> <li> <p>Navigate to the root of the ROS2 Workspace:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Run <code>colcon build</code>:</p> <pre><code>colcon build --packages-select part4_services --symlink-install \n</code></pre> </li> <li> <p>And re-source the <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Let's verify that this worked, using the <code>ros2</code> CLI (the same way as we did earlier when interrogating <code>tuos_interfaces/srv/NumberGame</code>):</p> <ol> <li> <p>First, list all the ROS service interfaces that are available on the system (<code>-s</code> to filter for service interface types remember!):</p> <pre><code>ros2 interface list -s\n</code></pre> <p>Scroll through this list and see if you can find <code>part4_services/srv/MyNumberGame</code> (or, use <code>grep</code> again).</p> </li> <li> <p>If it's there, use the <code>show</code> sub-command to show the data structure:</p> <pre><code>ros2 interface show part4_services/srv/MyNumberGame\n</code></pre> </li> </ol> <p>Does it match with the definition in our <code>MyNumberGame.srv</code> file?</p> </li> </ol>"},{"location":"course/part4/#ex4","title":"Exercise 4: Adapting the Number Game Server","text":"<p>We're going to take a copy of the <code>tuos_examples/number_game.py</code> server node now, and adapt it to use the service interface that we created above.</p> <ol> <li> <p>In TERMINAL 1 still, navigate into the <code>part4_services/scripts</code> directory:</p> <pre><code>cd ~/ros2_ws/src/part4_services/scripts/\n</code></pre> </li> <li> <p>Copy the <code>number_game.py</code> script from the course repo into here, renaming it to <code>my_number_game.py</code> at the same time:</p> <pre><code>cp ../../tuos_ros/tuos_examples/scripts/number_game.py ./my_number_game.py\n</code></pre> <p>This file should already have execute permissions, but it's always worth checking...</p> </li> <li> <p>Declare this as a package executable by going back to the <code>CMakeLists.txt</code> file and adding <code>my_number_game.py</code> to your list of Python executables:</p> CMakeLists.txt<pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/basic_velocity_control.py\n  scripts/stop_me.py\n  scripts/my_number_game.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Build and re-source now: </p> <p></p><pre><code>cd ~/ros2_ws/\n</code></pre> <pre><code>colcon build --packages-select part4_services --symlink-install\n</code></pre> <pre><code>source ~/.bashrc \n</code></pre><p></p> </li> <li> <p>Now, let's look at the code, and see what needs to be adapted:</p> <ol> <li> <p>Open up the <code>my_number_game.py</code> node in VS Code and review it.</p> </li> <li> <p>As it stands, the node imports the <code>NumberGame</code> service interface from <code>tuos_interfaces</code>, so you'll need to change this to use the interface from your own package now:</p> <pre><code>from part4_services.srv import MyNumberGame\n</code></pre> <p>You'll also need to change the <code>srv_type</code> definition, when the service is created in the <code>__init__()</code>:</p> <pre><code>self.srv = self.create_service(\n    srv_type=MyNumberGame,\n    srv_name='guess_the_number',\n    callback=self.srv_callback\n)\n</code></pre> </li> <li> <p>Everything that this service does (when a Request is sent to it), is contained within the <code>srv_callback()</code> method. </p> <p>In here, <code>request</code> parameters are processed, <code>response</code> parameters are defined and the overall <code>response</code> is returned once the callback tasks have been completed.</p> </li> <li> <p>You may have noticed that when we created our <code>MyNumberGame</code> interface in the previous exercise, the Response parameters were almost the same as those from Exercises 1 &amp; 2, except that some of their names had been changed slightly!</p> <p>Work through the <code>srv_callback()</code> method and make sure that all <code>response</code> attributes are renamed to match the new names that we've given them in our <code>part4_services/srv/MyNumberGame</code> interface. </p> </li> <li> <p>Remember that our <code>MyNumberGame</code> interface has an additional Request parameter too: </p> <pre><code>bool cheat\n</code></pre> <p>... i.e. a boolean flag with the attribute name <code>cheat</code>.</p> <p>Adapt the <code>srv_callback()</code> method further now to read this value as well. </p> <ul> <li> <p>If (when a request is made to the server) the value of <code>cheat</code> is <code>True</code> the hint that the server returns should contain the actual secret number! E.g.:</p> <pre><code>hint='The answer is 67!'\n</code></pre> </li> <li> <p>In such situations, the value of <code>response.num_guesses</code> should still go up by one, and the <code>correct</code> flag should still return <code>False</code>.</p> </li> </ul> </li> </ol> </li> <li> <p>Once you've adapted the node, test it out by running it:</p> <p>TERMINAL 1: </p><pre><code>ros2 run part4_services my_number_game.py\n</code></pre><p></p> <p>You should then be able to make calls to this from TERMINAL 2 using the <code>ros2 service call</code> sub-command, as we did in Exercise 2.</p> <p>Hint</p> <p>There are two request parameters now, so when sending a request from the command-line both need to be supplied. Do this by including them both within the braces (<code>{}</code>) and separated with a comma, e.g.</p> <pre><code>ros2 service call /guess_the_number part4_services/srv/MyNumberGame \"{guess: X, cheat: Y}\" \n</code></pre> </li> </ol>"},{"location":"course/part4/#ex5","title":"Exercise 5: Creating a Python Service Client","text":"<p>So far we've been making service calls from the command-line, but we can also call services from within Python Nodes. When a node calls (i.e. requests) a service, it becomes a Service \"Client\".</p> <ol> <li> <p>Make sure your <code>my_number_game.py</code> node is still active in TERMINAL 1 for this exercise.</p> </li> <li> <p>In TERMINAL 2, create a new file in the <code>part4_services/scripts</code> directory called <code>number_game_client.py</code> (using the <code>touch</code> command).</p> </li> <li> <p>Make this executable (with <code>chmod</code>).</p> </li> <li> <p>Add this to your package's Python executables list in the <code>CMakeLists.txt</code> file:</p> CMakeLists.txt<pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/basic_velocity_control.py\n  scripts/stop_me.py\n  scripts/my_number_game.py\n  scripts/number_game_client.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>Re-build your package (as before), remembering that there are three steps to this:</p> <ol> <li>Navigate to the root of the ROS2 workspace,</li> <li>Run <code>colcon build</code> (with the necessary additional arguments), </li> <li>Re-source your <code>~/.bashrc</code>.</li> </ol> </li> <li> <p>Now, open up the <code>number_game_client.py</code> file in VS Code, then take a look at the following:</p> <p></p> The <code>number_game_client.py</code> Node<p></p> <p>Review the above code carefully (including all the annotations), before taking a copy of it and pasting it into your own <code>number_game_client.py</code> file.</p> </li> <li> <p>You should now be able to run the code with <code>ros2 run</code>. To begin with, use a standard <code>ros2 run</code> call as follows:</p> <pre><code>ros2 run part4_services number_game_client.py\n</code></pre> <p>You should then get an output like this:</p> <pre><code>[INFO] [#####] [number_game_client]: Sending the request:\n - guess: 0\n - cheat: False\n   Awaiting response...\n[INFO] [#####] [number_game_client]: The server responded with:\n - Incorrect guess :(\n - Number of attempts so far: 1\n - A hint: 'Higher'.\n</code></pre> <p>Notice how the request parameters <code>guess</code> and <code>cheat</code> have defaulted to <code>0</code> and <code>False</code> respectively?</p> </li> <li> <p>The main reason for using parameters in our <code>number_game_client</code> node is so that we can actually change the value of the guess from the command-line. We can do this by adding some additional arguments to the <code>ros2 run</code> call:</p> <pre><code>ros2 run part4_services number_game_client.py --ros-args -p guess:=X\n</code></pre> <p>... replace <code>X</code> with an actual number!</p> </li> <li> <p>Have a go at cheating now too:</p> <pre><code>ros2 run part4_services number_game_client.py --ros-args -p cheat:=True\n</code></pre> </li> <li> <p>We can also set the two parameters simultaneously using multiple <code>-p</code> flags:</p> <pre><code>ros2 run part4_services number_game_client.py \\\n    --ros-args -p guess:=99 -p cheat:=True\n</code></pre> <p>Note</p> <p>The <code>\\</code> above just allows us to split the command across two separate lines, handy for when these things start to get a bit long!</p> </li> </ol>"},{"location":"course/part4/#the-map-saver-service","title":"The Map Saver Service","text":"<p>Clearly the examples that we've been working with here so far have been fairly trivial: it's unlikely that you'll ever need to program a robot to play the number game! The aim however has been to illustrate how ROS Services work, and how to develop your own. </p> <p>One application that you might find useful however is map saving. In Part 3 we learnt about SLAM, where we drove a robot around in an environment while SLAM algorithms were working in the background to generate a map of the world using data from the robot's LiDAR sensor and its odometry system:</p> <p></p> <p>Having mapped out the environment, we called up the <code>map_saver_cli</code> node from the <code>nav2_map_server</code> package, to save a copy of that map to a file:</p> <pre><code>ros2 run nav2_map_server map_saver_cli -f MAP_NAME\n</code></pre> <p>... wouldn't it be nice if there was a way to be able to do this programmatically (i.e. from within a Python node, for example) rather than having to run the above command manually? Well, there is a way, and guess what? ... It involves Services!</p>"},{"location":"course/part4/#ex6","title":"Exercise 6: Developing A Map Saver Service Client","text":"<ol> <li> <p>Make sure everything in TERMINALS 1 and 2 from the previous exercises are closed down now.</p> </li> <li> <p>In TERMINAL 1, let's fire up the Nav World again, like we did in Part 3:</p> <pre><code>ros2 launch tuos_simulations nav_world.launch.py\n</code></pre> <p></p> <p></p> </li> <li> <p>In TERMINAL 2, let's also fire up Cartographer again (the SLAM algorithms):</p> <pre><code>ros2 launch tuos_tb3_tools slam.launch.py environment:=sim\n</code></pre> <p></p> <p></p> </li> <li> <p>Open up another terminal instance now (TERMINAL 3), and use this one to fire up the Map Saver Service (wouldn't it be nice if we could launch all of these launch files at once?<sup>3</sup>):</p> <pre><code>ros2 launch nav2_map_server map_saver_server.launch.py\n</code></pre> <p>This will add a number of <code>/map_saver</code> services to our ROS network.</p> </li> <li> <p>In yet another new terminal instance (TERMINAL 4), use a <code>ros2 service</code> sub-command to identify all the <code>/map_saver</code> services (like we did in Exercise 1).</p> <p>Question</p> <p>Do you see an item in this list that could be related to saving a map? (It has a <code>/map_saver</code> prefix!<sup>4</sup>)</p> </li> <li> <p>Use another <code>ros2 service</code> sub-command to determine the type of interface used by this service (again, like we did in Exercise 1).</p> </li> <li> <p>Next, use a <code>ros2 interface</code> command to discover the structure of this service interface.</p> <p>Questions</p> <ol> <li>How many Request parameters does this interface have?</li> <li>How many Response parameters are there too?<sup>5</sup></li> <li>What are their data types?</li> </ol> </li> <li> <p>Develop a Python Service Client to make calls to this service:</p> <ol> <li> <p>Create a new Node in your <code>part4_services</code> package called <code>map_saver_client.py</code> for this. </p> <p>Things to remember when doing this:</p> <ul> <li> Create this in your <code>part4_services/scripts</code> directory.</li> <li> Make sure it has execute permissions.</li> <li> Declare it as a package executable in your <code>CMakeLists.txt</code>.</li> <li> Re-build your package with <code>colcon</code>, making sure you follow the full three-step process </li> </ul> </li> <li> <p>Use the <code>number_game_client.py</code> Node from Exercise 5 as a starting point when building your new <code>map_saver_client.py</code> node... all the same principals will apply here, you are just applying them to a different service (and therefore you need to account for a different service interface).</p> </li> <li> <p>Declare a parameter for your node to allow you to specify a file name for your map when the <code>map_saver_client.py</code> node is called e.g.:</p> <pre><code>ros2 run part4_services map_saver_client.py \\ \n    --ros-args -p map_file:=my_amazing_map\n</code></pre> <p>Follow the same approach as in Exercise 5 for this, and ensure that you specify a default value, for situations where the parameter value is not explicitly defined when the node is called.</p> </li> <li> <p>When constructing service requests, consider the following tips:</p> <ol> <li>The SLAM map data (as generated by Cartographer) is published to a topic called <code>/map</code>. </li> <li> <p>When providing a name for the map file:</p> <ul> <li>You don't need to include a file extension</li> <li> <p>File names are interpreted relative to your home directory, so:</p> <p><code>my_amazing_map</code> would result in a map file at <code>~/my_amazing_map.yaml</code></p> <p><code>my/amazing/map</code> would result in a map file at <code>~/my/amazing/map.yaml</code> (assuming the directory structure already exists!)</p> </li> </ul> </li> <li> <p>For further guidance see here for a usage example.</p> </li> <li>The server will apply its own defaults to certain request attributes, if they aren't defined explicitly by a client.</li> </ol> </li> </ol> </li> </ol>"},{"location":"course/part4/#summary-of-the-map-saver-service","title":"Summary of the Map Saver Service","text":"<p>Back in Part 3 we saved our SLAM map once via a command-line call after we had fully explored and mapped out the environment:</p> <pre><code>ros2 run nav2_map_server map_saver_cli -f MAP_NAME\n</code></pre> <p>... which resulted in something like:</p> <p> </p> The <code>MAP_NAME.pgm</code> file <p>In real-world tasks however (i.e. tasks that you might need to complete in the lab!), your robot might be exploring an environment autonomously, and you don't necessarily know when the full environment has been explored, nor are you always going to be there to run the <code>map_saver_cli</code> node manually! You might therefore want to program your robot with the ability to save a map incrementally and periodically as more and more of the environment is explored.</p> <p>The process that we explored in the previous exercise allows you to do just that! In the example, our client node was programmed to make only one request to the server and then stop. It could however be programmed to make regular service requests (say, once every 5 or 10 seconds) in order to continuously update its map as the robot explores further and further.</p> <p>Think about how you might adapt the <code>map_saver_client.py</code> node to achieve this, drawing upon other exercises that you have worked through in previous parts of this course.  </p>"},{"location":"course/part4/#wrapping-up","title":"Wrapping Up","text":"<p>In Part 4 we've learnt about ROS Services and why they might be useful for robot applications:</p> <ul> <li>Services differ from standard topic-based communication methods in ROS in that they are a call and response type of communication, taking place between one node and another.  </li> <li>Typically, a service Caller will request a service, and then wait for a response (although it is possible to do other things in the meantime).</li> <li>In general however, Services are useful for controlling quick, short-duration tasks or calculations.</li> </ul>"},{"location":"course/part4/#backup","title":"WSL-ROS2 Managed Desktop Users: Save your work!","text":"<p>Remember, to save the work you have done in WSL-ROS2 during this session so that you can restore it on a different machine at a later date. Run the following script in any idle WSL-ROS2 Terminal Instance now:</p> <pre><code>wsl_ros backup\n</code></pre> <p>You'll then be able to restore it to a fresh WSL-ROS2 environment next time you fire one up (<code>wsl_ros restore</code>).  </p> <ol> <li> <p>Remember: you can also use the <code>wsl_ros restore</code> command at any time.\u00a0\u21a9</p> </li> <li> <p>On the real Waffles, there's a service called <code>/sound</code>. Have a look at this next time you're in the lab... Once you've worked through the whole of Part 4 you'll know exactly how to interrogate this service and leverage the functionality that it provides!\u00a0\u21a9</p> </li> <li> <p>You can! We can use launch files to launch other launch files, see here for more info.\u00a0\u21a9</p> </li> <li> <p>There should be one in the list called <code>/map_saver/save_map</code> \u21a9</p> </li> <li> <p>The <code>nav2_msgs/srv/SaveMap</code> Service Interface has 6 Requests (<code>map_topic</code>, <code>map_url</code>, <code>image_format</code>, <code>map_mode</code>, <code>free_thresh</code>, <code>occupied_thresh</code>) and 1 Response (<code>result</code>).\u00a0\u21a9</p> </li> </ol>"},{"location":"course/part5/","title":"Part 5: Actions","text":""},{"location":"course/part5/#introduction","title":"Introduction","text":"<p> Exercises: 6 (5 core, 1 advanced) Estimated Completion Time: 3 hours (core exercises only) Difficulty Level: Advanced </p>"},{"location":"course/part5/#aims","title":"Aims","text":"<p>In this part of the course we'll learn about a third method of communication available in ROS: Actions.  Actions are essentially an advanced version of Services, and we'll look at exactly how these two differ and why you might choose to employ an action over a service for certain tasks. </p>"},{"location":"course/part5/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Recognise how ROS Actions differ from ROS Services and explain where this method might be useful in robot applications.</li> <li>Explain the structure of Action interfaces and identify the relevant information within them, enabling you to build Action Servers and Clients.</li> <li>Implement Python Action Client nodes that can handle feedback and results and can also cancel an action part-way through.</li> <li>Develop Action Server &amp; Client nodes that could be used as the basis for a robotic exploration strategy.</li> </ol>"},{"location":"course/part5/#quick-links","title":"Quick Links","text":""},{"location":"course/part5/#exercises","title":"Exercises","text":"<ul> <li>Exercise 1: Launching an Action Server and calling it from the command-line</li> <li>Exercise 2: Building a Python Action Client Node</li> <li>Exercise 3: Creating an Action Interface</li> <li>Exercise 4: Building the \"ExploreForward\" Action Server</li> <li>Exercise 5: Building a Basic \"ExploreForward\" Client</li> <li>Exercise 6 (Advanced): Implementing an Exploration Strategy</li> </ul>"},{"location":"course/part5/#additional-resources","title":"Additional Resources","text":"<ul> <li>A Minimal Action Client (for Exercise 2)</li> <li>The Explore Server Template (for Exercise 4)</li> </ul>"},{"location":"course/part5/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch your ROS Environment</p> <p>Launch your ROS environment now so that you have access to a Linux terminal instance (aka TERMINAL 1).</p> <p>Step 2: Restore your work (WSL-ROS2 Managed Desktop Users ONLY)</p> <p>Remember that any work that you do within the WSL-ROS2 Environment will not be preserved between sessions or across different University computers, and so you should be backing up your work to your <code>U:\\</code> drive regularly. When prompted (on first launch of WSL-ROS2 in TERMINAL 1) enter <code>Y</code> to restore this<sup>1</sup>.</p> <p>Step 3: Launch VS Code </p> <p>WSL users remember to check for this.</p> <p>Step 4: Make Sure The Course Repo is Up-To-Date</p> <p>Check that the Course Repo is up-to-date before you start on these exercises. See here for how to install and/or update.</p>"},{"location":"course/part5/#calling-an-action-server","title":"Calling an Action Server","text":"<p>Before we talk about what actions actually are, we're going to dive straight in and see one in action (excuse the pun). </p>"},{"location":"course/part5/#ex1","title":"Exercise 1: Launching an Action Server and calling it from the command-line","text":"<p>We'll play a little game here. We're going to launch our TurtleBot3 Waffle in a mystery environment now, and we're going to do this by launching Gazebo headless i.e. Gazebo will be running behind the scenes, but there'll be no Graphical User Interface (GUI) to show us what the environment actually looks like.  Then, we'll use an action server to make our robot scan the environment and take pictures for us, to reveal its surroundings!</p> <ol> <li> <p>To launch the TurtleBot3 Waffle in this mystery environment, use the following <code>ros2 launch</code> command in TERMINAL 1:</p> <pre><code>ros2 launch tuos_simulations mystery_world.launch.py\n</code></pre> <p>... nothing will appear to happen (for now!)</p> </li> <li> <p>Next, open up a new terminal (TERMINAL 2), and have a look at all the topics that are currently active on the ROS network (you should know exactly how to do this by now!)</p> <p>The output of this should confirm that ROS and our robot are indeed active...</p> How? <p>When the robot is active, the output of the <code>ros2 topic list</code> command should provide a long list of topics, a number of which we've been working with throughout this course so far, such as <code>cmd_vel</code>, <code>odom</code>, <code>scan</code>, and so on. If the Waffle simulation isn't active then we would be presented with a much smaller list, containing only the core ROS topics:</p> <p>TERMINAL 2: </p><pre><code>$ ros2 topic list\n/parameter_events\n/rosout\n</code></pre><p></p> </li> <li> <p>Next (still in TERMINAL 2), run the following command to launch an Action Server on the network:</p> <pre><code>ros2 run tuos_examples camera_sweep_action_server.py\n</code></pre> </li> <li> <p>Now, open up another new terminal instance (TERMINAL 3), but so that you can view this and TERMINAL 2 side-by-side. Enter the following command to list all actions that are active on the ROS network:</p> <pre><code>ros2 action list\n</code></pre> <p>There should be an item here called <code>/camera_sweep</code>, use the <code>info</code> command to find out more about this:</p> <pre><code>ros2 action info /camera_sweep\n</code></pre> <p>This tells us the name of the action: <code>Action: /camera_sweep</code>, as well as the number of client and server nodes this action has. Currently, the action should have 0 clients and 1 server, and the node acting as the server here should be listed as <code>/camera_sweep_action_server_node</code> (the node that we just launched with the <code>ros2 run</code> command in TERMINAL 2).</p> <p>Finally, call the <code>ros2 action info</code> command again, but this time providing an additional argument:</p> <pre><code>ros2 action info -t /camera_sweep\n</code></pre> <pre><code>Action: /camera_sweep\nAction clients: 0\nAction servers: 1\n    /camera_sweep_action_server [tuos_interfaces/action/CameraSweep]\n</code></pre> <p>The <code>-t</code> argument additionally shows the action type against the server node, indicating to us the type of interface used by the server.</p> </li> <li> <p>Let's now find out more about the interface itself. As with any interface (message, service or action) we can use the <code>ros2 interface</code> command to do this. In TERMINAL 3 enter the following:</p> <pre><code>ros2 interface show tuos_interfaces/action/CameraSweep\n</code></pre> <p>Which should present us with the following:</p> <pre><code>#goal\nfloat32 sweep_angle   # Angular sweep (in degrees) over which to capture images \nint32 image_count     # Number of images to capture during the sweep\n---\n#result\nstring[] image_paths  # Full path to each of the captured images\n---\n#feedback\nint32 current_image   # Number of images taken\nfloat32 current_angle # Current angular position of the robot (in degrees)\n</code></pre> <p>There are three parts to an action interface, and we'll talk about these in a bit more detail later on, but for now, all we need to know is that in order to call an action, we need to send the action server a Goal.</p> Comparing with ROS Services <p>This is a bit like sending a Request to a ROS Service Server, like we did in the previous session.</p> </li> <li> <p>We can issue a goal to an action server from the command-line using the <code>ros2 action</code> command again. Let's give this a go in TERMINAL 3.</p> <p>First, let's identify the right <code>ros2 action</code> sub-command:</p> <pre><code>ros2 action --help\n</code></pre> <pre><code>Commands:\n  info       Print information about an action\n  list       Output a list of action names\n  send_goal  Send an action goal\n</code></pre> <p>As above, there are three sub-commands to choose from, and we've already used the first two! Clearly, the <code>send_goal</code> command is the one we want now.</p> <p>Let's get some help on this one:</p> <pre><code>ros2 action send_goal --help\n</code></pre> <p>From this, we learn that there are three positional arguments, which must be supplied in the correct order:</p> <pre><code>ros2 action send_goal &lt;action_name&gt; &lt;action_type&gt; &lt;goal&gt;\n</code></pre> <p>We know from our earlier interrogation with the <code>ros2 action list</code>, <code>info</code> and <code>ros2 interface show</code> commands how to provide the right data here:</p> <ol> <li><code>action_name</code>: <code>/camera_sweep</code></li> <li><code>action_type</code>: <code>tuos_interfaces/action/CameraSweep</code></li> <li><code>goal</code>: a data packet (in YAML format) containing two parameters:<ol> <li><code>sweep_angle</code>: the angle (in degrees) that the robot will rotate on the spot (i.e. 'sweep')</li> <li><code>image_count</code>: the number of images it will capture from its front-facing camera while 'sweeping'</li> </ol> </li> </ol> </li> <li> <p>Now, again in TERMINAL 3, have a go at using the <code>ros2 action send_goal</code> command, but keep an eye on TERMINAL 2 as you do this:</p> <pre><code>ros2 action send_goal /camera_sweep tuos_interfaces/action/CameraSweep \\\n    \"{sweep_angle: 0, image_count: 0}\"\n</code></pre> <p>Having called the action, you should then be presented with a message (in TERMINAL 3) that the <code>Goal was rejected.</code> In TERMINAL 2 (where the action server is running), we should see some additional information about why this was the case. Read this, and then head back to TERMINAL 3 and have another go at sending a goal to the action server, by supplying valid inputs this time!</p> <p>Once valid goal parameters have been supplied, the action server (in TERMINAL 2), will respond to inform you of what it's going to do. You'll then need to wait for it to do its job...</p> </li> <li> <p>Once the action has completed (it could up to 20 seconds), a message should appear in TERMINAL 3 to inform us of the outcome:</p> <pre><code>Result:\n    image_paths: \n- ~/ros_action_examples/img01.jpg\n- ~/ros_action_examples/img02.jpg\n- ~/ros_action_examples/img03.jpg\n- ...\n\nGoal finished with status: SUCCEEDED\n</code></pre> <p>Additionally, we should see some further text in TERMINAL 2 as well:</p> <pre><code>[INFO] [#####] [camera_sweep_action_server_node]: camera_sweep_action_server_node completed successfully:\n  - Angular sweep = # degrees\n  - Images captured = #\n  - Time taken = # seconds\n</code></pre> <ol> <li> <p>The result of the action (presented to us in TERMINAL 3) is a series file paths, illustrating the images that have been captured, and where they have been saved on the filesystem. Navigate to this directory in TERMINAL 3 (using <code>cd</code>) and have a look at the content using <code>ll</code> (a handy alias for the <code>ls</code> command):</p> <pre><code>cd ~/ros_action_examples\n</code></pre> <pre><code>ll\n</code></pre> <p>You should see the same number of image files in there as was requested with the <code>image_count</code> parameter.</p> </li> <li> <p>Launch <code>eog</code> in this directory and click through all the images to reveal your robot's mystery environment:</p> <pre><code>eog .\n</code></pre> </li> </ol> </li> <li> <p>Let's do this one more time. Close down the <code>eog</code> window, head back to TERMINAL 3 and issue the <code>ros2 action send_goal</code> command again, but this time use the optional <code>-f</code> flag: </p> <pre><code>ros2 action send_goal -f /camera_sweep tuos_interfaces/action/CameraSweep \\\n    \"{sweep_angle: 0, image_count: 0}\" \n</code></pre> <p>Tip</p> <p>Don't forget to supply valid goal parameters again!</p> <p>Now, as well as being provided with a result once the action has completed, we're also provided with some regular updates while the action is in progress (aka \"feedback\")! </p> </li> <li> <p>To finish off, close down the action server in TERMINAL 2 and the headless Gazebo process in TERMINAL 1 by entering Ctrl+C in each terminal. </p> </li> </ol>"},{"location":"course/part5/#what-is-a-ros-action","title":"What is a ROS Action?","text":"<p>In the above exercise we launched an action server and then called it from the command-line using the <code>ros2 action send_goal</code> sub-command. In the same way as a ROS Service, the action also provided us with a result once the task had been completed.</p> <p>Using the <code>-f</code> flag we were able to ask the server to provide us with real-time feedback on how it was getting on (in TERMINAL 3).  Feedback is one of the key features that differentiates a ROS Action from a ROS Service: An Action Server provides feedback at regular intervals whilst working towards its goal. Another feature of ROS Actions is that they can be cancelled part-way through (which we'll play around with shortly).</p> <p></p> <p>Ultimately, Actions use a combination of both Topic- and Service-based communication, to create a more advanced messaging protocol. Building on ROS Services, Actions are designed to be used for longer running tasks, due to the provision of feedback and the ability to cancel a process part-way through. You can read more about Actions in the official ROS 2 documentation here (which also includes a nice animation to explain how they work).</p>"},{"location":"course/part5/#the-format-of-action-interfaces","title":"The Format of Action Interfaces","text":"<p>Like Services, Action Interfaces have multiple parts to them, and we need to know what format these action messages take in order to be able to use them.</p> <p>We ran <code>ros2 interface show</code> in the previous exercise, to interrogate the action interface used by the <code>/camera_sweep</code> action server:</p> <pre><code>ros2 interface show tuos_interfaces/action/CameraSweep\n</code></pre> <pre><code>#goal\nfloat32 sweep_angle   # Angular sweep (in degrees) over which to capture images \nint32 image_count     # Number of images to capture during the sweep\n---\n#result\nstring[] image_paths  # Full path to each of the captured images\n---\n#feedback\nint32 current_image   # Number of images taken\nfloat32 current_angle # Current angular position of the robot (in degrees)\n</code></pre> <p>As we know from Exercise 1, in order to call this action server we need to send a goal, and (in this case) there are two goal parameters that must be provided:</p> <ol> <li><code>sweep_angle</code>: a 32-bit floating-point value</li> <li><code>image_count</code>: a 32-bit integer</li> </ol> <p>Questions</p> <ul> <li>What are the names of the result and feedback interface parameters? (There are three in total.)</li> <li>What data types do these parameters use?</li> </ul> <p>You'll learn how we use this information to develop Python Action Server &amp; Client nodes in the following exercises.</p>"},{"location":"course/part5/#creating-python-action-clients","title":"Creating Python Action Clients","text":"<p>In the previous exercise we called a pre-existing Action Server from the command-line, by sending a goal to it. Let's look at how we can do this from within a Python ROS node now.</p>"},{"location":"course/part5/#ex2","title":"Exercise 2: Building a Python Action Client Node","text":"<ol> <li> <p>In TERMINAL 1 launch the mystery world simulation again, but this time with an additional argument:</p> <pre><code>ros2 launch tuos_simulations mystery_world.launch.py with_gui:=true\n</code></pre> <p>With the <code>with_gui</code> switch set to <code>true</code>, we should now be able to actually see the simulated world this time.</p> <p></p> (Not really that exciting is it!?)<p></p> </li> <li> <p>Then, in TERMINAL 2, launch the Camera Sweep Action Server again: </p> <pre><code>ros2 run tuos_examples camera_sweep_action_server.py\n</code></pre> </li> </ol>"},{"location":"course/part5/#part-1-a-minimal-action-client","title":"Part 1: A Minimal Action Client","text":"<ol> <li> <p>Now, in TERMINAL 3, create a new package called <code>part5_actions</code> using the approach that we've been using to create packages throughout this course.</p> </li> <li> <p>Navigate into the <code>scripts</code> folder of your package using the <code>cd</code> command:</p> <pre><code>cd ~/ros2_ws/src/part5_actions/scripts/\n</code></pre> </li> <li> <p>In here, create a new Python file called <code>camera_sweep_action_client.py</code> (using the <code>touch</code> command) and make it executable (using <code>chmod</code>). </p> </li> <li> <p>Then, declare this as an executable in the package's <code>CMakeLists.txt</code>:</p> CMakeLists.txt<pre><code># Install Python executables\ninstall(PROGRAMS\n  scripts/basic_velocity_control.py\n  scripts/stop_me.py\n  scripts/camera_sweep_action_client.py\n  DESTINATION lib/${PROJECT_NAME}\n)\n</code></pre> </li> <li> <p>At an absolute minimum, the Action Client can be constructed as follows:</p> <p></p> The <code>camera_sweep_action_client.py</code> Node<p></p> </li> <li> <p>Let's build the Node now, so that we can run it. Head back to TERMINAL 3 and use Colcon to build the package: </p> <pre><code>cd ~/ros2_ws/ \n</code></pre> <pre><code>colcon build --packages-select part5_actions --symlink-install\n</code></pre> </li> <li> <p>Re-source the <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> <li> <p>Run your node with <code>ros2 run</code>:</p> <pre><code>ros2 run part5_actions camera_sweep_action_client.py\n</code></pre> <p>With the command above as it is, the server will reject the goal (check back in TERMINAL 2 to find out why). How can you modify the above command (by adding some additional arguments), so that the <code>camera_sweep_action_client.py</code> node actually sends a valid goal?<sup>2</sup></p> </li> <li> <p>Having modified the <code>ros2 run</code> command to successfully send a valid goal to the <code>camera_sweep</code> action server, the robot should start to turn, capturing images as it goes. </p> <p>The Client that we've created here makes a call to the action server and then exits, it doesn't even wait for the server to finish its task! The only way that we'd know if the action was successful, is if we were to keep an eye on TERMINAL 2, to see the action server respond to the goal that was sent to it... The client itself provides no feedback during the action, nor the result at the end. Let's look to incorporate that now...</p> </li> </ol>"},{"location":"course/part5/#part-2-handling-a-result","title":"Part 2: Handling a Result","text":"<ol> <li> <p>Go back to the <code>camera_sweep_action_client.py</code> file in VS Code.</p> </li> <li> <p>In order to be able to handle the result that is sent from an action server, we first need to handle the response that the server sends to the goal itself.</p> <p>Within the <code>send_goal()</code> method of the <code>CameraSweepActionClient()</code> class, find the line that reads:</p> <pre><code>return self.actionclient.send_goal_async(goal)\n</code></pre> <p>and change this to:</p> <pre><code>self.send_goal_future = self.actionclient.send_goal_async(goal)\nself.send_goal_future.add_done_callback(self.goal_response_callback)\n</code></pre> <p>This method is no longer returning the future that is sent from <code>send_goal_async()</code>, but is now handling this and adding a callback to it: <code>goal_response_callback</code>. This callback can now be used to inform the client of whether the server has accepted the goal or not.</p> </li> <li> <p>We therefore need to define this callback now. Define it as a new class method of the <code>CameraSweepActionClient()</code> class (i.e. underneath the <code>send_goal()</code> class method that has already been defined)...</p> <pre><code>def goal_response_callback(self, future):\n    goal_handle = future.result()\n    if not goal_handle.accepted:\n        self.get_logger().warn(\"The goal was rejected by the server.\")\n        return\n\n    self.get_logger().info(\"The goal was accepted by the server.\")\n\n    self.get_result_future = goal_handle.get_result_async()\n    self.get_result_future.add_done_callback(self.get_result_callback)\n</code></pre> <p>The input to this method will be the future that is created by the <code>send_goal_async()</code> call. We assign this to an attribute called <code>goal_handle</code> here, and can then use this for two purposes:</p> <ol> <li>To check if the goal that we sent was accepted by the server</li> <li>If it was accepted, then we can get the result (using <code>get_result_async()</code>) and we can attach another callback to this to actually process that result: <code>get_result_callback</code>.</li> </ol> </li> <li> <p>Now we need to define this callback too. Define <code>get_result_callback</code> as another new method of the <code>CameraSweepActionClient()</code> class (i.e. underneath the <code>goal_response_callback()</code> class method that we have just defined)...</p> <pre><code>def get_result_callback(self, future):\n    result = future.result().result\n    self.get_logger().info(\n        f\"The action has completed.\\n\"\n        f\"Result (Image Paths):\\n  \"\n        + \"\\n  \".join(result.image_paths)\n    )\n    rclpy.shutdown()\n</code></pre> <p>The input to this class method is another future object which contains the actual result sent from the server. We assign this to <code>result</code> and use a <code>get_logger().info()</code> call to print this to the terminal when the action has finished.</p> <p>As we know from our work earlier, the <code>CameraSweep</code> Action interface contains one Result parameter called <code>image_paths</code>.</p> </li> <li> <p>Finally, in the <code>main</code> method, change this:</p> <p></p><pre><code>future = action_client.send_goal()\n</code></pre> to just this: <pre><code>action_client.send_goal()\n</code></pre> (because <code>send_goal()</code> no longer returns a <code>future</code>)<p></p> </li> <li> <p>And then also change:      </p><pre><code>rclpy.spin_until_future_complete(action_client, future)\n</code></pre><p></p> <p>to this:</p> <pre><code>rclpy.spin(action_client)\n</code></pre> </li> <li> <p>Save all your changes!</p> </li> <li> <p>Run this node again now (with the <code>ros2 run</code> command) and observe the changes in action.</p> <p>Our client node now presents us with the result that is sent by the server on completion of the action, but wouldn't it be nice if we could see the real-time feedback as the action takes place? Let's add this in now...</p> </li> </ol>"},{"location":"course/part5/#part-3-handling-feedback","title":"Part 3: Handling Feedback","text":"<ol> <li> <p>Go back to the <code>camera_sweep_action_client.py</code> file in VS Code.</p> </li> <li> <p>In order to be able to handle the feedback that is sent from an action server, we need to add yet another callback! Go back to the <code>send_goal()</code> method and the line where we are actually sending the goal to the server:</p> <pre><code>self.send_goal_future = self.actionclient.send_goal_async(goal)\n</code></pre> <p>As it stands, all we're doing here is sending the goal, but we can also add a feedback callback to this too:</p> <pre><code>self.send_goal_future = self.actionclient.send_goal_async(\n    goal=goal, \n    feedback_callback=self.feedback_callback\n)\n</code></pre> <p>The <code>feedback_callback</code> will be executed every time a new feedback message is received from the server.</p> </li> <li> <p>In order to define what we want to do with these feedback messages we need to add yet another new method to the <code>CameraSweepActionClient()</code> class. Underneath the <code>get_result_callback()</code> class method that we defined earlier, add this new one as well:</p> <pre><code>def feedback_callback(self, feedback_msg):\n    feedback = feedback_msg.feedback\n    fdbk_current_angle = feedback.current_angle\n    fdbk_current_image = feedback.current_image\n    self.get_logger().info(\n        f\"\\nFEEDBACK:\\n\"\n        f\"  - Current angular position = {fdbk_current_angle:.1f} degrees.\\n\"\n        f\"  - Image(s) captured so far = {fdbk_current_image}.\"\n    )\n</code></pre> <p>As we know from our work earlier, the <code>CameraSweep</code> interface contains two feedback parameters: <code>current_angle</code> and <code>current_image</code>.</p> </li> <li> <p>Save all your changes once again, run the node again with the <code>ros2 run</code> command and observe the changes in action.</p> <p>The node we've now built can send a goal to an action server, process the feedback returned from the server as the action is in progress, and present the result to us once everything is complete.</p> <p>As discussed earlier though, the other key feature of Actions is the ability to cancel them part-way through. So let's look at how to incorporate this now as well.</p> </li> </ol>"},{"location":"course/part5/#part-4-cancelling-an-action","title":"Part 4: Cancelling an Action","text":"<ol> <li> <p>First, create a copy of your <code>camera_sweep_action_client.py</code> node and call it <code>cancel_camera_sweep.py</code>. Make sure TERMINAL 3 is located in the <code>scripts</code> directory of your <code>part5_actions</code> package before running the following:</p> <pre><code>cp camera_sweep_action_client.py cancel_camera_sweep.py\n</code></pre> </li> <li> <p>Don't forget to declare this as an additional executable in the package's <code>CMakeLists.txt</code>. You'll then also need to re-build the package with <code>colcon build</code> (go back for a reminder).</p> </li> <li> <p>Open the <code>cancel_camera_sweep.py</code> file in VS Code.</p> </li> <li> <p>We want this client to be able to cancel the goal under two different circumstances:</p> <ol> <li>The client itself is shutdown by the user (via a Ctrl+C in the terminal)</li> <li>A conditional event that happens as the action is underway.</li> </ol> <p>In order to address item 1 first, we need to draw upon some of the work we did in Part 2 in the implementation of safe shutdown procedures...</p> </li> <li> <p>As you hopefully recall, we first need to import <code>SignalHandlerOptions</code> into our node, so add this as an additional import at the start of the code:</p> <pre><code>from rclpy.signals import SignalHandlerOptions\n</code></pre> <p>Then, in the <code>main()</code> node function, modify the <code>rclpy.init()</code> call:</p> <pre><code>rclpy.init(\n    args=args,\n    signal_handler_options=SignalHandlerOptions.NO\n)\n</code></pre> </li> <li> <p>Inside the <code>__init__()</code> method of our <code>CameraSweepActionClient()</code> class we now need to add some additional flags:</p> <pre><code>self.goal_succeeded = False\nself.goal_cancelled = False\nself.stop = False\n</code></pre> <p>In the <code>get_result_callback()</code> class method, we can then ensure that the <code>self.goal_succeeded</code> flag is set to <code>True</code> when a result is received. In this class method, locate the <code>rclpy.shutdown()</code> line and add the following additional line just above it:</p> <pre><code>self.goal_succeeded = True\n</code></pre> </li> <li> <p>Actions can be cancelled using a <code>cancel_goal_async()</code> method of the <code>goal_handle</code> that is obtained from the <code>goal_response_callback()</code>. As such, we need to make this accessible across our entire <code>CameraSweepActionClient()</code> class. Locate the <code>goal_response_callback()</code> class method, and add this line at the bottom as the last line of the <code>goal_response_callback()</code> method:</p> <pre><code>self.goal_handle = goal_handle\n</code></pre> <p>This makes <code>goal_handle</code> accessible across the entire <code>CameraSweepActionClient()</code> class as <code>self.goal_handle</code>. </p> </li> <li> <p>We can only attempt to cancel an Action when it's in progress, therefore the feedback callback is the best place to trigger this. Locate the <code>feedback_callback()</code> class method and place the following at the end of it:</p> <pre><code>if self.stop:\n    future = self.goal_handle.cancel_goal_async()\n    future.add_done_callback(self.cancel_goal)\n</code></pre> <p>Here, we call the <code>cancel_goal_async()</code> method from <code>self.goal_handle</code>, and add another new callback (<code>cancel_goal()</code>) to it (i.e. to encapsulate what we want to happen when the action is cancelled).</p> </li> <li> <p>Now, let's define this as another new (and final!) class method:</p> <pre><code>def cancel_goal(self, future):\n    cancel_response = future.result()\n    if len(cancel_response.goals_canceling) &gt; 0:\n        self.get_logger().info('Goal successfully canceled')\n        self.goal_cancelled = True\n    else:\n        self.get_logger().info('Goal failed to cancel')\n</code></pre> <p>The input to this callback is another future, which we can use to determine if the goal has been cancelled (as shown above). If it has, then we set our <code>self.goal_cancelled</code> flag to <code>True</code>.</p> </li> <li> <p>Finally, go back the <code>main()</code> function of the node. We're going to replace the <code>rclpy.spin(action_client)</code> line now, with a <code>rclpy.spin_once()</code>, wrapped inside a <code>try</code> - <code>except</code>, wrapped inside a <code>while</code> loop!</p> <pre><code>while not action_client.goal_succeeded:\n    try:\n        rclpy.spin_once(action_client)\n        if action_client.goal_cancelled:\n            break\n    except KeyboardInterrupt:\n        print(\"Ctrl+C\")\n        action_client.stop = True\n</code></pre> <p>The <code>while</code> loop will execute until the action completes successfully, or until the goal is cancelled, or we shutdown the node with a Ctrl+C interrupt.</p> <p>Look back through the node to see how all this will flow through your class.</p> </li> <li> <p>We may wish to cancel a goal conditionally if - say - too much time has elapsed since the call was made, or the caller has been made aware of something else that has happened in the meantime (perhaps we're running out of storage space on the robot and can't save any more images!). </p> <p>For the purposes of this exercise, we want to modify our node so that the action is always cancelled after a total of 5 images have been captured. This can be done by making a fairly small modification to the <code>feedback_callback()</code>. Have a go at implementing this now. </p> <ul> <li>Have a go now at introducing a conditional call to the <code>cancel_goal()</code> method once a total of 5 images have been captured.</li> <li>You could use the <code>current_image</code> attribute from the <code>CameraSweepFeedback</code> message to trigger this.</li> </ul> </li> </ol>"},{"location":"course/part5/#a-summary-of-ros-actions","title":"A Summary of ROS Actions","text":"<p>ROS Actions work a lot like ROS Services, but they have the following key differences:</p> <ol> <li>They can be cancelled: If something is taking too long, or if something else has happened, then an Action Client can cancel an Action whenever it needs to.</li> <li>They provide feedback: so that a client can monitor what is happening and act accordingly (i.e. cancel the action, if necessary).</li> </ol> <p>This mechanism is therefore useful for operations that may take a long time to execute, and where intervention might be necessary.</p>"},{"location":"course/part5/#creating-your-own-action-servers-clients-and-interfaces","title":"Creating Your Own Action Servers, Clients and Interfaces","text":"<p>Important</p> <p>Cancel all active processes that you may have running before moving on.</p> <p>So far we have looked at how to call a pre-existing action server, but what about if we actually want to set up our own, and use our own custom Action Interfaces too? </p> <p>To start with, have a look at the Action Server that you've been working with in the previous exercises. You've been launching this with the following command:</p> <pre><code>ros2 run tuos_examples camera_sweep_action_server.py\n</code></pre> <p>Questions</p> <ul> <li>Which package does the action server node belong to?</li> <li>Where (in that package directory) is this node likely to be located?</li> </ul> <p>Once you've identified the name and the location of the source code, open it up in VS Code and have a look through it to see how it all works. Don't worry too much about all the content associated with obtaining and manipulating camera images in there, we'll learn more about this in the next part of this course. Instead, focus on the general overall structure of the code and the way that the action server is implemented.</p> <p>Some things to review:</p> <ol> <li> <p>The way the server is initialised and the numerous callbacks that are attached to it:</p> <pre><code>self.actionserver = ActionServer(\n    node=self, \n    action_type=CameraSweep,\n    action_name=\"camera_sweep\",\n    execute_callback=self.server_execution_callback, # (1)!\n    callback_group=ReentrantCallbackGroup(), # (2)!\n    goal_callback=self.goal_callback, # (3)!\n    cancel_callback=self.cancel_callback # (4)!\n)\n</code></pre> <ol> <li>This callback contains all the code that will be executed by the server once a valid goal is sent to it (i.e. the core functionality of the Action)</li> <li>This server node is set up as a Multi-threaded Executor (see the setup in <code>main()</code>), to control the execution of the various callbacks that we need. Here, we're assigning the Action Server to a Reentrant Callback Group, allowing all its callbacks to run in parallel, as well as other subscriber callbacks too. </li> <li>This callback is used to check the goal parameters that have been sent to the server, to decide whether to accept or reject a request. </li> <li>This callback contains everything that needs to happen in the event that the Action is cancelled part-way through.</li> </ol> </li> <li> <p>Take a look at the various Action callbacks to see what's happening in each:</p> <ol> <li>How are goal parameters checked, and subsequently accepted or rejected?</li> <li>How are cancellations implemented, and how is this monitored in the main <code>server_execution_callback()</code>?</li> <li>How is feedback handled and published?</li> <li>How is the result handled and published too?</li> </ol> </li> <li> <p>Finally, consider the shutdown operations.</p> </li> </ol>"},{"location":"course/part5/#explore","title":"Implementing an \"Exploration\" strategy using the Action Framework","text":"<p>An exploration strategy allows a robot to autonomously navigate an unknown environment while simultaneously avoiding crashing into things! One way to achieve this is to utilise two distinct motion states: moving forwards and turning on the spot, and repeatedly switching between them. Brownian Motion and Levy Flight are examples of this kind of approach. Randomising the time spent in either, or both, of these two states will result in a navigation strategy that allows a robot to slowly and randomly explore an environment. Forward motion could be performed until - say - a certain distance has been travelled, a set time has elapsed or something gets in the way. Likewise, the direction, speed and/or duration of turning could also be randomised to achieve this.</p> <p>Over the next few exercises we'll construct our own Action Interface, Server and Client nodes with the aim of creating the basis for a basic exploration behaviour. Having completed these exercises you'll have something that - with further development - could be turned into an exploration type behaviour such as the above.</p>"},{"location":"course/part5/#ex3","title":"Exercise 3: Creating an Action Interface","text":"<p>In Exercise 2 we created the <code>part5_actions</code> package. Inside this package we will now define an Action Interface to be used by the subsequent Action Server and Client that we will create in the later exercises. </p> <ol> <li> <p>Action interfaces must be defined within an <code>action</code> folder at the root of the package directory, so let's create this now in TERMINAL 1:</p> <ol> <li> <p>First, navigate into your package:</p> <pre><code>cd ~/ros2_ws/src/part5_actions\n</code></pre> </li> <li> <p>Then use <code>mkdir</code> to make a new directory:</p> <pre><code>mkdir action\n</code></pre> </li> </ol> </li> <li> <p>In here we'll now define an Action Interface called <code>ExploreForward</code>:</p> <pre><code>touch action/ExploreForward.action\n</code></pre> </li> <li> <p>Open this up in VS Code and define the data structure of the Interface as follows:</p> ExploreForward.action<pre><code>#goal\nfloat32 fwd_velocity               # The speed at which the robot should move forwards (m/s)\nfloat32 stopping_distance          # Minimum distance of an approaching obstacle before the robot stops (meters) \n---\n#result\nfloat32 total_distance_travelled   # Total distance travelled during the action (meters)\nfloat32 closest_obstacle           # LaserScan distance to the closest detected obstacle up ahead (meters)\n---\n#feedback\nfloat32 current_distance_travelled # Distance travelled so far during the action (meters)\n</code></pre> <p>This interface therefore has two goal parameters, one feedback parameter, and two result parameters:</p> <p>Goal:</p> <ol> <li><code>fwd_velocity</code>: The speed (in m/s) at which the robot should move forwards when the action server is called. </li> <li><code>stopping_distance</code>: The distance (in meters) at which the robot should stop ahead of any objects or boundary walls that are in front of it (this data will come from <code>LaserScan</code> data from the robot's LiDAR sensor).</li> </ol> <p>Feedback:</p> <ol> <li><code>current_distance_travelled</code>: The distance travelled (in meters) since the current action was initiated (based on <code>Odometry</code> data).</li> </ol> <p>Result:</p> <ol> <li><code>total_distance_travelled</code>: The total distance travelled (in meters) over the course of the action (based on <code>Odometry</code> data).</li> <li><code>closest_obstacle</code>: The distance to the closest obstacle up ahead of the robot when the action completes.</li> </ol> </li> <li> <p>Now, declare this interface in the package's <code>CMakeLists.txt</code> file, so that the necessary Python code can be created:</p> CMakeLists.txt<pre><code>find_package(rosidl_default_generators REQUIRED)\nrosidl_generate_interfaces(${PROJECT_NAME}\n  \"action/ExploreForward.action\" \n)\n</code></pre> </li> <li> <p>Also modify the <code>package.xml</code> file (above the <code>&lt;export&gt;</code> line) to include the necessary <code>rosidl</code> dependencies:</p> package.xml<pre><code>&lt;buildtool_depend&gt;rosidl_default_generators&lt;/buildtool_depend&gt;\n&lt;exec_depend&gt;rosidl_default_runtime&lt;/exec_depend&gt;\n&lt;member_of_group&gt;rosidl_interface_packages&lt;/member_of_group&gt;\n</code></pre> </li> <li> <p>Now run <code>colcon</code> to generate the necessary source code for this new interface: </p> <ol> <li> <p>First, always make sure you're in the root of the Workspace:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Then run <code>colcon build</code>:</p> <pre><code>colcon build --packages-select part5_actions --symlink-install \n</code></pre> </li> <li> <p>And finally re-source the <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>We can now verify that it worked. </p> <ol> <li> <p>Use <code>ros2 interface</code> to list all available interface types, but use the <code>-a</code> option to display only action-type interfaces:</p> <pre><code>ros2 interface list -a\n</code></pre> <p>Look for a message with the prefix \"<code>part5_actions</code>\".</p> </li> <li> <p>Next, show the data structure of the interface:</p> <pre><code>ros2 interface show part5_actions/action/ExploreForward\n</code></pre> <p>This should match the content of the <code>part5_actions/action/ExploreForward.action</code> file that we created above.</p> </li> </ol> </li> </ol>"},{"location":"course/part5/#ex4","title":"Exercise 4: Building the \"ExploreForward\" Action Server","text":"<ol> <li> <p>In TERMINAL 1 navigate to the <code>scripts</code> folder of your <code>part5_actions</code> package, then:</p> <ol> <li>Create a Python script called <code>explore_server.py</code></li> <li>Make it executable</li> <li>Declare it as an executable in <code>CMakeLists.txt</code></li> </ol> </li> <li> <p>Open up the file in VS Code.</p> </li> <li> <p>The job of the Server node is as follows:</p> <ul> <li>The action server should make the robot move forwards until it detects an obstacle up ahead.</li> <li>It should use the <code>ExploreForward.action</code> Interface that we created in the previous exercise. </li> <li> <p>The Server will therefore need to be configured to accept two goal parameters:</p> <ol> <li>The speed (in m/s) at which the robot should move forwards when the action server is called. </li> <li> <p>The distance (in meters) at which the robot should stop ahead of any objects or boundary walls that are in front of it.</p> <p>To do this you'll need to subscribe to the <code>/scan</code> topic. Be aware that an object won't necessarily be directly in front of the robot, so you may need to monitor a range of <code>LaserScan</code> data points (within the <code>ranges</code> array) to make the collision avoidance effective (recall the LaserScan callback example from Part 3).</p> </li> </ol> </li> <li> <p>Be sure to do some error checking on the goal parameters to ensure that a valid request is made. This is done by attaching a <code>goal_callback</code> to the Action Server. </p> <ul> <li><code>fwd_velocity</code>: Should be a velocity that is moderate, and within the robot's velocity limits.</li> <li><code>stopping_distance</code>: Should be greater than the minimum detectable limit of the LiDAR Sensor, large enough to safely avoid collisions. </li> </ul> </li> <li> <p>Whilst your server performs its task it should provide the following feedback to a Client:</p> <ol> <li> <p>The distance travelled (in meters) since the current action was initiated.</p> <p>To do this you'll need to subscribe to the <code>/odom</code> topic. Remember the work that you did in Part 2 on this. </p> <p>Tips</p> <ul> <li>The robot's orientation shouldn't change over the course of a single action call, only its <code>linear.x</code> and <code>linear.y</code> positions should vary.</li> <li>Bear in mind however that the robot won't necessarily be moving along the <code>X</code> or <code>Y</code> axis, so you will need to consider the total distance travelled in the <code>X-Y</code> plane.</li> <li>We did this in the Part 2 <code>move_square</code> exercise, so refer to this if you need a reminder.</li> </ul> </li> </ol> </li> <li> <p>Finally, on completion of the action, your server should be configured to provide the following two result parameters:</p> <ol> <li>The total distance travelled (in meters) over the course of the action.</li> <li>The distance to the obstacle that made the robot stop (if the action server has done its job properly, then this should be very similar to the <code>stopping_distance</code> that was provided by the Action Client in the goal).</li> </ol> </li> </ul> </li> <li> <p>There's some template code to help you with this:</p> <p></p> The <code>explore_server.py</code> Template<p></p> <p>You might also want to take a look at the <code>camera_sweep_action_server.py</code> code from the earlier exercises to help you construct this: a lot of the techniques used here will be similar (excluding all the camera related stuff).  </p> </li> </ol> <p>Testing</p> <p>When you want to test things out, why not use the Nav World environment from Part 3 (in TERMINAL 1):</p> <pre><code>ros2 launch tuos_simulations nav_world.launch.py\n</code></pre> <p></p> <p>Don't forget that in order to launch the server you'll need to have built everything with <code>colcon</code> by following the usual three stage process.</p> <p>Do this in TERMINAL 2, and you'll then be able to run it:</p> <pre><code>ros2 run part5_actions explore_server.py\n</code></pre> <p>Also, don't forget that you don't need to have developed a Python Client Node in order to test the server. Use the <code>ros2 action send_goal</code> CLI tool to make calls to the server (like we did in Exercise 1).</p>"},{"location":"course/part5/#ex5","title":"Exercise 5: Building a Basic \"ExploreForward\" Client","text":"<ol> <li> <p>In TERMINAL 3 navigate to the <code>scripts</code> folder of your <code>part5_actions</code> package, create a Python script called <code>explore_client.py</code>, make it executable, and add this to your <code>CMakeLists.txt</code>.</p> </li> <li> <p>Run <code>colcon build</code> on it now, so you don't have to worry about it later (again, following the three stage process as above)...</p> <p>Here it is again, for good measure:</p> <ol> <li> <p>Step 1:</p> <pre><code>cd ~/ros2_ws\n</code></pre> </li> <li> <p>Step 2:</p> <pre><code>colcon build --packages-select part5_actions --symlink-install\n</code></pre> </li> <li> <p>Step 3:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Open up the <code>explore_client.py</code> file in VS Code.</p> </li> <li> <p>The job of the Action Client is as follows:</p> <ul> <li>The client needs to issue a correctly formatted goal to the server.</li> <li>The client should be programmed to monitor the feedback data from the Server.  If it detects (from the feedback) that the robot has travelled a distance greater than 2 meters without detecting an obstacle, then it should cancel the current action call.</li> </ul> </li> <li> <p>Use the techniques that we used in the Client node from Exercise 2 as a guide to help you with this. </p> </li> <li> <p>Once you have everything in place, launch the action client with <code>ros2 run</code> as below:</p> <pre><code>ros2 run part5_actions explore_client.py\n</code></pre> <p>If all is good, then this client node should call the action server, which will (in turn) make the robot move forwards until it gets a certain distance away from an obstacle up ahead (<code>stopping_distance</code>), at which point the robot will stop, and your client node should then stop too. Once this happens, reorient your robot (using the <code>teleop_keyboard</code> node) and launch the client node again to make sure that it is robustly stopping in front of obstacles repeatedly, and when approaching them from a range of different angles. </p> <p>Important</p> <p>Make sure that your cancellation functionality works correctly too, ensuring that:</p> <ol> <li>The robot never moves any further than 2 meters during a given action call</li> <li>An action is aborted mid-way through if the client node is shut down with Ctrl+C</li> </ol> </li> </ol>"},{"location":"course/part5/#ex6","title":"Exercise 6 (Advanced): Implementing an Exploration Strategy","text":"<p>Up to now, your Action Client node should have the capability to call the <code>ExploreForward.action</code> server to make the robot move forwards by 2 meters, or until it reaches an obstacle (whichever occurs first), but you could build on this now and turn it into a full exploration behaviour:</p> <ul> <li>Between action calls, your client node could make the robot turn on the spot to face a different direction and then issue a further action call to make the robot move forwards once again.</li> <li>The turning process could be done at random (ideally), or by a fixed amount every time.</li> <li>By programming your client node to repeat this process over and over again, the robot would (somewhat randomly) travel around its environment safely, stopping before it crashes into any obstacles and reorienting itself every time it stops moving forwards. </li> </ul>"},{"location":"course/part5/#wrapping-up","title":"Wrapping Up","text":"<p>In Part 5 of this course you've learnt:</p> <ul> <li>How ROS Actions work and why they might be useful.</li> <li>How to develop Action Client Nodes in Python which can monitor the action in real-time (via feedback), and which can also cancel the requested action, if required.</li> <li>How to use standard ROS tools to interrogate action interfaces, thus allowing us to build clients to call them</li> <li>How to build custom Action servers, clients and interfaces.</li> <li>How to harness this framework to implement an exploration strategy. </li> </ul>"},{"location":"course/part5/#topics-services-or-actions-which-to-choose","title":"Topics, Services or Actions: Which to Choose?","text":"<p>You should now have developed a good understanding of the three communication methods that are available within ROS to facilitate communication between ROS Nodes:</p> <ol> <li>Topic-based messaging.</li> <li>ROS Services.</li> <li>ROS Actions.</li> </ol> <p>Through this course you've gained some practical experience using all three of these, but you may still be wondering how to select the appropriate one for a certain robot task... </p> <p>This ROS.org webpage summarises all of this very nicely (and briefly), so you should have a read through this to make sure you know what's what. In summary though:</p> <ul> <li>Topics: Are most appropriate for broadcasting continuous data-streams such as sensor data and robot state information, and for publishing data that is likely to be required by a range of Nodes across a ROS network.</li> <li>Services: Are most appropriate for very short procedures like quick calculations (inverse kinematics etc.) and performing short discrete actions that are unlikely to go wrong or will not need intervention (e.g. turning on a warning LED when a battery is low).</li> <li>Actions: Are most appropriate for longer running tasks (like moving a robot), or for operations where we might need to change our mind and do something different or cancel an invoked behaviour part way through.</li> </ul>"},{"location":"course/part5/#backup","title":"WSL-ROS2 Managed Desktop Users: Save your work!","text":"<p>Remember, to save the work you have done in WSL-ROS2 during this session so that you can restore it on a different machine at a later date. Run the following script in any idle WSL-ROS2 Terminal Instance now:</p> <pre><code>wsl_ros backup\n</code></pre> <p>You'll then be able to restore it to a fresh WSL-ROS2 environment next time you fire one up (<code>wsl_ros restore</code>).  </p> <ol> <li> <p>Remember: you can also use the <code>wsl_ros restore</code> command at any time.\u00a0\u21a9</p> </li> <li> <p>You need to explicitly set the <code>goal_images</code> and <code>goal_angle</code> parameters at runtime, much like we did here with the <code>number_game_client.py</code> service client.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/part6/","title":"Part 6: Cameras, Machine Vision & OpenCV","text":""},{"location":"course/part6/#introduction","title":"Introduction","text":"<p> Exercises: 4 Estimated Completion Time: 2 hours Difficulty Level: Advanced </p>"},{"location":"course/part6/#aims","title":"Aims","text":"<p>In this part of the course we'll make use of the Waffle's camera, and look at how to work with images in ROS. Here we'll look at how to build ROS nodes that capture images and process them. We'll explore some ways in which this data can be used to inform decision-making in robotic applications.  </p>"},{"location":"course/part6/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Use a range of ROS tools to interrogate camera image topics on a ROS Network and view the images being streamed to them.</li> <li>Use the computer vision library OpenCV with ROS, to obtain camera images and process them in real-time.  </li> <li>Apply filtering processes to isolate objects of interest within an image.</li> <li>Develop object detection nodes and harness the information generated by these processes to control a robot's position.</li> <li>Use camera data as a feedback signal to implement a line following behaviour using proportional control.</li> </ol>"},{"location":"course/part6/#quick-links","title":"Quick Links","text":""},{"location":"course/part6/#exercises","title":"Exercises","text":"<ul> <li>Exercise 1: Using the <code>rqt_image_view</code> node whilst changing the robot's viewpoint</li> <li>Exercise 2: Object Detection</li> <li>Exercise 3: Using Image Moments for Robot Control</li> <li>Exercise 4: Line following</li> </ul>"},{"location":"course/part6/#additional-resources","title":"Additional Resources","text":"<ul> <li>The Initial Object Detection Code (for Exercise 2)</li> <li>A Complete Worked Example of the <code>object_detection.py</code> Node</li> <li>The <code>line_follower</code> Template (for Exercise 4)</li> </ul>"},{"location":"course/part6/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch your ROS Environment</p> <p>Launch your ROS environment now so that you have access to a Linux terminal instance (aka TERMINAL 1).</p> <p>Step 2: Restore your work (WSL-ROS2 Managed Desktop Users ONLY)</p> <p>Remember that any work that you do within the WSL-ROS2 Environment will not be preserved between sessions or across different University computers, and so you should be backing up your work to your <code>U:\\</code> drive regularly. When prompted (on first launch of WSL-ROS2 in TERMINAL 1) enter <code>Y</code> to restore this<sup>1</sup>.</p> <p>Step 3: Launch VS Code </p> <p>WSL users remember to check for this.</p> <p>Step 4: Make Sure The Course Repo is Up-To-Date</p> <p>Check that the Course Repo is up-to-date before you start on these exercises. See here for how to update. </p> <p>Step 5: Launch the Robot Simulation</p> <p>In this session we'll start by working with the same mystery world environment from Part 5. In TERMINAL 1, use the following command to load it:</p> <p></p><pre><code>ros2 launch tuos_simulations coloured_pillars.launch.py\n</code></pre> ...and then wait for the Gazebo window to open:<p></p> <p></p>"},{"location":"course/part6/#working-with-cameras-and-images-in-ros","title":"Working with Cameras and Images in ROS","text":""},{"location":"course/part6/#camera-topics-and-data","title":"Camera Topics and Data","text":"<p>There are a number of tools that we can use to view the live images that are being captured by a robot's camera in ROS. As with all robot data, these streams are published to topics, so we firstly need to identify those topics.</p> <p>In a new terminal instance (TERMINAL 2), run <code>ros2 topic list</code> to see the full list of topics that are currently active on our system. Conveniently, all the topics related to our robot's camera are prefixed with <code>/camera</code>! Filter the <code>ros2 topic list</code> output using <code>grep</code> (a Linux command), to only show topics with this prefix:</p> <pre><code>ros2 topic list | grep /camera\n</code></pre> <p>This should provide the following filtered list:</p> <pre><code>/camera/camera_info\n/camera/image_raw\n/camera/image_raw/compressed\n/camera/image_raw/compressedDepth\n/camera/image_raw/theora\n/camera/image_raw/zstd\n</code></pre> <p>The main item that we're interested in here is the raw image data, and the key topic that we'll therefore be using here is:</p> <pre><code>/camera/image_raw\n</code></pre> <p>Run <code>ros2 topic info</code> on this to identify the interface used by this topic.</p> <p>Then, run <code>ros2 interface show</code> on the interface to learn about the data format. You should end up with an output that looks like this (simplified slightly here):</p> <pre><code># This message contains an uncompressed image\n# (0, 0) is at top-left corner of image\n\nstd_msgs/Header header # Header timestamp should be acquisition time of image\n        builtin_interfaces/Time stamp\n                int32 sec\n                uint32 nanosec\n        string frame_id\n\nuint32 height                # image height, that is, number of rows\nuint32 width                 # image width, that is, number of columns\n\nstring encoding       # Encoding of pixels -- channel meaning, ordering, size\n                      # taken from the list of strings in include/sensor_msgs/image_encodings.hpp\n\nuint8 is_bigendian    # is this data bigendian?\nuint32 step           # Full row length in bytes\nuint8[] data          # actual matrix data, size is (step * rows)\n</code></pre> <p></p> <p>Questions</p> <ol> <li>What type of interface is used by this topic, and which package is this derived from?</li> <li>Using <code>ros2 topic echo</code> and the information about the topic message (as shown above) determine the size of the images that our robot's camera will capture (i.e. its dimensions, in pixels).  It will be quite important to know this when we start manipulating these camera images later on. </li> <li>Finally, considering the list above again, which part of the message do you think contains the actual image data?</li> </ol>"},{"location":"course/part6/#viz","title":"Visualising Camera Streams","text":"<p>We can view the images being streamed to the above camera topic (in real-time) in a variety of different ways, and we'll explore a couple of these now.</p> <p>One way is to use RViz, which can be launched using the following <code>ros2 launch</code> command in TERMINAL 2:</p> <pre><code>ros2 launch tuos_tb3_tools rviz.launch.py environment:=sim\n</code></pre> <p>Once RViz launches, you should see a camera panel in the bottom-left corner with a live stream of the images being obtained from the robot's camera.</p> <p></p> <p>Close down RViz by entering Ctrl+C in TERMINAL 2.  </p>"},{"location":"course/part6/#ex1","title":"Exercise 1: Using RQT whilst changing the robot's viewpoint","text":"<p>Another tool we can use to view camera data-streams is the RQT.</p> <ol> <li> <p>Enter the following command in TERMINAL 2 to launch it:</p> <p></p><pre><code>rqt\n</code></pre> <p></p> </li> <li> <p>From the top menu select <code>Plugins</code> &gt; <code>Vizualisation</code> &gt; <code>Image View</code>.</p> <p></p> <p></p> <p>This allows us to easily view images that are being published to any camera topic on the ROS network. Another useful feature is the ability to save these images (as <code>.jpg</code> files) to the filesystem: See the \"Save image\" button highlighted in the figure above. This might be useful later on...</p> </li> <li> <p>Click the drop-down box in the top left of the window to select an image topic to display.  Select <code>/camera/image_raw</code> (if it's not already selected).</p> </li> <li> <p>Keep this window open now, and launch a new terminal instance (TERMINAL 3).</p> </li> <li> <p>Launch the <code>teleop_keyboard</code> node. Rotate the robot on the spot, keeping an eye on the RQT Image View window as you do this. Stop the robot once one of the coloured pillars in the arena is roughly in the centre of the robot's field of vision, then close the <code>teleop_keyboard</code> node and RQT Image View by entering Ctrl+C in TERMINAL 3 and TERMINAL 2 respectively.</p> </li> </ol>"},{"location":"course/part6/#opencv","title":"OpenCV and ROS","text":"<p>OpenCV is a mature and powerful computer vision library designed for performing real-time image analysis, and it is therefore extremely useful for robotic applications.  The library is cross-platform and there is a Python API (<code>cv2</code>), which we'll be using to do some computer vision tasks of our own during this lab session. While we can work with OpenCV using Python straight away (via the API), the library can't directly interpret the native image format used by the ROS, so there is an interface that we need to use.  The interface is called CvBridge, which is a ROS package that handles the conversion between ROS and OpenCV image formats.  We'll therefore need to use these two libraries (OpenCV and CvBridge) hand-in-hand when developing ROS nodes to perform computer vision related tasks.</p>"},{"location":"course/part6/#object-detection","title":"Object Detection","text":"<p>One common job that we often want a robot to perform is object detection, and we will illustrate how this can be achieved using OpenCV tools for colour filtering, to detect the coloured pillar that your robot should now be looking at.  </p>"},{"location":"course/part6/#ex2","title":"Exercise 2: Object Detection","text":"<p>In this exercise you will learn how to use OpenCV to capture images, filter them and perform other analysis to confirm the presence and location of features that we might be interested in.</p>"},{"location":"course/part6/#step-1-getting-started","title":"Step 1: Getting Started","text":"<ol> <li> <p>First create a new package called <code>part6_vision</code> using the usual approach. </p> </li> <li> <p>Navigate into the <code>scripts</code> directory of this new package (using <code>cd</code>), create a new file called <code>object_detection.py</code> (using <code>touch</code>), make this executable (<code>chmod</code>) and declare this as an executable in the package's <code>CMakeLists.txt</code> (you've done this lots of times now, but check back through previous parts of the course if you need a reminder on how to do any of it).</p> </li> <li> <p>Open up the <code>object_detection.py</code> Python file in VS Code, and take a look at the following code:</p> <p></p> The <code>object_detection.py</code> code<p></p> <p>Read the annotations so that you understand how the node works and what should happen when you run it!</p> </li> <li> <p>Copy the code into the empty <code>object_detection.py</code> file and save it.</p> </li> <li> <p>Build the package with <code>colcon</code>:</p> <ol> <li> <p>In TERMINAL 2, make sure you're in the root of the Workspace:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Run <code>colcon build</code>:</p> <pre><code>colcon build --packages-select part6_vision --symlink-install \n</code></pre> </li> <li> <p>And finally re-source the <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Run the node using <code>ros2 run ...</code> </p> <p>An image should appear from the robot's camera and then vanish again after a few seconds.</p> </li> <li> <p>As you should know from reading the explainer, the node has just saved this image to a location on your filesystem. Navigate to this filesystem location and view the image using <code>eog</code>.</p> <p>What you may have noticed from the terminal output when you ran the <code>object_detection.py</code> node is that the robot's camera captures images at a native size of 1080x1920 pixels.  That's over 2 million pixels in total in a single image (2,073,600 pixels per image, to be exact), each pixel having a blue, green and red value associated with it - so that's a lot of data in a single image file! </p> <p>Question</p> <p>The size of the image file (in bytes) was actually printed to the terminal when you ran the <code>object_detection.py</code> node. Did you notice how big it was exactly?</p> </li> </ol> <p>Processing an image of this size is therefore hard work for a robot: any analysis we do will be slow and any raw images that we capture will occupy a considerable amount of storage space. The next step then is to reduce this down by cropping the image to a more manageable size.</p>"},{"location":"course/part6/#step-2-cropping","title":"Step 2: Cropping","text":"<p>We're going to modify the <code>object_detection.py</code> node now to:</p> <ul> <li>Capture a new image in its native size</li> <li>Crop it down to focus in on a particular area of interest</li> <li>Save both of the images (the cropped one should be much smaller than the original)  </li> </ul> <p></p> <ol> <li> <p>In your <code>object_detection.py</code> node locate the line:</p> <pre><code>self.show_image(img=cv_img, img_name=\"step1_original\")\n</code></pre> </li> <li> <p>Underneath this, add the following additional lines of code:</p> <pre><code>crop_width = width - 400\ncrop_height = 400\ncrop_y0 = int((width / 2) - (crop_width / 2))\ncrop_z0 = int((height / 2) - (crop_height / 2))\ncropped_img = cv_img[crop_z0:crop_z0+crop_height, crop_y0:crop_y0+crop_width]\n\nself.show_image(img=cropped_img, img_name=\"step2_cropping\")\n</code></pre> </li> <li> <p>Run the node again.  </p> <p>You'll be presented with two image windows now, but be patient! You'll have to wait for a few seconds for each window to open!</p> <p>Navigate back to the directory where the images have been saved, and use <code>eog</code> again to view them and take a closer look.</p> </li> </ol> <p>The code that we've just added here has created a new image object called <code>cropped_img</code> from a subset of the original (<code>cv_img</code>). This was achieved by specifying a desired <code>crop_height</code> and <code>crop_width</code> relative to the original image dimensions (in pixels). Additionally, we've also specified where in the original image (in terms of pixel coordinates) we want this subset to start, using <code>crop_y0</code> and <code>crop_z0</code>. This process is illustrated in the figure below:</p> <p></p> <p>The original image (<code>cv_img</code>) is cropped using a process called \"slicing\":</p> <p></p><pre><code>cropped_img = cv_img[\n    crop_z0:crop_z0+crop_height,\n    crop_y0:crop_y0+crop_width\n]\n</code></pre> This may seem quite confusing, but hopefully the figure below illustrates what's going on here:<p></p> <p></p>"},{"location":"course/part6/#step-3-masking","title":"Step 3: Masking","text":"<p>As discussed above, an image is essentially a series of pixels each with a blue, green and red value associated with it to represent the actual image colours. From the original image that we have just obtained and cropped, we now want to get rid of any colours other than those associated with the pillar that we want the robot to detect. We therefore need to apply a filter to the pixels, which we will ultimately use to discard any pixel data that isn't related to the coloured pillar, whilst retaining data that is.  </p> <p>This process is called masking and, to achieve this, we need to set some colour thresholds. This can be difficult to do in a standard Blue-Green-Red (BGR) or Red-Green-Blue (RGB) colour space, and you can see a good example of this in this article from RealPython.com.  We'll apply some steps discussed in this article to convert our cropped image into a Hue-Saturation-Value (HSV) colour space instead, which makes the process of colour masking a bit easier.</p> <ol> <li> <p>First, analyse the Hue and Saturation values of the cropped image. To do this, navigate to the <code>~/object_detection</code> directory, where the raw images are all being saved:</p> <pre><code>cd ~/object_detection\n</code></pre> <p>Then, run the following ROS Node (from the <code>tuos_examples</code> package), supplying the name of the cropped image as an additional argument:</p> <pre><code>ros2 run tuos_examples image_colours.py step2_cropping.jpg\n</code></pre> </li> <li> <p>The node should produce a scatter plot, illustrating the Hue and Saturation values of each of the pixels in the image. Each data point in the plot represents a single image pixel and each is coloured to match its RGB value:</p> <p></p> <p></p> </li> <li> <p>You should see from the image that all the pixels related to the coloured pillar that we want to detect are clustered together.  We can use this information to specify a range of Hue and Saturation values that can be used to mask our image: filtering out any colours that sit outside this range and thus allowing us to isolate the pillar itself. The pixels also have a Value (or \"Brightness\"), which isn't shown in this plot. As a rule of thumb, a range of brightness values between 100 and 255 generally works quite well.</p> <p></p> <p></p> <p>In this case then, we select upper and lower HSV thresholds as follows:</p> <pre><code>lower_threshold = (115, 225, 100)\nupper_threshold = (130, 255, 255)\n</code></pre> <p>Use the plot that you have generated yourself to determine your own upper and lower thresholds. </p> <p>OpenCV contains a built-in function to detect which pixels of an image fall within a specified HSV range: <code>cv2.inRange()</code>.  This outputs a matrix, the same size and shape as the number of pixels in the image, but containing only <code>True</code> (<code>1</code>) or <code>False</code> (<code>0</code>) values, illustrating which pixels do have a value within the specified range and which don't.  This is known as a Boolean Mask (essentially, a series of ones or zeroes).  We can then apply this mask to the image, using a Bitwise AND operation, to get rid of any image pixels whose mask value is <code>False</code> and keep any flagged as <code>True</code> (or in range).</p> </li> <li> <p>To do this, first locate the following line in your <code>object_detection.py</code> node:</p> <pre><code>self.show_image(img=cropped_img, img_name=\"step2_cropping\")\n</code></pre> </li> <li> <p>Underneath this, add the following:</p> <pre><code>hsv_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2HSV)\nlower_threshold = (115, 225, 100)\nupper_threshold = (130, 255, 255)\nimg_mask = cv2.inRange(hsv_img, lower_threshold, upper_threshold)\n\nself.show_image(img=img_mask, img_name=\"step3_image_mask\")\n</code></pre> </li> <li> <p>Now, run the node again. A third image should also be generated now. </p> <p>As shown in the figure below, the third image should simply be a black and white representation of the cropped image, where the white regions should indicate the areas of the image where pixel values fall within the HSV range specified earlier. </p> <p>Notice (from the text printed to the terminal) that the cropped image and the image mask have the same dimensions, but the image mask file has a significantly smaller file size.  While the mask contains the same number of pixels, these pixels only have a value of <code>1</code> or <code>0</code>, whereas - in the cropped image of the same pixel size - each pixel has a Red, Green and Blue value: each ranging between <code>0</code> and <code>255</code>, which represents significantly more data.</p> <p></p> <p></p> </li> </ol>"},{"location":"course/part6/#bitwise_and","title":"Step 4: Filtering","text":"<p>Finally, we can apply this mask to the cropped image, generating a final version of it where only pixels marked as <code>True</code> in the mask retain their RGB values, and the rest are simply removed.  As discussed earlier, we use a Bitwise AND operation to do this and, once again, OpenCV has a built-in function to do this: <code>cv2.bitwise_and()</code>.</p> <ol> <li> <p>Locate the following line in your <code>object_detection.py</code> node:</p> <pre><code>self.show_image(img=img_mask, img_name=\"step3_image_mask\")\n</code></pre> </li> <li> <p>And, underneath this, add the following:</p> <pre><code>filtered_img = cv2.bitwise_and(cropped_img, cropped_img, mask = img_mask)\n\nself.show_image(img=filtered_img, img_name=\"step4_filtered_image\")\n</code></pre> </li> <li> <p>Run this node again, and a fourth image should also be generated now, this time showing the cropped image taken from the robot's camera, but only containing data related to coloured pillar, with all other background image data removed (and rendered black):</p> <p></p> <p></p> </li> </ol>"},{"location":"course/part6/#image-moments","title":"Image Moments","text":"<p>You have now successfully isolated an object of interest within your robot's field of vision, but perhaps we want to make our robot move towards it, or - conversely - make our robot navigate around it and avoid crashing into it!  We therefore also need to know the position of the object in relation to the robot's viewpoint, and we can do this using image moments.</p> <p>The work we have just done above led to us obtaining what is referred to as a colour blob.  OpenCV also has built-in tools to allow us to calculate the centroid of a colour blob like this, allowing us to determine where exactly within an image the object of interest is located (in terms of pixels).  This is done using the principle of image moments: essentially statistical parameters related to an image, telling us how a collection of pixels (i.e. the blob of colour that we have just isolated) are distributed within it. You can read more about Image Moments here. Using this process, the central coordinates of a colour blob can be obtained by considering some key moments of the image mask that we obtained from thresholding earlier:</p> <ul> <li>\\(M_{00}\\): the sum of all non-zero pixels in the image mask (i.e. the size of the colour blob, in pixels)</li> <li>\\(M_{10}\\): the sum of all the non-zero pixels in the horizontal (y) axis, weighted by row number</li> <li>\\(M_{01}\\): the sum of all the non-zero pixels in the vertical (z) axis, weighted by column number</li> </ul> <p>Remember</p> <p>We refer to the horizontal as the y-axis and the vertical as the z-axis here, to match the terminology that we have used previously to define our robot's principal axes.</p> <p>We don't really need to worry about the derivation of these moments too much though.  OpenCV has a built-in <code>moments()</code> function that we can use to obtain this information from an image mask (such as the one that we generated earlier):</p> <pre><code>m = cv2.moments(img_mask)\n</code></pre> <p>So, using this we can obtain the <code>y</code> and <code>z</code> coordinates of the blob centroid quite simply:</p> <pre><code>cy = m['m10']/(m['m00']+1e-5)\ncz = m['m01']/(m['m00']+1e-5) \n</code></pre> <p>Question</p> <p>We're adding a very small number to the \\(M_{00}\\) moment here to make sure that the divisor in the above equations is never zero and thus ensuring that we never get caught out by any \"divide-by-zero\" errors. Why might this be necessary?</p> <p></p> <p>Once again, there is a built-in OpenCV tool that we can use to add a circle onto an image to illustrate the centroid location within the robot's viewpoint: <code>cv2.circle()</code>.  This is how we produced the red circle that you can see in the figure above.  You can see how this is implemented here:</p> <p></p> A complete worked example of the <code>object_detection.py</code> node <p></p> <p>In our case, we can't actually change the position of our robot in the z axis, so the <code>cz</code> centroid component here might not be that important to us for navigation purposes.  We may however want to use the centroid coordinate <code>cy</code> to understand where a feature is located horizontally in our robot's field of vision, and use this information to turn towards it (or away from it, depending on what we are trying to achieve).  We can then use this as the basis for some real closed-loop control.</p>"},{"location":"course/part6/#ex3","title":"Exercise 3: Using Image Moments for Robot Control","text":"<p>Inside the <code>tuos_examples</code> package there is a node that has been developed to illustrate how all the OpenCV tools that you have explored so far could be used to search an environment and stop a robot when it is looking directly at an object of interest. All the tools that are used in this node should be familiar to you by now, and in this exercise you're going to make a copy of this node and modify it to enhance its functionality.</p> <p>Make sure the \"Coloured Pillars\" world is still active and continue with the following steps now in TERMINAL 2.</p> <ol> <li> <p>The node is called <code>colour_search.py</code>, and it is located in the <code>scripts</code> folder of the <code>tuos_examples</code> package. Copy this into the <code>scripts</code> folder of your own <code>part6_vision</code> package by first ensuring that you are located in the desired destination folder:</p> <pre><code>cd ~/ros2_ws/src/part6_vision/scripts\n</code></pre> </li> <li> <p>Then, copy the <code>colour_search.py</code> node using <code>cp</code> as follows:</p> <pre><code>cp ~/ros2_ws/src/tuos_ros/tuos_examples/scripts/colour_search.py ./\n</code></pre> </li> <li> <p>Open up the <code>colour_search.py</code> file in VS Code to view the content.  Have a look through it and see if you can make sense of how it works.  The overall structure should be fairly familiar to you by now: we have a Python class structure, a Subscriber with a callback function, a timer with a callback containing all the robot control code, and a lot of the OpenCV tools that you have explored so far in this part of the course.  Essentially this node functions as follows:</p> <ol> <li>The robot turns on the spot whilst obtaining images from its camera (by subscribing to the <code>/camera/image_raw</code> topic).</li> <li>Camera images are obtained, cropped, then a threshold is applied to the cropped images to detect the blue pillar in the simulated environment.</li> <li>If the robot can't see a blue pillar then it turns on the spot quickly.</li> <li>Once detected, the centroid of the blue blob representing the pillar is calculated to obtain its current location in the robot's viewpoint.</li> <li>As soon as the blue pillar comes into view the robot starts to turn more slowly instead.</li> <li>The robot stops turning as soon as it determines that the pillar is situated directly in front of it (determined using the <code>cy</code> component of the blue blob centroid).</li> <li>The robot then waits for a while and then starts to turn again.</li> <li>The whole process repeats until it finds the blue pillar once again.</li> </ol> </li> <li> <p>Add the <code>colour_search.py</code> node to the list of Python executables in your <code>part6_vision/CMakeLists.txt</code> file.</p> </li> <li> <p>Add the following additional dependency to your <code>part6_vision/package.xml</code>:</p> <pre><code>&lt;exec_depend&gt;geometry_msgs&lt;/exec_depend&gt;\n</code></pre> </li> <li> <p>Re-build your package with <code>colcon</code>.</p> </li> <li> <p>Run the node as it is to see this in action.  Observe the log messages as they are printed to the terminal throughout execution.</p> </li> <li> <p>Your task is to then modify the node so that it stops in front of every coloured pillar in the arena (there are four in total, each of a different colour, as you know). For this, you may need to use some of the methods that you have explored in the previous exercises.</p> <ol> <li>You might first want to use some of the methods that we used to obtain and analyse some images from the robot's camera:<ol> <li>Use the <code>teleop_keyboard</code> node to manually move the robot, making it look at every coloured pillar in the arena individually.</li> <li>Run the <code>object_detection.py</code> node that you developed in the previous exercise to capture an image, crop it, save it to the filesystem and then feed this cropped image into the <code>image_colours.py</code> node from the <code>tuos_examples</code> package (as you did earlier)</li> <li>From the plot that is generated by the <code>image_colours.py</code> node, determine some appropriate HSV thresholds to apply for each coloured pillar in the arena.</li> </ol> </li> <li>Once you have the right thresholds, then you can add these to your <code>colour_search.py</code> node so that it has the ability to detect every pillar in the same way that it currently detects the blue one.</li> </ol> </li> </ol>"},{"location":"course/part6/#pid","title":"PID Control and Line Following","text":"<p>Line following is a handy skill for a robot to have! We can achieve this on our TurtleBot3 using its camera system and the image processing techniques that have been covered so far in this session.</p> <p>A well established algorithm for closed-loop control is known as PID Control, and this can be used to achieve such line following behaviour.</p> <p>At the heart of this is the principle of Negative-Feedback control, which considers a Reference Input, a Feedback Signal and the Error between these.</p> <p></p> <p> </p>      Negative-Feedback Control     Adapted from Arturo Urquizo, CC BY-SA 3.0, via Wikimedia Commons    <p>The Reference Input represents a desired state that we would like our system to maintain. If we want our TurtleBot3 to successfully follow a coloured line on the floor, we will need it to keep the colour blob that represents that coloured line in the centre of its view point at all times. The desired state would therefore be to maintain the <code>cy</code> centroid of the colour blob in the centre of its vision.</p> <p>A Feedback Signal informs us of what the current state of the system actually is. In our case, this feedback signal would be the real-time location of the coloured line in the live camera images, i.e. its <code>cy</code> centroid (obtained using processing methods such as those covered in Exercise 3 above). </p> <p>The difference between these two things is the Error, and the PID control algorithm provides us with a means to control this error and minimise it, so that our robot's actual state matches the desired state. i.e.: the coloured line is always in the centre of its viewpoint.</p> <p></p> <p></p> <p></p> <p>The PID algorithm is as follows:</p> \\[ u(t)=K_{P} e(t) + K_{I}\\int e(t)dt + K_{D}\\dfrac{d}{dt}e(t) \\] <p>Where \\(u(t)\\) is the Controlled Output, \\(e(t)\\) is the Error (as illustrated in the figure above) and \\(K_{P}\\), \\(K_{I}\\) and \\(K_{D}\\) are Proportional, Integral and Differential Gains respectively. These three gains are constants that must be established for any given system through a process called tuning. We will explore this tuning process in the practical exercise that follows.</p>"},{"location":"course/part6/#ex4","title":"Exercise 4: Line Following","text":""},{"location":"course/part6/#ex4a","title":"Part A: Setup","text":"<ol> <li> <p>Make sure that all ROS processes from the previous exercise are shut down now, including the <code>colour_search.py</code> node, and the Gazebo simulation in TERMINAL 1.</p> </li> <li> <p>In TERMINAL 1 launch a new simulation from the <code>tuos_simulations</code> package:</p> <pre><code>ros2 launch tuos_simulations line_following.launch.py tuning:=true\n</code></pre> <p>Your robot should be launched onto a long thin track with a straight pink line painted down the middle of the floor:</p> <p></p> <p></p> </li> <li> <p>In TERMINAL 2 you should still be located in your <code>part6_vision/scripts</code> directory, but if not then go there now:</p> <pre><code>cd ~/ros2_ws/src/part6_vision/scripts\n</code></pre> </li> <li> <p>Perform the necessary steps to create a new empty Python file called <code>line_follower.py</code> and prepare it for execution as a node within your package.</p> </li> <li> <p>Once that's done open up the empty file in VS Code, then have a look at the following code template:</p> <p></p> The <code>line_follower.md</code> code template<p></p> <p>The template contains three \"TODOs\" that you need to complete, all of which are explained in detail in the code annotations, so read these carefully. Ultimately, you did all of this in Exercise 2, so go back here if you need a reminder on how any of it works. </p> <p>Your aim here is to get the code to generate a cropped image, with the coloured line isolated and located within it, like this:</p> <p></p> <p></p> </li> </ol>"},{"location":"course/part6/#ex4b","title":"Part B: Implementing and Tuning a Proportional Controller","text":"<p>Referring back to the equation for the PID algorithm as discussed above, the Proportional, Integral and Differential components all have different effects on a system in terms of its ability to maintain the desired state (the reference input). The gain terms associated with each of these components (\\(K_{P}\\), \\(K_{I}\\) and \\(K_{D}\\)) must be tuned appropriately for any given system in order to achieve stability of control.</p> <p>A PID Controller can actually take three different forms:</p> <ol> <li>\"P\" Control: Only a Proportional gain (\\(K_{P}\\)) is used, all other gains are set to zero.</li> <li>\"PI\" Control: Proportional and Integral gains (\\(K_{P}\\) and \\(K_{I}\\)) are applied, the Differential gain is set to zero. </li> <li>\"PID\" Control: The controller makes use of all three gain terms (\\(K_{P}\\), \\(K_{I}\\) and \\(K_{D}\\))</li> </ol> <p>In order to allow our TurtleBot3 to follow a line, we actually only really need a \"P\" Controller, so our control equation becomes quite simple, reducing to:</p> \\[ u(t)=K_{P} e(t) \\] <p>The next task then is to adapt our <code>line_follower.py</code> node to implement this control algorithm and find a proportional gain that is appropriate for our system.</p> <ol> <li> <p>Return to your <code>line_follower.py</code> file. Underneath the line that reads:</p> <pre><code>cv2.waitKey(1)\n</code></pre> <p>Paste the following additional code:</p> <pre><code>kp = self.get_parameter('kp').get_parameter_value().double_value\nreference_input = ?\nfeedback_signal = cy\nerror = feedback_signal - reference_input \n\nang_vel = kp * error\nself.get_logger().info(\n    f\"\\nkp = {kp:.4f},\"\n    f\"\\nError = {error:.1f} pixels,\"\n    f\"\\nControl Signal = {ang_vel:.2f} rad/s.\"\n)\n</code></pre> <p></p> <p>Fill in the Blank!</p> <p>What is the Reference Input to the control system (<code>reference_input</code>)? Refer to this figure from earlier. </p> <p>Here we have implemented our \"P\" Controller. The Control Signal that is being calculated here is the angular velocity that will be applied to our robot (the code won't make the robot move just yet, but we'll get to that bit shortly!) The Controlled Output will therefore be the angular position (i.e. the yaw) of the robot.  </p> </li> <li> <p>Run the code as it is, and consider the following:</p> <ol> <li>What proportional gain (\\(K_{P}\\)) are we applying?</li> <li>What is the maximum angular velocity that can be applied to our robot? Is the angular velocity that has been calculated actually appropriate?</li> <li>Is the angular velocity that has been calculated positive or negative? Will this make the robot turn in the right direction and move towards the line?  </li> </ol> </li> <li> <p>Let's address the third question (c) first...</p> <p>A positive angular velocity should make the robot turn anti-clockwise (i.e. to the left), and a negative angular velocity should make the robot turn clockwise (to the right).</p> <p>To start with, the line should be to the left of the robot, which means a positive angular velocity is required to make the robot turn towards it. </p> <p>If the value of the Control Signal that is being calculated by our proportional controller (as printed to the terminal) is negative, then this isn't correct. </p> <p>The sign of our proportional gain (\\(K_{P}\\)) should be changed in order to correct this. \\(K_{P}\\) is defined in our code as a Parameter called <code>kp</code>, and if we don't explicitly define a value for this (which so far we have not done) then a default value will be used. We defined this default value in our Node's <code>__init__()</code>, when we declared the parameter to begin with:</p> <pre><code>self.declare_parameter(\"kp\", 0.01)\n</code></pre> <p>With your node still running, change the value now with a <code>ros2 param</code> call from TERMINAL 3:</p> <p></p><pre><code>ros2 param set /line_follower kp -0.01\n</code></pre> ... the value of the Control Signal (i.e. <code>ang_vel</code>) should now be greater than zero, which (when velocity commands are actually published) will make the robot turn left, i.e. towards the line, thereby attempting to minimise the position error.<p></p> </li> <li> <p>Stop the node with Ctrl+C. Change the code so that the <code>declare_parameter()</code> line now sets the <code>kp</code> parameter to be negative by default:</p> <pre><code>self.declare_parameter(\"kp\", -0.01)\n</code></pre> </li> <li> <p>Next, let's address the second of the above questions (b)...</p> <p>The maximum angular velocity that can be applied to our robot is \u00b11.82 rad/s. If our proportional controller is calculating a value for the Control Signal that is greater than 1.82, or less than -1.82 then this needs to be limited. In between the following two lines of code:</p> <pre><code>ang_vel = kp * error\nself.get_logger().info(...\n</code></pre> <p>Insert the following: </p><pre><code>if ang_vel &lt; -1.82:\n    ang_vel = -1.82\nelif ang_vel &gt; 1.82:\n    ang_vel = 1.82\n</code></pre><p></p> </li> <li> <p>Finally, we need to think about the actual proportional gain that is being applied. This is where we need to tune our system by finding a proportional gain (\\(K_{P}\\)) value that controls our system appropriately.</p> <p>Return to your <code>line_follower.py</code> file. Underneath the lines that read:</p> <pre><code>self.get_logger().info(\n    f\"\\nkp = {kp:.4f},\"\n    f\"\\nError = {error:.1f} pixels,\"\n    f\"\\nControl Signal = {ang_vel:.2f} rad/s.\"\n)\n</code></pre> <p>Paste the following:</p> <pre><code>self.vel_cmd.twist.linear.x = 0.1\nself.vel_cmd.twist.angular.z = ang_vel\nself.vel_pub.publish(self.vel_cmd)\n</code></pre> <p>Once running, the code should now make the robot move with a constant linear velocity of 0.1 m/s at all times, while its angular velocity will be determined by our proportional controller, based on the controller error and the proportional gain parameter <code>kp</code>.</p> <p>The figure below illustrates the effects different values of proportional gain can have on a system.</p> <p></p>      Courtesy of Prof. Roger Moore     Taken from COM2009 Lecture 6: PID Control    <p></p> <p>Run the code and see what happens. You should find that the robot behaves quite erratically, indicating that <code>kp</code> (currently at 0.01) is probably too large.</p> </li> <li> <p>Try reducing <code>kp</code> by a factor of 100. From TERMINAL 3: </p> <pre><code>ros2 param set /line_follower kp -0.0001\n</code></pre> <p>With a modified <code>kp</code> gain, you should find that the robot now gradually approaches the line, but it can take a while for it to do so.</p> <p>Tip</p> <p>The line that the robot is trying to follow is quite long, but if it gets to the end of it, or if it gets off track and ends up quite far away from the line, then you can always:</p> <ol> <li>Stop your <code>line_following.py</code> node with Ctrl+C.</li> <li> <p>Launch the <code>teleop_keyboard</code> node:</p> <pre><code>ros2 run turtlebot3_teleop teleop_keyboard\n</code></pre> </li> <li> <p>Drive the robot back to the line manually.</p> </li> <li>Stop the <code>teleop_keyboard</code> node with Ctrl+C then launch your <code>line_following.py</code> node again. </li> </ol> </li> <li> <p>Next, increase <code>kp</code> by a factor of 10: </p> <p>Once again (if you need to), use the <code>teleop_keyboard</code> node to get the robot back to the start line. Then, re-launch the <code>line_follower.py</code> node.</p> <p>Update the proportional gain once more with a <code>ros2 param</code> call: </p> <pre><code>ros2 param set /line_follower kp -0.001\n</code></pre> <p>With this new <code>kp</code> gain, the robot should now reach the line much quicker, and follow the line well once it reaches it.</p> </li> <li> <p>Could <code>kp</code> be modified any more to improve the control further? Play around a bit more and see what happens. We'll but this to the test on a more challenging track in the next part of this exercise.</p> </li> </ol>"},{"location":"course/part6/#ex4c","title":"Part C: Advanced Line Following","text":"<ol> <li> <p>Shut down the Line Following \"tuning\" environment if it's still running.</p> </li> <li> <p>Then, in TERMINAL 1 fire up a new one:</p> <pre><code>ros2 launch tuos_simulations line_following.launch.py\n</code></pre> <p>Your robot should be launched into an environment with a more interesting line to follow:</p> <p></p> <p></p> </li> <li> <p>In TERMINAL 2, run your <code>line_follower.py</code> node and see how it performs. Does your proportional gain need to be adjusted further to optimise the performance?</p> </li> <li> <p>Next, think about conditions where the line can't initially be seen...</p> <p>As you know, the angular velocity is determined by considering the <code>cy</code> component of a colour blob representing the line. What happens in situations where the blob of colour isn't there though?  What influence would this have on the Control Signals that are calculated by the proportional controller? To consider this further, try launching the robot in the same arena but with a different starting pose, and think about how you might approach this situation:</p> <pre><code>ros2 launch tuos_simulations line_following.launch.py \\\n    x_pose:=-3 y_pose:=-3 yaw:=-1.57\n</code></pre> </li> <li> <p>Finally, what happens when the robot reaches the finish line? How could you add additional functionality to ensure that the robot stops when it reaches this point? What features of the arena could you use to trigger this?</p> </li> </ol>"},{"location":"course/part6/#wrapping-up","title":"Wrapping Up","text":"<p>In this session you have learnt how to use data from a robot's camera to extract further information about its environment.  The camera allows our robot to \"see\" and the information that we obtain from this device can allow us to develop more advanced robotic behaviours such as searching for objects, follow things or - conversely - moving away or avoiding them.  You have learnt how to do some basic tasks with OpenCV, but this is a huge and very capable library of computer vision tools, and we encourage you to explore this further yourselves to enhance some of the basic principles that we have shown you today.</p>"},{"location":"course/part6/#backup","title":"WSL-ROS2 Managed Desktop Users: Save your work!","text":"<p>Remember, to save the work you have done in WSL-ROS2 during this session so that you can restore it on a different machine at a later date. Run the following script in any idle WSL-ROS2 Terminal Instance now:</p> <pre><code>wsl_ros backup\n</code></pre> <p>You'll then be able to restore it to a fresh WSL-ROS2 environment whenever you need it again (<code>wsl_ros restore</code>).  </p> <ol> <li> <p>Remember: you can also use the <code>wsl_ros restore</code> command at any time.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/extras/","title":"Additional Resources","text":""},{"location":"course/extras/#additional-resources","title":"Additional Resources","text":"<p>Additional resources to help you in your work with ROS 2 and the Waffles.</p>"},{"location":"course/extras/course-repo/","title":"The TUoS ROS Course Repo","text":"<p>A ROS Metapackage called <code>tuos_ros</code> has been put together to support these courses. This package is available on GitHub here. This repo contains the following ROS packages:</p> Package Name Description <code>tuos_examples</code> Some example scripts to support certain exercises in The ROS 2 Course <code>tuos_interfaces</code> Some custom ROS interfaces to support certain exercises in The ROS 2 Course <code>tuos_simulations</code> Some Gazebo Ignition simulations to support certain exercises in The ROS 2 Course <code>tuos_task_sims</code> Simulation resources to support your development work in real robot based course assignments <code>tuos_tb3_tools</code> Scripts and visualisation tools for the Waffles (real and sim)"},{"location":"course/extras/course-repo/#installing","title":"Installing","text":"<p>Early on in Part 1 of the ROS 2 Course we explain how to install the <code>tuos_ros</code> course repo, but here are those instructions again:</p> <ol> <li> <p>Navigate to your ROS Workspace <code>src</code> directory:</p> <pre><code>cd ~/ros2_ws/src/\n</code></pre> </li> <li> <p>Clone the repo from GitHub:</p> <pre><code>git clone https://github.com/tom-howard/tuos_ros.git -b jazzy\n</code></pre> </li> <li> <p>Navigate back one directory, into the root* of the ROS workspace:</p> <pre><code>cd ~/ros2_ws/\n</code></pre> </li> <li> <p>Run <code>colcon build</code> to compile the packages:</p> <pre><code>colcon build --packages-up-to tuos_ros\n</code></pre> </li> <li> <p>Then re-source your <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol>"},{"location":"course/extras/course-repo/#verify","title":"Verify","text":"<p>Once you've installed it, then you can verify that the build process has worked using the following command:</p> <pre><code>colcon_cd tuos_ros\n</code></pre> <p>Which should take you to a directory within the repo that's also called <code>tuos_ros</code>, i.e.:</p> <pre><code>~/ros2_ws/src/tuos_ros/tuos_ros\n</code></pre>"},{"location":"course/extras/course-repo/#updating","title":"Updating","text":"<p>The course repo may be updated every now and again, so its worth checking regularly that you have the most up-to-date version. You can do this by pulling down the latest updates from GitHub using <code>git pull</code>:</p> <pre><code>cd ~/ros2_ws/src/tuos_ros/ &amp;&amp; git pull\n</code></pre> <p>If you see the following message:</p> <pre><code>-bash: cd: tuos_ros/: No such file or directory\n</code></pre> <p>... then go back and make sure you've installed the repo first!</p> <p>Then, run <code>colcon build</code> and re-source your environment:</p> <pre><code>cd ~/ros2_ws &amp;&amp; colcon build --packages-up-to tuos_ros &amp;&amp; source ~/.bashrc\n</code></pre>"},{"location":"course/extras/launch-files/","title":"Launch Files (Advanced)","text":"<p>As we know from the work we've done in this course, ROS applications can be executed in two different ways:  </p> <ol> <li> <p>Using the <code>ros2 run</code> command:</p> <pre><code>ros2 run {Package name} {Node name}\n</code></pre> </li> <li> <p>Using the <code>ros2 launch</code> command:</p> <pre><code>ros2 launch {Package name} {Launch file}\n</code></pre> </li> </ol> <p>The <code>ros2 launch</code> command, used in combination with launch files, offers a few advantages over <code>ros2 run</code>, for example:</p> <ol> <li>Multiple nodes can be executed simultaneously.</li> <li>From within one launch file, we can call other launch files.</li> <li>We can pass in additional arguments to launch things conditionally, or to change the behaviour of our ROS applications dynamically.</li> </ol> <p>Point 1 above is explored in Part 3 of the ROS 2 Course (Exercise 1). In this section we'll explore Points 2 &amp; 3 further<sup>1</sup>.</p> <p>Note</p> <p>Make sure you check for updates to the course repo before moving on!</p>"},{"location":"course/extras/launch-files/#identifying-launch-arguments","title":"Identifying Launch Arguments","text":"<p>We can use the <code>-s</code> option with <code>ros2 launch</code> to discover the additional arguments that can be supplied to any given launch file. Take the <code>waffle.launch.py</code> launch file from <code>tuos_simulations</code>, for example:</p> <pre><code>ros2 launch tuos_simulations waffle.launch.py -s\n</code></pre> <p>You should be presented with a range of arguments here, starting with:</p> <pre><code>$ ros2 launch tuos_simulations waffle.launch.py -s\nArguments (pass arguments as '&lt;name&gt;:=&lt;value&gt;'):\nurdf_file_name : turtlebot3_waffle.urdf\n\n    'with_gui':\n        Select whether to launch Gazebo with or without Gazebo Client (i.e. the GUI).\n        (default: 'true')\n</code></pre> <p>Scroll to the bottom of the list, and you should see the following:</p> <pre><code>    'x_pose':\n        Starting X-position of the robot\n        (default: '0.0')\n\n    'y_pose':\n        Starting Y-position of the robot\n        (default: '0.0')\n\n    'yaw':\n        Starting orientation of the robot (radians)\n        (default: '0.0')\n</code></pre> <p>Using these arguments, we can control the position and orientation of the Waffle when it is spawned into the simulated world. Try this:</p> <pre><code>ros2 launch tuos_simulations waffle.launch.py x_pose:=1 y_pose:=0.5\n</code></pre> <p>The robot should spawn into an empty world, but at coordinate position \\(x=1.0\\), \\(y=0.5\\), rather than \\(x=0\\), \\(y=0\\), as would normally be the case.</p>"},{"location":"course/extras/launch-files/#launching-launch-files-from-launch-files","title":"Launching Launch Files from Launch Files!","text":"<p>As above, we learnt about how to create a basic launch file in Part 3 of the ROS Course. Using what we learnt here, we can develop launch files to execute as many nodes as we want on a ROS network simultaneously. Another thing we can do with launch files however is launch other launch files! </p> <p>Think back to Part 3 Exercise 2 now, where we explored ROS 2 Parameters. To explore this, we created a <code>param_circle.py</code> node (based on the <code>move_circle.py</code> node from Part 2) that would make the robot move in a circle at a radius dictated by the <code>radius</code> ROS parameter.</p> <p>Ultimately, in order for this node to do anything, we must first have a robot simulation up and running, e.g.: </p> <pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre> <p>We could actually wrap the execution of the simulation and our <code>param_circle.py</code> node inside a single launch file, so that it can all be launched together using a single <code>ros2 launch</code> command...</p> <p>Note</p> <p>This exercise is here for illustration purposes.</p> <p>DON'T include launch descriptions for simulations in any work that you submit for course assignments! </p> <ol> <li> <p>In a ROS 2 terminal, return to the <code>launch</code> directory of the <code>part3_beyond_basics</code> package. First, navigate into the package root:</p> <pre><code>cd ~/ros2_ws/src/part3_beyond_basics\n</code></pre> <p>... and then into the <code>launch</code> directory from there:</p> <pre><code>cd launch/\n</code></pre> </li> <li> <p>Make a new launch file in here, called <code>circle.launch.py</code>:</p> <pre><code>touch circle.launch.py\n</code></pre> </li> <li> <p>Open this up in VS Code and enter the following:</p> circle.launch.py<pre><code>from launch import LaunchDescription\nfrom launch_ros.actions import Node\n\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import PathJoinSubstitution\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    return LaunchDescription([\n        IncludeLaunchDescription( # (1)!\n            PythonLaunchDescriptionSource( # (2)!\n                PathJoinSubstitution([ # (3)!\n                    FindPackageShare(\"turtlebot3_gazebo\"), # (4)!\n                    \"launch\", \n                    \"empty_world.launch.py\" \n                ])\n            )\n        )\n    ])\n</code></pre> <ol> <li>To include another launch file in a launch description, we use a <code>IncludeLaunchDescription()</code> class instance (imported from a module called <code>launch.actions</code>).</li> <li> <p>We want to launch the \"Empty World\" simulation from the <code>turtlebot3_gazebo</code> package, which (as we know) can be done from a terminal with the following command:</p> <pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre> <p>Based on the above, we know that the launch file itself is a Python launch file, due to the <code>.py</code> file extension at the end.</p> <p>As such, the launch description that we want to include is a Python launch description, which must therefore be defined using a <code>PythonLaunchDescriptionSource()</code> instance (imported from a module called <code>launch.launch_description_sources</code>)</p> </li> <li> <p>The <code>PathJoinSubstitution()</code> class (from the <code>launch.substitutions</code> library) can be used to build file paths from a list of individual components (other file paths, folder names and launch file names).</p> <p>Here we're using this to construct a full file path to the launch file that we want to execute.</p> <p>... but how do we know what that is? See below for more information...</p> </li> <li> <p>We need to know the full path to the launch file that we want to execute. We don't always know where this file is on our filesystem (launch files that are outside the ROS 2 workspace, that we haven't created ourselves for example).</p> <p>We can use another class called <code>FindPackageShare()</code> (from yet another library called <code>launch_ros.substitutions</code>). This provides us with the path to the root of this package directory.</p> <p>Installed ROS packages (including the ones that we create ourselves) are always located in - and executed from - a \"share\" directory, hence <code>FindPackageShare()</code>. There will be multiple share directories on our system:</p> <ul> <li><code>/opt/ros/jazzy/share/</code></li> <li> <p><code>~/ros2_ws/install/part3_beyond_basics/share/</code></p> <p>... for example.</p> </li> </ul> </li> </ol> <p>Currently, this launch file only contains the description of the <code>empty_world.launch.py</code> launch file from the <code>turtlebot3_gazebo</code> package. There's a few new things that have been introduced here to achieve this, so click on the  icons in the code above to find out what all these things are doing.</p> </li> <li> <p>Now, add a <code>Node()</code> item to the launch description so that the <code>param_circle.py</code> node (from your <code>part3_beyond_basics</code> package) is launched after the \"Empty World\" simulation has been launched.</p> <p>Refer back to Part 3 Exercise 1 for a reminder of how to do this.</p> </li> <li> <p>Run <code>colcon build</code> on your package to build this new launch file.</p> </li> <li> <p>Finally, when you're ready, execute your new <code>circle.launch.py</code> launch file:</p> <pre><code>ros2 launch part3_beyond_basics circle.launch.py\n</code></pre> </li> </ol> <p>If you've done this successfully, on launching the above command the Gazebo Empty World simulation should launch and, once it's loaded up, the robot should instantly start moving in a circle.</p>"},{"location":"course/extras/launch-files/#passing-launch-arguments","title":"Passing Launch Arguments","text":"<p>How do we pass an argument to a launch file (<code>tuos_simulations/waffle.launch.py</code>, for example) that is declared within another launch file? </p> <p>Taking the same approach as above, a basic launch description for the <code>tuos_simulations/waffle.launch.py</code> sim would look like this:</p> launch_args_example.launch.py<pre><code>from launch import LaunchDescription\nfrom launch_ros.actions import Node\n\nfrom launch.actions import IncludeLaunchDescription\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import PathJoinSubstitution\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    return LaunchDescription([\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource(\n                PathJoinSubstitution([\n                    FindPackageShare(\"tuos_simulations\"),\n                    \"launch\", \n                    \"waffle.launch.py\" \n                ])\n            )\n        )\n    ])\n</code></pre> <p>To launch this and supply the <code>x_pose</code> and <code>y_pose</code> launch arguments to it as well, we need to add the following to the <code>PythonLaunchDescriptionSource()</code> definition:</p> <pre><code>IncludeLaunchDescription(\n    PythonLaunchDescriptionSource(\n        PathJoinSubstitution([\n            FindPackageShare(\"tuos_simulations\"), \n            \"launch\", \n            \"waffle.launch.py\"\n        ])\n    ),\n    launch_arguments={ # (1)!\n        'x_pose': '1.0',\n        'y_pose': '0.5' # (2)!\n    }.items()\n)\n</code></pre> <ol> <li>Arguments are passed to the launch file via the <code>launch_arguments</code> option of <code>IncludeLaunchDescription()</code>.</li> <li> <p>Arguments are passed as a dictionary, which can contain multiple key value pairs separated by commas: <code>dict = {key1:value1, key2:value2, ... }</code>. </p> <p>In this case, keys are the names of the launch arguments to be passed to the <code>waffle.launch.py</code> launch file, and values are the actual values we want to assign to those arguments (and which can be changed as required).</p> </li> </ol>"},{"location":"course/extras/launch-files/#command-line-arguments-for-launch-files","title":"Command Line Arguments for Launch Files","text":"<p>To create arguments for our own launch files and to be able to pass these arguments into our own Nodes, we need to use parameters. Once again, we learnt about these in Part 3 of the course (in Exercise 2), where we created the <code>param_circle.py</code> node that has already been discussed above.</p> <p>If you have a simulation still running, close this down now. For the remainder of this section, you should launch an empty world manually, from a separate terminal, whenever you need it (either the <code>tuos_simulations/waffle.launch.py</code> or <code>turtlebot3_gazebo/empty_world.launch.py</code> worlds would be appropriate).</p>"},{"location":"course/extras/launch-files/#declaring-command-line-arguments-for-launch-files","title":"Declaring Command Line Arguments for Launch Files","text":"<p>Let's now create a very basic launch file called <code>cli_example.launch.py</code> now, to launch this node alone (once again, why not create this in your <code>part3_beyond_basics</code> package where you'll be building up quite a collection of launch files by now!):</p> cli_example.launch.py<pre><code>from launch import LaunchDescription \nfrom launch_ros.actions import Node \n\ndef generate_launch_description(): \n    return LaunchDescription([ \n        Node( \n            package='part3_beyond_basics', \n            executable='param_circle.py', \n            name='my_param_circle_node' \n        )\n    ])\n</code></pre> <p>As we know, this node uses a ROS 2 parameter called <code>radius</code> to control the size of the circle that the robot will follow. We can declare a value for this at run time from within this launch file and set its value via a command line argument passed to the launch file itself. </p> <p>To do this, we first use the <code>DeclareLaunchArgument</code> action, which must be included as an item in the <code>LaunchDescription</code>:</p> cli_example.launch.py<pre><code>from launch import LaunchDescription \nfrom launch_ros.actions import Node \nfrom launch.actions import DeclareLaunchArgument # (1)!\n\ndef generate_launch_description(): \n    return LaunchDescription([ \n        DeclareLaunchArgument(\n            name='circle_radius', \n            description=\"Sets the desired radius of the circle (in meters).\",\n            default_value='1.0'\n        ), # (2)!\n        Node( \n            package='part3_beyond_basics', \n            executable='param_circle.py', \n            name='my_param_circle_node' \n        )\n    ])\n</code></pre> <ol> <li>We need to import <code>DeclareLaunchArgument</code> so that we can use it in the launch file.</li> <li>Don't forget the comma to separate the two launch description items: <code>DeclareLaunchArgument()</code> and <code>Node()</code>! </li> </ol> <p>We're defining three things when declaring the launch argument:</p> <ol> <li><code>name</code>: The name of the argument.</li> <li><code>description</code>: A description of what this argument is used for.</li> <li><code>default_value</code>: A value that will be assigned if we don't provide one when executing the launch file.</li> </ol>"},{"location":"course/extras/launch-files/#passing-launch-file-arguments-to-python-nodes-via-parameters","title":"Passing Launch File Arguments to Python Nodes (via Parameters)","text":"<p>We defined a launch argument in the step above, but (currently) this argument isn't actually being passed to our <code>param_circle.py</code> node. To do this, we can add an argument to the <code>Node()</code> launch description item:</p> cli_example.launch.py<pre><code>from launch import LaunchDescription \nfrom launch_ros.actions import Node \nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration # (1)!\n\ndef generate_launch_description(): \n    return LaunchDescription([ \n        DeclareLaunchArgument(\n            name='circle_radius', \n            description=\"Sets the desired radius of the circle (in meters).\",\n            default_value='1.0'\n        ),\n        Node( \n            package='part3_beyond_basics', \n            executable='param_circle.py', \n            name='my_param_circle_node',\n            parameters=[{'radius': LaunchConfiguration('circle_radius')}] \n        )\n    ])\n</code></pre> <ol> <li>Another new import here!!</li> </ol> <p>Remember that our <code>param_circle.py</code> node uses a parameter called <code>radius</code>, and we are passing this into this launch file using the value supplied by the launch file argument <code>circle_radius</code>. So, if we call this launch file without supplying the <code>circle_radius</code> argument, a default value of <code>1.0</code> will be set (instead of the default value of <code>0.5</code> set by the node itself). Test this out by running this launch file without passing a value for the <code>circle_radius</code> argument first:</p> <pre><code>ros2 launch part3_beyond_basics cli_example.launch.py\n</code></pre> <p>You should see regular messages printed to the terminal to indicate the radius that the node is attempting to achieve:</p> <pre><code>...\n[param_circle.py-1] [INFO] [###] [my_param_circle_node]: Moving with radius: 1.00 [m]\n...\n</code></pre> <p>Now, do this again but this time specifying a value for <code>circle_radius</code>:</p> <pre><code>ros2 launch part3_beyond_basics cli_example.launch.py circle_radius:=0.3\n</code></pre> <p>This time, the regular status messages (and the movement of the robot) should have changed:</p> <pre><code>...\n[param_circle.py-1] [INFO] [###] [my_param_circle_node]: Moving with radius: 0.30 [m]\n...\n</code></pre>"},{"location":"course/extras/launch-files/#summary","title":"Summary","text":"<p>In the above two sections we've seen how we can build a launch file that accepts a command line argument, and how we can pass the value of that command line argument into a ROS node.</p> <ol> <li> <p>For more advanced launch file features, have a look at this guide.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/part1/custom_msg_pub/","title":"The `Example` Message Publisher","text":""},{"location":"course/part1/custom_msg_pub/#the-example-message-publisher","title":"The <code>Example</code> Message Publisher","text":""},{"location":"course/part1/custom_msg_pub/#the-code","title":"The Code","text":"<p>Copy all the code below into your <code>custom_msg_publisher.py</code> file and review the annotations to see what's different to the basic publisher from Exercise 5.</p> custom_msg_publisher.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\n\nfrom part1_pubsub.msg import Example # (1)!\n\nclass SimplePublisher(Node):\n\n    def __init__(self):\n        super().__init__(\"simple_publisher\")\n\n        self.my_publisher = self.create_publisher(\n            msg_type=Example, # (2)!\n            topic=\"my_topic\",\n            qos_profile=10,\n        )\n\n        publish_rate = 1 # Hz\n        self.timer = self.create_timer(\n            timer_period_sec=1/publish_rate,\n            callback=self.timer_callback\n        )\n\n        self.get_logger().info(\n            f\"The '{self.get_name()}' node is initialised.\"\n        )\n\n    def timer_callback(self):\n        ros_time = self.get_clock().now().seconds_nanoseconds()\n\n        topic_msg = Example() # (3)!\n        topic_msg.info = \"The ROS time is...\"\n        topic_msg.time = ros_time[0]\n        self.my_publisher.publish(topic_msg)\n        self.get_logger().info(\n            f\"Publishing: '{topic_msg.info} {topic_msg.time:.0f}'\"\n        )\n\ndef main(args=None):\n    rclpy.init(args=args)\n    my_simple_publisher = SimplePublisher()\n    rclpy.spin(my_simple_publisher)\n    my_simple_publisher.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>We're now importing the <code>Example</code> message from our own <code>part1_pubsub</code> package.</p> </li> <li> <p>We're also now declaring that <code>\"my_topic\"</code> will use the <code>Example</code> message data structure to send messages.</p> </li> <li> <p>We need to deal with the topic messages differently now, to account for the more complex structure.</p> <p>We now populate our messages with two fields: <code>info</code> (a <code>string</code>) and <code>time</code> (an <code>int</code>). Identify what has changed here...</p> </li> </ol>"},{"location":"course/part1/publisher/","title":"A Simple Publisher Node","text":""},{"location":"course/part1/publisher/#the-code","title":"The Code","text":"<p>Copy all the code below into your <code>publisher.py</code> file and review the annotations to understand how it all works.</p> <p></p> <p>Don't forget the Shebang!</p> <p>The very first line of code looks like a comment, but it is actually a very crucial part of the script:</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>This is called the Shebang, and it tells the operating system which interpreter to use to execute the code. In our case here, it tells the operating system where to find the right Python interpreter that should be used to actually execute the code.</p> publisher.py<pre><code>#!/usr/bin/env python3\n# A simple ROS2 Publisher\n\nimport rclpy # (1)!\nfrom rclpy.node import Node\n\nfrom example_interfaces.msg import String # (2)!\n\nclass SimplePublisher(Node): # (3)!\n\n    def __init__(self):\n        super().__init__(\"simple_publisher\") # (4)!\n\n        self.my_publisher = self.create_publisher(\n            msg_type=String,\n            topic=\"my_topic\",\n            qos_profile=10,\n        ) # (5)!\n\n        publish_rate = 1 # Hz\n        self.timer = self.create_timer(\n            timer_period_sec=1/publish_rate, \n            callback=self.timer_callback\n        ) # (6)!\n\n        self.get_logger().info(\n            f\"The '{self.get_name()}' node is initialised.\" # (7)!\n        )\n\n    def timer_callback(self): # (8)!\n        ros_time = self.get_clock().now().seconds_nanoseconds()\n\n        topic_msg = String()\n        topic_msg.data = f\"The ROS time is {ros_time[0]} (seconds).\"\n        self.my_publisher.publish(topic_msg)\n        self.get_logger().info(f\"Publishing: '{topic_msg.data}'\")\n\ndef main(args=None): # (9)!\n    rclpy.init(args=args)\n    my_simple_publisher = SimplePublisher()\n    rclpy.spin(my_simple_publisher)\n    my_simple_publisher.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__': # (10)!\n    main()\n</code></pre> <ol> <li> <p><code>rclpy</code> is the ROS Client Library for Python. </p> <p>This is a vital import that allows us to create ROS nodes and initialise them on the ROS network.</p> <p>We also import the <code>Node</code> class from the <code>rclpy.node</code> library. This is a ready-made Python Class that contains all the necessary functionality that a Python ROS Node might need, so we'll use this as the basis for our own node (which we'll create shortly).</p> </li> <li> <p>We also need to import the <code>String</code> message type from the <code>example_interfaces.msg</code> library for publishing our messages.</p> </li> <li> <p>We create a Python class called <code>SimplePublisher()</code>, which we'll use to encapsulate all the functionality of our node.</p> <p>The vast majority of the functionality of this node is inherited from the <code>rclpy.node</code>, <code>Node()</code> Class which we imported above. </p> </li> <li> <p>Using the <code>super()</code> method we call the <code>__init__()</code> method from the parent Node class that our <code>SimplePublisher</code> class is derived from.</p> <p>We provide a name here, which is the name that will be used to register our node on the ROS network (we can call the node anything that we want, but it's a good idea to call it something meaningful).</p> </li> <li> <p>We then use the <code>create_publisher()</code> method (inherited from the <code>Node</code> class) in order to provide our node with the ability to publish messages to a ROS Topic. When calling this we provide 3 key bits of information:</p> <ol> <li> <p><code>msg_type</code>: The type of message that we want to publish.</p> <p>In our case, a <code>String</code> message from the <code>example_interfaces.msg</code> module.</p> </li> <li> <p><code>topic</code>: The name of the topic that we want to publish these messages to.</p> <p>This could be an existing topic (in which case, we'd need to make sure we used the correct message type), or a new topic (in which case, the name can be anything we want it to be).</p> <p>In our case, we want to create a new topic on the ROS network called <code>\"my_topic\"</code>.</p> </li> <li> <p><code>qos_profile</code>: A queue size, which is a \"Quality of Service\" (QoS) setting which limits the amount of messages that are queued in a buffer. </p> <p>In our case, we're setting this to <code>10</code>, which is generally appropriate for most of the applications that we'll be working on.</p> </li> </ol> </li> <li> <p>Here, we're calling the <code>create_timer()</code> method, which we'll use to control the rate at which messages are published to our topic. Here we define 2 things:</p> <ol> <li> <p><code>timer_period_sec</code>: The rate at which we want the timer to run. This must be provided as a period, in seconds. In the line above, we have specified a publishing frequency (in Hz):</p> <p></p><code>publish_rate = 1 # Hz</code><p></p> <p>So the associated time period (in seconds) is: </p> <p></p>\\(T = \\frac{1}{f}\\)<p></p> </li> <li> <p><code>callback</code>: This is a function that will be executed every time the timer elapses at the desired rate (1 Hz). We're specifying a function called <code>timer_callback</code>, which we'll define later on in the code...</p> </li> </ol> </li> <li> <p>Finally, we use the <code>get_logger().info()</code> method to send a Log message to the terminal to inform us that the initialisation of our node is complete.</p> </li> <li> <p>Here we define the timer callback function. Anything in here will execute at the rate that we specified when we created the <code>create_timer()</code> instance before. In our case:</p> <ol> <li>Use the <code>get_clock()</code> method to get the current ROS Time.</li> <li>Instantiate a <code>String()</code> message (defined as <code>topic_msg</code>).</li> <li>Populate this message with data. In our case, a statement that includes the ROS Time, as obtained above.</li> <li>Call the <code>publish()</code> method of our <code>my_publisher</code> object, to actually publish this message to the <code>\"my_topic\"</code> topic.</li> <li>Send the message data to the terminal as a log message as well, so that we can see what it is when our Node is actually running.</li> </ol> </li> <li> <p>With the functionality of our <code>SimplePublisher</code> class now established, we define a <code>main()</code> function for the Node. This will be fairly common to most Python Nodes that we create, with the following 5 key processes:</p> <ol> <li>Initialise the <code>rclpy</code> library.</li> <li>Create an instance of our <code>SimplePublisher()</code> node.</li> <li>\"Spin\" the node to keep it alive so that any callbacks can execute as required (in our case here, just the <code>timer_callback()</code>). </li> <li>Destroy the node once termination is requested (triggered by entering Ctrl+C in the terminal).</li> <li>Shutdown the <code>rclpy</code> library.</li> </ol> </li> <li> <p>Finally, we call the <code>main()</code> function to set everything going. We do this inside an <code>if</code> statement, to ensure that our node is the main executable (i.e. it has been executed directly (via <code>ros2 run</code>), and hasn't been called by another script)</p> </li> </ol>"},{"location":"course/part1/publisher/#defining-package-dependencies","title":"Defining Package Dependencies","text":"<p>We're importing a couple of Python libraries into our node here, which means that our package has two dependencies: <code>rclpy</code> and <code>example_interfaces</code>:</p> <pre><code>import rclpy \nfrom rclpy.node import Node\n\nfrom example_interfaces.msg import String\n</code></pre> <p>Its good practice to add these dependencies to your <code>package.xml</code> file. Locate this file (<code>ros2_ws/src/part1_pubsub/package.xml</code>), open it up and find the following line:</p> <pre><code>&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;\n</code></pre> <p><code>rclpy</code> is therefore already defined as an execution dependency (which means that our package needs this library in order to execute our code), but we need to add <code>example_interfaces</code> as well, so add the following additional line underneath:</p> <pre><code>&lt;exec_depend&gt;example_interfaces&lt;/exec_depend&gt;\n</code></pre> <p>Job done. Save the file and close it.</p>"},{"location":"course/part1/subscriber/","title":"A Simple Subscriber Node","text":""},{"location":"course/part1/subscriber/#the-code","title":"The Code","text":"<p>Copy all the code below into your <code>subscriber.py</code> file and (again) make sure you read the annotations to understand how it all works!</p> subscriber.py<pre><code>#!/usr/bin/env python3\n# A simple ROS2 Subscriber\n\nimport rclpy # (1)!\nfrom rclpy.node import Node\n\nfrom example_interfaces.msg import String\n\nclass SimpleSubscriber(Node): # (2)! \n\n    def __init__(self): \n        super().__init__(\"simple_subscriber\") # (3)!\n\n        self.my_subscriber = self.create_subscription(\n            msg_type=String,\n            topic=\"{BLANK}\",\n            callback=self.msg_callback,\n            qos_profile=10,\n        ) # (4)!\n\n        self.get_logger().info(\n            f\"The '{self.get_name()}' node is initialised.\"\n        ) # (5)!\n\n    def msg_callback(self, topic_message: String): # (6)!\n        # (7)!\n        self.get_logger().info(f\"The '{self.get_name()}' node heard:\") \n        self.get_logger().info(f\"'{topic_message.data}'\")\n\ndef main(args=None): # (8)!\n    rclpy.init(args=args)\n    my_simple_subscriber = SimpleSubscriber()\n    rclpy.spin(my_simple_subscriber)\n    my_simple_subscriber.destroy_node()\n    rclpy.shutdown() \n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>As with our publisher node, we need to import the <code>rclpy</code> client library and the <code>String</code> message type from the <code>example_interfaces.msg</code> library in order to write a Python ROS Node and use the relevant ROS messages:</p> </li> <li> <p>This time, we create a Python Class called <code>SimpleSubscriber()</code> instead, but which still inherits the <code>Node</code> class from <code>rclpy</code> as we did with the Publisher before.</p> </li> <li> <p>Once again, using the <code>super()</code> method we call the <code>__init__()</code> method from the parent Node class that our <code>SimpleSubscriber</code> class is derived from, and provide a name to use to register in on the network.</p> </li> <li> <p>We're now using the <code>create_subscription()</code> method here, which will allow this node to subscribe to messages on a ROS Topic. When calling this we provide 4 key bits of information:</p> <ol> <li> <p><code>msg_type</code>: The type of message that the topic uses (which we could obtain by running the <code>ros2 topic info</code> command).</p> <p>We know (having just created the publisher), that our topic uses <code>String</code> messages (from <code>example_interfaces</code>).</p> </li> <li> <p><code>topic</code>: The name of the topic that we want to listen (or subscribe) to.</p> <p>Fill in the Blank!</p> <p>Replace the <code>??</code> in the code above with the name of the topic that our <code>publisher.py</code> node was set up to publish to!</p> </li> <li> <p><code>callback</code>: When building a subscriber, we need a callback function, which is a function that will execute every time a new message is received from the topic.</p> <p>At this stage, we define what this callback function is called (<code>self.msg_callback</code>), and we'll actually define the function itself further down within the Class.</p> </li> <li> <p><code>qos_profile</code>: As before, a queue size to limit the amount of messages that are queued in a buffer. </p> </li> </ol> </li> <li> <p>Print a Log message to the terminal to indicate that the initialisation process has taken place.</p> </li> <li> <p>Here we're defining what will happen each time our subscriber receives a new message. This callback function must have only one argument (other than <code>self</code>), which will contain the message data that has been received:</p> <p>We're also using a Python Type Annotation here too, which informs the interpreter that the <code>topic_message</code> that is received by the <code>msg_callback</code> function will be of the <code>String</code> data type.</p> <p>(All this really does is allow autocomplete functionality to work within our text editor, so that whenever we want to pull an attribute from the <code>toic_message</code> object it will tell us what attributes actually exist within the object.)</p> </li> <li> <p>In this simple example, all we're going to do on receipt of a message is to print a couple of log messages to the terminal, to include: </p> <ol> <li> <p>The name of this node (using the <code>self.get_name()</code> method)</p> </li> <li> <p>The message that has been received (<code>topic_mesage.data</code>)</p> </li> </ol> </li> <li> <p>The rest of this is exactly the same as before with our publisher.</p> </li> </ol>"},{"location":"course/part1/subscriber/#dfts","title":"Don't Forget the Shebang!","text":"<p>Remember: don't forget the shebang, it's very important!</p> <pre><code>#!/usr/bin/env python3\n</code></pre>"},{"location":"course/part2/move_circle/","title":"A Simple Velocity Control Node (Move Circle)","text":""},{"location":"course/part2/move_circle/#the-initial-code","title":"The Initial Code","text":"<p>Start by returning to the <code>publisher.py</code> file from your <code>part1_pubsub</code> package (or return here), and copy the contents into your new <code>move_circle.py</code> file. Then, adapt the code as follows.</p>"},{"location":"course/part2/move_circle/#creating-the-move-circle-node","title":"Creating the \"Move Circle\" Node","text":""},{"location":"course/part2/move_circle/#imports","title":"Imports","text":"<p>Once again, <code>rclpy</code> and the <code>Node</code> class from the <code>rclpy.node</code> library are vital for any node we create, so the first two imports will remain the same.</p> <p>We need to import the message type used by the <code>/cmd_vel</code> topic here though, in order to be able to format velocity commands appropriately. We can find all the necessary information about this message type by using the <code>ros2 topic info</code> command:</p> <pre><code>$ ros2 topic info /cmd_vel\nType: geometry_msgs/msg/TwistStamped\n...\n</code></pre> <p>And so our message import becomes:</p> <pre><code>from geometry_msgs.msg import TwistStamped # (1)!\n</code></pre> <ol> <li>In place of: <code>from example_interfaces.msg import String</code></li> </ol>"},{"location":"course/part2/move_circle/#change-the-class-name","title":"Change the Class Name","text":"<p>Previously our publisher class was called <code>SimplePublisher()</code>, change this to <code>Circle</code>:</p> <pre><code>class Circle(Node):\n</code></pre>"},{"location":"course/part2/move_circle/#initialising-the-class","title":"Initialising the Class","text":"<ol> <li> <p>Initialise the node with an appropriate name:</p> <pre><code>super().__init__(\"move_circle\")\n</code></pre> </li> <li> <p>Change the <code>create_publisher()</code> parameters:</p> <pre><code>self.my_publisher = self.create_publisher(\n    msg_type=??, # (1)!\n    topic=??, # (2)!\n    qos_profile=10,\n)\n</code></pre> <ol> <li>What is the name of the interface that we are using here?</li> <li>What's the name of the topic that we want to publish our velocity commands to?</li> </ol> </li> <li> <p>We'll need to publish velocity commands at a rate of at least 10 Hz, so define this here, and then set up a timer accordingly:</p> <pre><code>publish_rate = 10 # Hz\nself.timer = self.create_timer(\n    timer_period_sec=1/publish_rate, \n    callback=self.timer_callback\n)\n</code></pre> </li> </ol>"},{"location":"course/part2/move_circle/#modifying-the-timer-callback","title":"Modifying the Timer Callback","text":"<p>Here, we'll publish our velocity commands:</p> <pre><code>def timer_callback(self):\n    radius = 0.5 # meters\n    linear_velocity = 0.1 # meters per second [m/s]\n    angular_velocity = ?? # radians per second [rad/s] # (1)!\n\n    topic_msg = TwistStamped() # (2)!\n    topic_msg.twist.linear.x = linear_velocity\n    topic_msg.twist.angular.z = angular_velocity\n    self.my_publisher.publish(topic_msg) # (3)!\n\n    self.get_logger().info( # (4)!\n        f\"Linear Velocity: {topic_msg.twist.linear.x:.2f} [m/s], \"\n        f\"Angular Velocity: {topic_msg.twist.angular.z:.2f} [rad/s].\",\n        throttle_duration_sec=1, # (5)!\n    )\n</code></pre> <ol> <li> <p>Having defined the radius of the circle, and the linear velocity that we want the robot to move at, how would we calculate the angular velocity that should be applied?</p> <p>Consider the equation for angular velocity:</p> <p></p> <p></p> \\[ \\omega=\\frac{v}{r} \\] </li> <li> <p><code>/cmd_vel</code> uses <code>TwistStamped</code> messages, so we instantiate one here, and assign the linear and angular velocity values (as set above) to the relevant message fields. Remember, we talked about all this here.</p> </li> <li> <p>Once the appropriate velocity fields have been set, publish the message.</p> </li> <li> <p>Publish a ROS Log Message to inform us (in the terminal) of the velocity control values that are being published by the node.</p> </li> <li> <p>Remember in the Odometry Subscriber how we used a counter (<code>self.counter</code>) and an <code>if()</code> statement to control the rate at which these log messages were generated?</p> <p>We can actually achieve exactly the same thing by simply supplying a <code>throttle_duration_sec</code> argument to the <code>get_logger().info()</code> call. Much easier, right?</p> </li> </ol>"},{"location":"course/part2/move_circle/#updating-main","title":"Updating \"Main\"","text":"<p>Once again, don't forget to update any relevant parts of the <code>main</code> function to ensure that we're instantiating the <code>Circle()</code> class, spinning and shutting it down correctly.</p>"},{"location":"course/part2/move_circle/#package-dependencies","title":"Package Dependencies","text":"<p>Our <code>move_circle.py</code> node has a new package import:</p> <pre><code>from geometry_msgs.msg import TwistStamped\n</code></pre> <p>Our <code>part2_navigation</code> package therefore has a new dependency, so we need to add this to our package's <code>package.xml</code> file.</p> <p>Earlier on we added <code>nav_msgs</code> to this (for the <code>odom_subscriber.py</code> node). Below this, add a new <code>&lt;exec_depend&gt;</code> for <code>geometry_msgs</code>:</p> package.xml<pre><code>&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;\n&lt;exec_depend&gt;nav_msgs&lt;/exec_depend&gt;\n&lt;exec_depend&gt;geometry_msgs&lt;/exec_depend&gt;  &lt;!-- (1)! --&gt;\n</code></pre> <ol> <li>ADD THIS LINE!</li> </ol> <p>Save the file and close it.</p>"},{"location":"course/part2/move_square/","title":"Odometry-based Navigation (Move Square)","text":"<p>A combined publisher-subscriber node to achieve odometry-based control...</p> <p>Below you will find a template Python script to show you how you can both publish to <code>/cmd_vel</code> and subscribe to <code>/odom</code> in the same node.  This will help you build a closed-loop controller to make your robot follow a square motion path of size: 1m x 1m. </p> <p>You can publish velocity commands to <code>/cmd_vel</code> to make the robot move, monitor the robot's position and orientation in real-time, determine when the desired movement has been completed, and then update the velocity commands accordingly.  </p>"},{"location":"course/part2/move_square/#suggested-approach","title":"Suggested Approach","text":"<p>Moving in a square can be achieved by switching between two different movement states sequentially: Moving forwards and turning on the spot. At the start of each movement step we can read the robot's current odometry, and then use this as a reference to compare to, and to tell us when the robot's position/orientation has changed by the required amount, e.g.:</p> <ol> <li>With the robot stationary, read the odometry to determine its current X and Y position in the environment.</li> <li>Move forwards until the robot's X and Y position indicate that it has moved forwards by 1m.</li> <li>Stop moving forwards.</li> <li>Read the robot's odometry to determine its current orientation (\"yaw\"/<code>\u03b8<sub>z</sub></code>).</li> <li>Turn on the spot until the robot's orientation changes by 90\u00b0 (or the equivalent in radians).</li> <li>Stop turning.</li> <li>Repeat.  </li> </ol>"},{"location":"course/part2/move_square/#the-code","title":"The Code","text":"move_square.py<pre><code>import rclpy\nfrom rclpy.node import Node\nfrom rclpy.signals import SignalHandlerOptions\n\nfrom geometry_msgs.msg import TwistStamped # (1)!\nfrom nav_msgs.msg import Odometry # (2)!\n\nfrom part2_navigation_modules.tb3_tools import quaternion_to_euler # (3)!\nfrom math import sqrt, pow, pi # (4)!\n\nclass Square(Node):\n\n    def __init__(self):\n        super().__init__(\"move_square\")\n\n        self.first_message = False\n        self.turn = False \n\n        self.vel_msg = TwistStamped() # (5)! \n        # (6)!\n        self.x = 0.0; self.y = 0.0; self.theta_z = 0.0\n        self.xref = 0.0; self.yref = 0.0; self.theta_zref = 0.0\n        # (7)!\n        self.yaw = 0.0 \n        self.displacement = 0.0 \n\n        self.vel_pub = self.create_publisher(\n            msg_type=TwistStamped,\n            topic=\"cmd_vel\",\n            qos_profile=10,\n        )\n\n        self.odom_sub = self.create_subscription(\n            msg_type=Odometry,\n            topic=\"odom\",\n            callback=self.odom_callback,\n            qos_profile=10,\n        )\n\n        ctrl_rate = 10 # hz\n        self.timer = self.create_timer(\n            timer_period_sec=1/ctrl_rate,\n            callback=self.timer_callback,\n        )\n\n        self.shutdown = False\n\n        self.get_logger().info(\n            f\"The '{self.get_name()}' node is initialised.\"\n        )\n\n    def on_shutdown(self):\n        print(\"Stopping the robot...\")\n        self.vel_pub.publish(TwistStamped())\n        self.shutdown = True\n\n    def odom_callback(self, msg_data: Odometry):\n        pose = msg_data.pose.pose \n\n        (roll, pitch, yaw) = quaternion_to_euler(pose.orientation) # (8)!\n\n        self.x = pose.position.x # (9)!\n        self.y = pose.position.y\n        self.theta_z = abs(yaw) # abs(yaw) makes life much easier!!\n\n        if not self.first_message: # (10)!\n            self.first_message = True\n            self.xref = self.x\n            self.yref = self.y\n            self.theta_zref = self.theta_z\n\n    def timer_callback(self):\n        # here is where the code to control the motion of the robot \n        # goes. Add code here to make the robot move in a square of\n        # dimensions 1 x 1m...\n        if self.turn:\n            # turn by 90 degrees...\n\n\n        else:\n            # move forwards by 1m...\n\n\n        # publish whatever velocity command has been set above:\n        self.vel_pub.publish(self.vel_msg)\n\ndef main(args=None):\n    rclpy.init(\n        args=args,\n        signal_handler_options=SignalHandlerOptions.NO,\n    )\n    move_square = Square()\n    try:\n        rclpy.spin(move_square)\n    except KeyboardInterrupt:\n        print(\n            f\"{move_square.get_name()} received a shutdown request (Ctrl+C).\"\n        )\n    finally:\n        move_square.on_shutdown()\n        while not move_square.shutdown:\n            continue\n        move_square.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <ol> <li>Import the <code>TwistStamped</code> message for publishing velocity commands to <code>/cmd_vel</code>.</li> <li>Import the <code>Odometry</code> message, for use when subscribing to the <code>/odom</code> topic.</li> <li> <p>Import the <code>quaternion_to_euler</code> method from <code>tb3_tools.py</code>... </p> <p>This is a hand method that's included in the ROS 2 Package Template to convert orientation from quaternions to Euler angles (about the principal axes).</p> <p>Any ROS 2 package that you create with our ROS 2 package template will contain this method for you to use!</p> </li> <li> <p>Finally, import some useful mathematical operations (and <code>pi</code>), which may prove useful for this task:</p> <p></p><p></p> Mathematical Operation Python Implementation \\(\\sqrt{a+b}\\) <code>sqrt(a+b)</code> \\(a^{2}+(bc)^{3}\\) <code>pow(a, 2) + pow(b*c, 3)</code> \\(\\pi r^2\\) <code>pi * pow(r, 2)</code> <p></p><p></p> </li> <li> <p>Here we establish a <code>TwistStamped</code> message, which we can populate with velocities and then publish to <code>/cmd_vel</code> within the <code>timer_callback()</code> method (in order to make the robot move).</p> </li> <li> <p>Here, we define some variables that we can use to store relevant bits of odometry data while our node is running (and read it back to implement feedback control):</p> <ul> <li><code>self.x</code>, <code>self.y</code> and <code>self.theta_z</code> will be used by the <code>odom_callback()</code> to store the robot's current pose</li> <li><code>self.xref</code>, <code>self.yref</code> and <code>self.theta_zref</code> can be used in the <code>timer_callback()</code> method to keep a record of where the robot was at a given moment in time (and determine how far it has moved since that point)</li> </ul> </li> <li> <p>We'll also need to keep track of how far the robot has travelled (or turned) in order to determine when sufficient movement has taken place to trigger a switch to the alternative state, i.e.:</p> <ul> <li><code>if</code> travelled 1 meter, <code>then</code>: turn</li> <li><code>if</code> turned 90\u00b0, <code>then</code>: move forward</li> </ul> </li> <li> <p>Here we obtain the robot's current orientation (in quaternions) and convert it to Euler angles (in radians) about the principal axes, where:</p> <ul> <li>\"roll\" = <code>\u03b8<sub>x</sub></code></li> <li>\"pitch\" = <code>\u03b8<sub>y</sub></code></li> <li>\"yaw\" = <code>\u03b8<sub>z</sub></code></li> </ul> </li> <li> <p>We're only interested in <code>x</code>, <code>y</code> and <code>\u03b8<sub>z</sub></code>, so we assign these to class variables <code>self.x</code>, <code>self.y</code> and <code>self.theta_z</code>, so that we can access them elsewhere within our <code>Square()</code> class.</p> </li> <li> <p>Sometimes, it can take a few moments for the first topic message to come through, and it's useful to know when that's happened so that you know you are dealing with actual topic data! Here, we're just setting a flag to <code>True</code> once the callback function has executed for the first time (i.e. the first topic message has been received).</p> </li> </ol>"},{"location":"course/part2/move_square/#alternative-approach-waypoint-tracking","title":"Alternative Approach: Waypoint Tracking","text":"<p>A square motion path can be fully defined by the coordinates of its four corners, and we can make the robot move to each of these corners one-by-one, using its odometry system to monitor its real-time position, and adapting linear and angular velocities accordingly.</p> <p>This is slightly more complicated, and you might want to wait until you have a bit more experience with ROS before tackling it this way.</p>"},{"location":"course/part2/odom_subscriber/","title":"An Odometry Subscriber Node","text":""},{"location":"course/part2/odom_subscriber/#the-initial-code","title":"The Initial Code","text":"<p>Having copied the <code>subscriber.py</code> file from your <code>part1_pubsub</code> package, you'll start out with the code discussed here.</p> <p>Let's look at what we need to change now.</p>"},{"location":"course/part2/odom_subscriber/#from-simple-subscriber-to-odom-subscriber","title":"From Simple Subscriber to Odom Subscriber","text":""},{"location":"course/part2/odom_subscriber/#imports","title":"Imports","text":"<p>We will generally rely on <code>rclpy</code> and the <code>Node</code> class from the <code>rclpy.node</code> library for most nodes that we will create, so our first two imports will remain the same. </p> <p>We won't be working with <code>String</code> type messages any more however, so we need to replace this line in order to import the correct message type. As we know from earlier in Part 2, the <code>/odom</code> topic uses messages of the type <code>nav_msgs/msg/Odometry</code>:</p> <pre><code>$ ros2 topic info /odom\nType: nav_msgs/msg/Odometry\n...\n</code></pre> <p>This tells us everything we need to know to construct the Python import statement correctly:</p> <pre><code>from nav_msgs.msg import Odometry\n</code></pre> <p>You'll also need to import a mathematical tool from the <code>math</code> module:</p> <pre><code>from math import atan2\n</code></pre> <p>(for reasons that will become clearer shortly.)</p>"},{"location":"course/part2/odom_subscriber/#change-the-class-name","title":"Change the Class Name","text":"<p>Previously our class was called <code>SimpleSubscriber()</code>, change this to something more appropriate now, e.g.: <code>OdomSubscriber()</code>:</p> <pre><code>class OdomSubscriber(Node):\n</code></pre>"},{"location":"course/part2/odom_subscriber/#initialising-the-class","title":"Initialising the Class","text":"<p>The structure of this remains largely the same, we just need to modify a few things: </p> <ol> <li> <p>Change the name that is used to register the node on the ROS Network:</p> <pre><code>super().__init__(\"odom_subscriber\")\n</code></pre> </li> <li> <p>Change the subscription parameters:</p> <pre><code>self.my_subscriber = self.create_subscription(\n    msg_type=Odometry, # (1)!\n    topic=\"/odom\", # (2)!\n    callback=self.msg_callback, \n    qos_profile=10,\n)\n</code></pre> <ol> <li><code>/odom</code> uses the Odometry message type (as imported above)</li> <li>The topic name is <code>/odom</code>. You can also omit the forward slash when defining this, so <code>topic=\"odom\"</code> would also work. </li> </ol> </li> <li> <p>The final thing we'll do inside our class' <code>__init__</code> method (after we've set up the subscriber) is initialise a counter:</p> <pre><code>self.counter = 0 \n</code></pre> <p>The reason for this will be explained shortly...</p> </li> </ol>"},{"location":"course/part2/odom_subscriber/#calculating-euler-angles-from-quaternions","title":"Calculating Euler Angles from Quaternions","text":"<p>After the <code>__init__</code> class method, define a new method inside the <code>OdomSubscriber()</code> class, called <code>quaternion_to_euler</code>:</p> <pre><code>def quaternion_to_euler(self, orientation):\n    x = orientation.x\n    y = orientation.y\n    z = orientation.z\n    w = orientation.w\n\n    yaw = # TODO: calculate yaw...\n\n    return yaw # (in radians)\n</code></pre> <p>This function will receive the orientation data from the <code>/odom</code> topic (in quaternions) and needs to output the <code>yaw</code> angle in radians. Your job now is to establish the actual conversion process (which can be found here). Implement the calculation so that your <code>quaternion_to_euler()</code> method actually outputs a correct yaw angle (in radians) for the robot.</p>"},{"location":"course/part2/odom_subscriber/#modifying-the-message-callback","title":"Modifying the Message Callback","text":"<p>Head to the existing <code>msg_callback</code> class method now and change this as follows:</p> <pre><code>def msg_callback(self, topic_message: Odometry): # (1)!\n\n    pose = topic_message.pose.pose # (2)!\n\n    # (3)!\n    pos_x = pose.position.x\n    pos_y = pose.position.y\n    pos_z = pose.position.z\n\n    yaw = self.quaternion_to_euler(pose.orientation) # (4)!\n\n    if self.counter &gt; 10: # (5)!\n        self.counter = 0\n        self.get_logger().info(\n            f\"x = {pos_x:.3f} (m), y = ? (m), yaw = ? (radians)\"\n        ) # (6)!\n    else:\n        self.counter += 1\n</code></pre> <ol> <li>This is a type annotation. The topic that we are subscribing to has changed (previously <code>/my_topic</code>, now <code>/odom</code>), and the new topic uses a different datatype (a.k.a. \"ROS Interface\"). We therefore update the type annotation to match the new type of data that will be entering this callback method (via the <code>topic_mesage</code> variable).</li> <li> <p>We're only really interested in the Pose part of the odometry data, so we assign this to a variable.</p> </li> <li> <p>As we know by now, Pose contains information about both the \"position\" and \"orientation\" of the robot, we extract the position values first and assign them to the variables <code>pos_x</code>, <code>pos_y</code> and <code>pos_z</code>.</p> <p>Position data is provided in meters, so we don't need to do any conversion on this and can use the data directly.</p> </li> <li> <p>Orientation data is in quaternions, so we need to convert this to a Euler angle representation. We're calling a class method called <code>self.quaternion_to_euler()</code> to handle this conversion, which you should have established in the previous step.</p> </li> <li> <p>Here we print out the values that we're interested in to the terminal.</p> <p>This callback function will execute every time a new message is published to the <code>odom</code> topic, which occurs at a rate of around 20 times per second (20 Hz).</p> Tip <p>We can use he <code>ros2 topic hz</code> function to tell us this:</p> <pre><code>$ ros2 topic hz /odom\naverage rate: 18.358\nmin: 0.037s max: 0.088s std dev: 0.01444s window: 20\n</code></pre> <p>That's a lot of messages to be printed to the terminal every second! We therefore use an <code>if</code> statement and a <code>counter</code> to ensure that our <code>print</code> statement only executes for 1 in every 10 topic messages instead.</p> </li> <li> <p>Task: Continue formatting the <code>print</code> message to display the three odometry values that are relevant to our robot!  </p> </li> </ol>"},{"location":"course/part2/odom_subscriber/#updating-main","title":"Updating \"Main\"","text":"<p>The only thing left to do now is update any relevant parts of the <code>main</code> function to ensure that you are instantiating, spinning and shutting down your node correctly.</p>"},{"location":"course/part2/odom_subscriber/#package-dependencies","title":"Package Dependencies","text":"<p>Once again, we're importing a couple of Python libraries into our node here, which means that our package has two dependencies: <code>rclpy</code> and <code>nav_msgs</code>:</p> <pre><code>import rclpy \nfrom rclpy.node import Node\n\nfrom nav_msgs.msg import Odometry\n</code></pre> <p>We should therefore add these dependencies to our <code>package.xml</code> file (<code>part2_navigation/package.xml</code>). Open it up and find the following line:</p> <pre><code>&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;\n</code></pre> <p>The package template already includes a dependency for <code>rclpy</code>, since this is pretty fundamental to our work here, but we do need to add <code>nav_msgs</code> as well, so add the following additional line underneath:</p> <pre><code>&lt;exec_depend&gt;nav_msgs&lt;/exec_depend&gt;\n</code></pre> <p>Save the file and close it.</p>"},{"location":"course/part3/lidar_subscriber/","title":"Building a Basic LaserScan Subscriber Node","text":""},{"location":"course/part3/lidar_subscriber/#the-code","title":"The Code","text":"<p>Copy all the code below into your <code>lidar_subscriber.py</code> file and then review the annotations to understand how it all works.</p> lidar_subscriber.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.signals import SignalHandlerOptions # (1)!\n\nfrom sensor_msgs.msg import LaserScan # (2)!\nimport numpy as np # (3)!\n\nclass LidarSubscriber(Node): \n\n    def __init__(self): \n        super().__init__(\"lidar_subscriber\")\n\n        self.lidar_sub = self.create_subscription(\n            msg_type=LaserScan,\n            topic=\"/scan\",\n            callback=self.lidar_callback,\n            qos_profile=10,\n        ) # (4)!\n\n        self.get_logger().info(f\"The '{self.get_name()}' node is initialised.\")\n\n    def lidar_callback(self, scan_data: LaserScan): \n        left = scan_data.ranges[0:21] \n        right = scan_data.ranges[-20:] # (5)!\n        front = np.array(right + left) # (6)!\n\n        valid_data = front[front != float(\"inf\")] # (7)!\n        if np.shape(valid_data)[0] &gt; 0: # (8)!\n            single_point_average = valid_data.mean() # (9)!\n        else:\n            single_point_average = float(\"nan\") # (10)!\n\n        self.get_logger().info(\n            f\"LiDAR Reading (front): {single_point_average:.3f} meters.\",\n            throttle_duration_sec = 1,\n        ) # (11)!\n\ndef main(args=None):\n    rclpy.init(\n        args=args,\n        signal_handler_options=SignalHandlerOptions.NO\n    )\n    node = LidarSubscriber()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        print(\"Shutdown request (Ctrl+C) detected...\")\n    finally:\n        node.destroy_node()\n        rclpy.shutdown() \n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li>None of this should be new to you by now. Remember from Part 2 that we're using <code>SignalHandlerOptions</code> to handle shutdown requests (triggered by Ctrl+C).</li> <li>We're building a <code>/scan</code> subscriber here, and we know that this topic uses the <code>sensor_msgs/msg/LaserScan</code> interface type, so we import this here.</li> <li><code>numpy</code> is a Python library that allows us to work with numeric data, very useful for big arrays like <code>ranges</code>.</li> <li>We construct a subscriber in much the same way as we have done in Parts 1 and 2, this time targetting the <code>/scan</code> topic though.</li> <li> <p>From the front of the robot, we obtain a 20\u00b0 arc of scan data either side of the x-axis (see the figure below).</p> </li> <li> <p>Then, we combine the <code>left_20_deg</code> and <code>right_20_deg</code> data arrays, and convert this from a Python list to a <code>numpy</code> array (see the figure below).</p> </li> <li> <p>This illustrates one of the great features of <code>numpy</code> arrays: we can filter them.</p> <p>Remember that <code>front</code> is now a <code>numpy</code> array containing 40 data points.</p> <p>Remember also, that there will typically be several <code>inf</code> values scattered around the LaserScan array, resulting from sensor readings that are outside the sensor's measurement range (i.e. greater than <code>range_max</code> or less than <code>range_min</code>). We need to get rid of these, so we ask <code>numpy</code> to filter our array as follows:</p> <ol> <li> <p>Of all the values in the <code>front</code> array, determine which ones are not equal to <code>inf</code>: </p> <p><code>front != float(\"inf\")</code></p> </li> <li> <p>Use this filter to remove these <code>inf</code> values from the <code>front</code> array:</p> <p><code>front[front != float(\"inf\")]</code></p> </li> <li> <p>Return this as a new <code>numpy</code> array called <code>valid_data</code>:</p> <p><code>valid_data = front[front != float(\"inf\")]</code></p> </li> </ol> </li> <li> <p>In certain situations (i.e. in very sparse environments) all values could be equal to<code>inf</code> (imagine the \"empty world\" simulation). Here we're checking the size of the <code>valid_data</code> array to make sure that we haven't just removed all values through the above filtering process!</p> </li> <li> <p>If the array is not empty, then use the <code>mean()</code> method to determine the average of all readings within the dataset</p> </li> <li>If the array is empty, then return \"not a number\" (aka \"nan\") instead </li> <li> <p>Print this value to the terminal, but throttle the messages so that only one is displayed every second</p> <p>Question</p> <p>If we didn't throttle this, what rate would the messages be printed at?</p> </li> </ol> <p>The data processing is illustrated in the figure below:</p> <p></p>"},{"location":"course/part3/lidar_subscriber/#package-dependencies","title":"Package Dependencies","text":"<p>This node has dependencies on two external Python libraries (in addition to <code>rclpy</code>): </p> <pre><code>from sensor_msgs.msg import LaserScan\nimport numpy as np\n</code></pre> <p>As such, you should include these in the <code>package.xml</code> file (under the <code>&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;</code> line):</p> package.xml<pre><code>&lt;exec_depend&gt;sensor_msgs&lt;/exec_depend&gt;\n&lt;exec_depend&gt;python3-numpy&lt;/exec_depend&gt;\n</code></pre>"},{"location":"course/part3/move_circle/","title":"A move_circle.py Worked Example","text":"<p>A working <code>move_circle.py</code> node (from Part 2 Exercise 5) complete with a proper shutdown procedure. Use this as a starting point for your <code>param_circle.py</code> node for the Part 3 Parameters Exercise.</p>"},{"location":"course/part3/move_circle/#the-code","title":"The Code","text":"move_circle.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy \nfrom rclpy.node import Node\nfrom rclpy.signals import SignalHandlerOptions\n\nfrom geometry_msgs.msg import TwistStamped\n\nclass Circle(Node): \n\n    def __init__(self):\n        super().__init__(\"move_circle\") \n\n        self.my_publisher = self.create_publisher(\n            msg_type=TwistStamped,\n            topic=\"/cmd_vel\",\n            qos_profile=10,\n        ) \n\n        publish_rate = 1 # Hz\n        self.timer = self.create_timer(\n            timer_period_sec=1/publish_rate, \n            callback=self.timer_callback\n        ) \n\n        self.get_logger().info(\n            f\"The '{self.get_name()}' node is initialised.\" \n        )\n\n        self.shutdown = False\n\n    def on_shutdown(self):\n        self.get_logger().info(\n            \"Stopping the robot...\"\n        )\n        self.my_publisher.publish(TwistStamped()) \n        self.shutdown = True\n\n    def timer_callback(self):\n        radius = 0.5 # meters\n        linear_velocity = 0.1 # meters per second [m/s]\n        angular_velocity = linear_velocity / radius  \n\n        topic_msg = TwistStamped() \n        topic_msg.twist.linear.x = linear_velocity\n        topic_msg.twist.angular.z = angular_velocity\n        self.my_publisher.publish(topic_msg) \n\n        self.get_logger().info( \n            f\"Linear Velocity: {topic_msg.twist.linear.x:.2f} [m/s], \"\n            f\"Angular Velocity: {topic_msg.twist.angular.z:.2f} [rad/s].\",\n            throttle_duration_sec=1, \n        )\n\ndef main(args=None): \n    rclpy.init(\n        args=args,\n        signal_handler_options=SignalHandlerOptions.NO\n    )\n    move_circle = Circle()\n    try:\n        rclpy.spin(move_circle)\n    except KeyboardInterrupt:\n        print(\n            f\"{move_circle.get_name()} received a shutdown request (Ctrl+C).\"\n        )\n    finally:\n        move_circle.on_shutdown()\n        while not move_circle.shutdown:\n            continue\n        move_circle.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__': \n    main()\n</code></pre>"},{"location":"course/part3/move_circle/#adding-package-dependencies","title":"Adding Package Dependencies","text":"<p>Make sure you define this node's dependencies within your package's <code>package.xml</code> file.</p> <p>Find this line:</p> package.xml<pre><code>...\n&lt;exec_depend&gt;geometry_msgs&lt;/exec_depend&gt;\n...\n</code></pre>"},{"location":"course/part4/number_game_client/","title":"Creating a Python Service Client","text":"<p>Copy all the code below into your <code>number_game_client.py</code> file and then review the annotations to understand how it all works.</p> number_game_client.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\n\nfrom part4_services.srv import MyNumberGame\n\nclass NumberGameClient(Node):\n\n    def __init__(self):\n        super().__init__('number_game_client')\n\n        self.client = self.create_client(\n            srv_type=MyNumberGame, \n            srv_name='guess_the_number'\n        ) # (1)!\n\n        self.declare_parameters(\n            namespace='',\n            parameters=[\n                ('guess', 0),\n                ('cheat', False)\n            ]\n        ) # (2)!\n\n        while not self.client.wait_for_service(timeout_sec=1.0):\n            self.get_logger().info(\n                \"Waiting for service...\"\n            ) # (3)!\n\n    def send_request(self): # (4)!\n        guess_input = self.get_parameter(\n            'guess' \n        ).get_parameter_value().integer_value # (5)!\n        cheat_input = self.get_parameter(\n            'cheat'\n        ).get_parameter_value().bool_value # (6)!\n\n        self.get_logger().info(\n            f\"Sending the request:\\n\"\n            f\" - guess: {guess_input}\\n\"\n            f\" - cheat: {cheat_input}\\n\"\n            f\"   Awaiting response...\"\n        ) # (7)!\n\n        request = MyNumberGame.Request() # (8)!\n        request.guess = guess_input\n        request.cheat = cheat_input\n\n        return self.client.call_async(request) # (9)!\n\ndef main():\n    rclpy.init()\n    client = NumberGameClient()\n\n    future = client.send_request() # (10)!\n    rclpy.spin_until_future_complete(client, future) # (11)!\n    response = future.result() # (12)!\n\n    client.get_logger().info(\n        f\"The server responded with:\\n\"\n        f\" - {'You guessed correctly! :)' if response.correct else 'Incorrect guess :('}\\n\"\n        f\" - Number of attempts so far: {response.num_guesses}\\n\"\n        f\" - A hint: '{response.hint}'.\"\n    ) # (13)!\n\n    client.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>Creating a Service Client is done using the <code>create_client()</code> class method, providing the name of the service that we want to call (<code>srv_name</code>), and specifying the interface type used by it (<code>srv_type</code>). </p> <p><code>srv_type</code> and <code>srv_name</code> must match the definition in the server, in order to be able to communicate and send requests to it. </p> </li> <li> <p>We're declaring some parameters here (recall Part 3). We'll be using these differently this time however: to create a command-line interface (CLI) for our node. Essentially, we'll be setting these parameters dynamically when we call it with <code>ros2 run</code> (so that we can change the values each time we launch the client).</p> <p>We're defining two parameters for the node, to match the attributes of the service request:</p> <p>1.<code>\"guess\"</code>: An integer, with a default value of <code>0</code>.  2. <code>\"cheat\"</code>: A boolean, with a default value of <code>False</code>.</p> <p>(You'll see how this all works shortly, when we actually run the node.)</p> </li> <li> <p>We use a <code>while</code> loop here to halt the execution of the code at this point and wait for the service to become available (if it isn't already). </p> <p>We can't send a request to a service that isn't actually running!</p> </li> <li> <p>In this class method we construct the service request and send it.</p> <p>(This method is called in the <code>main()</code> function below.)</p> </li> <li> <p>Read the value of the <code>guess</code> parameter, which we'll set from the command-line when we call the node (with <code>ros2 run</code>).</p> <p>This has been split across three lines, otherwise it gets too long and runs off the screen!</p> </li> <li> <p>Read the value of the <code>cheat</code> parameter, which we'll also set from the command-line (we'll look at this shortly).</p> <p>Also split across three lines for no reason other than readability!</p> </li> <li> <p>Here we're printing the parameter values to the terminal as a log message, to confirm exactly what request will be sent to the server.</p> </li> <li> <p>Here we actually construct the request, using a <code>part4_services/srv/MyNumberGame</code> interface class instance, as imported up at the top.</p> </li> <li> <p><code>call_async(request)</code> then actually sends this request to the server.</p> </li> <li> <p>We then call our client's <code>send_request()</code> class method, which in turn (as you know from above) will initiate the construction of the request and send it to the server. </p> <p>The output of this function is the output of the <code>call_async(request)</code> call, which we assign to a variable called <code>future</code>.</p> </li> <li> <p>We use the <code>rclpy.spin_until_future_complete()</code> method here, which (as the name suggests) will allow our node (<code>client</code>) to spin only until our service request (<code>future</code>) has completed. </p> </li> <li> <p>Once we've reached this point then the service has completed and returned its Response. </p> <p>We obtain the response from our <code>future</code> object so that we can read its values...</p> </li> <li> <p>To finish off, we construct a final log message containing the values returned by the Server (i.e. the Response). </p> <p>We know what these attributes are called, because we defined them in the <code>MyNumberGame.srv</code> file, which we can recall at any point using <code>ros2 interface show</code>:</p> <pre><code>$ ros2 interface show part4_services/srv/MyNumberGame\n\nint32 guess\nbool cheat\n---\nint32 num_guesses\nstring hint\nbool correct\n</code></pre> </li> </ol>"},{"location":"course/part5/explore_server/","title":"The Part 5 Explore Server Template","text":""},{"location":"course/part5/explore_server/#the-code","title":"The Code","text":"explore_server.py<pre><code>#!/usr/bin/env python3\n\n# Import the core Python libraries for ROS and to implement ROS Actions:\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionServer, GoalResponse, CancelResponse\n\n# Import additional rclpy libraries for multithreading and shutdown\nfrom rclpy.executors import MultiThreadedExecutor\nfrom rclpy.callback_groups import ReentrantCallbackGroup\nfrom rclpy.signals import SignalHandlerOptions\n\n# Import our package's action interface:\nfrom part5_actions.action import ExploreForward\n# Import other key ROS interfaces that this server will use:\nfrom geometry_msgs.msg import TwistStamped\nfrom nav_msgs.msg import Odometry\nfrom sensor_msgs.msg import LaserScan\n\n# Import some other useful Python Modules\nfrom math import sqrt, pow\nimport numpy as np\n\nclass ExploreForwardServer(Node):\n\n    def __init__(self):\n        super().__init__(\"explore_forward_server_node\")\n\n        self.posx = 0.0 # (1)!\n        self.posy = 0.0\n        self.lidar_reading = 0.0\n\n        self.shutdown = False # (2)!\n\n        self.await_odom = True # (3)!\n        self.await_lidar = True\n\n        self.loop_rate = self.create_rate(\n            frequency=5, \n            clock=self.get_clock()\n        ) # (4)!\n\n        ## TASK: create a /cmd_vel publisher\n        self.vel_pub = self.create_publisher(...)\n\n        ## TASK: create an /odom subscriber \n        self.odom_sub = self.create_subscription(...)\n\n        ## TASK: create a /scan subscriber\n        self.lidar_sub = self.create_subscription(...)\n\n        # Creating the action server:\n        self.actionserver = ActionServer(\n            node=self, \n            action_type=ExploreForward,\n            action_name=\"explore_forward\",\n            execute_callback=self.server_execution_callback,\n            callback_group=ReentrantCallbackGroup(),\n            goal_callback=self.goal_callback,\n            cancel_callback=self.cancel_callback\n        )\n\n    def odom_callback(self, odom_msg: Odometry):\n        \"\"\"\n        Callback for the /odom subscriber\n        \"\"\"\n        ## TASK: obtain the robot's X and Y position:\n        self.posx = # (5)!\n        self.posy = \n\n        self.await_odom = False\n\n    def lidar_callback(self, scan_msg: LaserScan):\n        \"\"\"\n        Callback for the /scan subscriber\n        \"\"\"\n        ## TASK: Obtain the front 40 degrees of LiDAR\n        # data, filter it and return the average distance\n        # (or 'nan'):\n        self.lidar_reading = # (6)!\n\n        self.await_lidar = False\n\n    def goal_callback(self, goal: ExploreForward.Goal):\n        \"\"\"\n        A callback to check that the goal inputs are valid        \n        \"\"\"\n        goal_ok = True\n        ## TASK: check the 'fwd_velocity' goal parameter is valid\n        # to ensure that the robot moves forward but that it \n        # doesn't exceed is max velocity limit:\n        if goal.fwd_velocity ...\n\n\n\n            goal_ok = False\n\n        ## TASK: check the 'stopping_distance' goal parameter is valid\n        # (it should be at least 0.2m to make sure it doesn't get too \n        # close!)\n        if goal.stopping_distance ...\n\n\n\n            goal_ok = False\n\n        return GoalResponse.ACCEPT if goal_ok else GoalResponse.REJECT\n\n    def cancel_callback(self, goal):\n        \"\"\"\n        A callback to trigger cancellation of the action\n        \"\"\"\n        self.get_logger().info('Received a cancel request...')\n        return CancelResponse.ACCEPT\n\n    def on_shutdown(self):\n        \"\"\"\n        A method to stop the robot on shutdown\n        \"\"\"\n        for i in range(5):\n            self.vel_pub.publish(TwistStamped())\n        self.shutdown = True\n\n    def server_execution_callback(self, goal):\n        \"\"\"\n        A callback to encapsulate the action that is performed\n        when the server is called (i.e. a valid goal is issued)\n        \"\"\"\n        result = ExploreForward.Result()\n        feedback = ExploreForward.Feedback()\n        fwd_vel = goal.request.fwd_velocity\n        stop_dist = goal.request.stopping_distance\n\n        self.get_logger().info(\n            f\"\\n#####\\n\"\n            f\"The '{self.get_name()}' has been called.\\n\"\n            f\"Goal:\\n\"\n            f\"  - explore at {fwd_vel:.2f} m/s\\n\"\n            f\"  - stop {stop_dist:.2f} m in front of something\\n\" \n            f\"Here we go...\"\n            f\"\\n#####\\n\")\n\n        # set the robot's velocity (based on the request):\n        vel_cmd = TwistStamped()\n        vel_cmd...\n\n        # Get the robot's current position:\n        while self.await_odom or self.await_lidar:\n            continue\n        ref_posx = self.posx\n        ref_posy = self.posy\n        dist_travelled = 0.0\n\n        ## TASK: establish a while loop that executes until lidar readings \n        # indicate that an object is closer than the requested stopping distance\n        while SOMETHING &gt; SOMETHING_ELSE:\n\n            ## TASK: publish a velocity command to make the robot move forward \n            # at the requested velocity...\n\n            # check if there has been a request to cancel the action:\n            if goal.is_cancel_requested:\n                # stop the robot:\n                for i in range(5):\n                    self.vel_pub.publish(TwistStamped())\n                goal.canceled()\n                self.get_logger().info(\n                    f\"Cancelled.\"\n                )\n                result.total_distance_travelled = dist_travelled\n                result.closest_obstacle = float(self.lidar_reading)\n                return result\n\n            ## TASK: calculate the distance travelled so far:\n            dist_travelled = \n\n            feedback.current_distance_travelled = dist_travelled\n            goal.publish_feedback(feedback)\n\n            self.loop_rate.sleep() # (7)!\n\n        for i in range(5):\n            self.vel_pub.publish(TwistStamped())\n\n        self.get_logger().info(\n            f\"{self.get_name()} complete.\"\n        )\n        goal.succeed()\n        return result\n\ndef main(args=None):\n    rclpy.init(\n        args=args,\n        signal_handler_options=SignalHandlerOptions.NO\n    )\n    node = ExploreForwardServer()\n    executor = MultiThreadedExecutor()\n    executor.add_node(node)\n    try:\n        node.get_logger().info(\n            \"Starting the Server (shut down with Ctrl+C)\"\n        )\n        executor.spin()\n    except KeyboardInterrupt:\n        node.get_logger().info(\n            \"Server shut down with Ctrl+C\"\n        )\n    finally:\n        ## TASK: complete all necessary shutdown operations.\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li>Some variables to store data from the <code>/odom</code> (<code>posx</code> and <code>posy</code>) and <code>/scan</code> (<code>lidar_reading</code>) subscribers, and share this data across the class.</li> <li>A shutdown flag (you've seen this before)</li> <li>Some flags to determine when we've received data from the <code>/odom</code> and <code>/scan</code> subscribers, to ensure that our action server's main execution callback doesn't begin until we have some valid data to work with (see usage below).</li> <li>Here we're creating an object that we can use in our action server's main execution callback to control the rate of execution inside a <code>while</code> loop (see usage below).</li> <li>Refer back to the Part 2 Odometry Subscriber for help with this (if you need it).</li> <li>Refer back to the Part 3 Lidar Subscriber for help with this (if you need it).</li> <li> <p>Calling this here will block any further code execution until enough time has elapsed. </p> <p>This time is dictated by the <code>frequency</code> parameter that we defined when we set this up earlier:</p> <pre><code>self.loop_rate = self.create_rate(\n    frequency=5, \n    clock=self.get_clock()\n)\n</code></pre> </li> </ol>"},{"location":"course/part5/explore_server/#dependencies","title":"Dependencies","text":"<p>We're introducing a few more dependencies here, due to the various additional Python modules that the Explore Server imports. Make sure you add these to your <code>package.xml</code> file: </p> package.xml<pre><code>&lt;exec_depend&gt;sensor_msgs&lt;/exec_depend&gt;\n&lt;exec_depend&gt;geometry_msgs&lt;/exec_depend&gt;\n&lt;exec_depend&gt;nav_msgs&lt;/exec_depend&gt;\n&lt;exec_depend&gt;python3-numpy&lt;/exec_depend&gt;\n</code></pre>"},{"location":"course/part5/minimal_action_client/","title":"A Minimal Action Client","text":""},{"location":"course/part5/minimal_action_client/#the-code","title":"The Code","text":"<p>Review the code (including the annotations) and then take a copy of it.</p> camera_sweep_action_client.py<pre><code>#! /usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient # (1)!\n\nfrom tuos_interfaces.action import CameraSweep # (2)!\n\nclass CameraSweepActionClient(Node):\n\n    def __init__(self):\n        super().__init__(\"camera_sweep_action_client\") # (3)!\n        self.actionclient = ActionClient(\n            node=self, \n            action_type=CameraSweep, \n            action_name=\"camera_sweep\"\n        ) # (4)!\n\n        self.declare_parameters(\n            namespace='',\n            parameters=[\n                ('goal_images', 0),\n                ('goal_angle', 0)\n            ]\n        ) # (5)!\n\n    def send_goal(self): # (6)!\n        images = self.get_parameter(\n            'goal_images' \n        ).get_parameter_value().integer_value \n        angle = self.get_parameter(\n            'goal_angle'\n        ).get_parameter_value().integer_value\n\n        goal = CameraSweep.Goal() # (7)!\n        goal.sweep_angle = float(angle)\n        goal.image_count = images\n\n        self.actionclient.wait_for_server() # (8)!\n\n        # send the goal to the action server:\n        return self.actionclient.send_goal_async(goal)\n\ndef main(args=None): # (9)!\n    rclpy.init(args=args)\n    action_client = CameraSweepActionClient()\n    future = action_client.send_goal()\n    rclpy.spin_until_future_complete(action_client, future)\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>As you know by now, in order to develop ROS nodes using Python we need to import the <code>rclpy</code> client library, and the <code>Node</code> class to base our node upon. In addition, here we're also importing an <code>ActionClient</code> class too.  </p> </li> <li> <p>We know that the <code>/camera_sweep</code> Action server uses the <code>CameraSweep</code> <code>action</code> interface from the <code>tuos_interfaces</code> package, so we import that here too (which we use to make a call to the server). </p> </li> <li> <p>Standard practice when we initialise ROS nodes: we must give them a name</p> </li> <li> <p>Here, we instantiate an <code>ActionClient</code> class object. In doing this we define the <code>node</code> to add the action client too (in our case <code>self</code>, i.e. our <code>CameraSweepActionClient</code> class). We then also define the interface type used by the server (<code>CameraSweep</code>), and the name of the action that we want to call (<code>action_name=\"camera_sweep\"</code>).</p> </li> <li> <p>Here we're declaring two ROS parameters: <code>goal_images</code> and <code>goal_angle</code>. </p> <p>We'll use these to set goals for the action server at runtime.</p> <p>By default, these values are set to <code>0</code>, so if we don't explicitly set values for these two parameters then they will remain at <code>0</code>!</p> <p>Question</p> <p>How do we set values for parameters at runtime (i.e. when we execute this node using <code>ros2 run</code>)?</p> <p>Recall how we did this in Part 4. </p> </li> <li> <p>Here we define a class method to construct and deliver a goal to the server. </p> </li> <li> <p>As we know from earlier, a <code>CameraSweep.Goal()</code> contains two parameters that we can assign values to: <code>sweep_angle</code> and <code>image_count</code>.</p> <p>As above, the values assigned to these are derived from two ROS parameters: <code>goal_angle</code> and <code>goal_images</code>.</p> <p>Remember</p> <p>By default, both parameters will have a value of <code>0</code> unless we explicitly assign a value to them (see above)!</p> <p>How do we assign values to these parameters at runtime? Recall how we did this in Part 4.</p> </li> <li> <p>The goal is sent to the server using the <code>send_goal_async()</code> method, which returns a future: i.e. something that will happen in the future, that we can wait on. This future is returned once the goal parameters have been accepted by the server, not when the action server has actually completed its job.</p> </li> <li> <p>In our <code>main</code> method we initialise <code>rclpy</code> and our <code>CameraSweepActionClient</code> class (nothing new here), but then we call the <code>send_goal()</code> method of our class (as discussed above), which returns a future. We can then use the <code>rclpy.spin_until_future_complete()</code> method to spin up our node only until this future object has finished.</p> </li> </ol>"},{"location":"course/part5/minimal_action_client/#package-dependencies","title":"Package Dependencies","text":"<p>The action client has two key dependencies, so we need to modify the <code>package.xml</code> file (below the <code>&lt;exec_depend&gt;rclpy&lt;/exec_depend&gt;</code> line) to include these:</p> package.xml<pre><code>&lt;exec_depend&gt;action_msgs&lt;/exec_depend&gt;\n&lt;exec_depend&gt;tuos_interfaces&lt;/exec_depend&gt;\n</code></pre>"},{"location":"course/part6/line_follower/","title":"Part 6 Line Following (Setup)","text":""},{"location":"course/part6/line_follower/#part-6-line-following-setup","title":"Part 6 Line Following (Setup)","text":"<p>Use this code as a starting point for Part A of the Line Following exercise.</p> line_follower.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.signals import SignalHandlerOptions\n\nimport cv2\nfrom cv_bridge import CvBridge, CvBridgeError\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import TwistStamped\n\nclass LineFollower(Node):\n\n    def __init__(self):\n        super().__init__(\"line_follower\")\n\n        self.declare_parameter(\"kp\", 0.01)\n\n        self.camera_sub = self.create_subscription(\n            msg_type=Image,\n            topic=\"/camera/image_raw\",\n            callback=self.camera_callback,\n            qos_profile=10,\n        )\n\n        self.vel_pub = self.create_publisher(\n            msg_type=TwistStamped,\n            topic=\"/cmd_vel\",\n            qos_profile=10\n        )\n\n        self.vel_cmd = TwistStamped()\n        self.shutdown = False\n\n    def shutdown_ops(self):\n        self.get_logger().info(\n            \"Shutting down...\"\n        )\n        cv2.destroyAllWindows()\n        for i in range(5):\n            self.vel_pub.publish(TwistStamped())\n        self.shutdown = True\n\n    def camera_callback(self, img_data):\n        cvbridge_interface = CvBridge()\n        try:\n            cv_img = cvbridge_interface.imgmsg_to_cv2(\n                img_data, desired_encoding=\"bgr8\")\n        except CvBridgeError as e:\n            self.get_logger().warn(f\"{e}\")\n\n        cv2.imshow(\"camera image\", cv_img)\n\n        height, width, _ = cv_img.shape\n        ## TODO 1 (1)\n\n        ## TODO 2 (2)\n\n        ## TODO 3 (3)\n\n        cv2.waitKey(1)\n\ndef main(args=None):\n    rclpy.init(\n        args=args,\n        signal_handler_options=SignalHandlerOptions.NO\n    )\n    node = LineFollower()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\n            f\"{node.get_name()} received a shutdown request (Ctrl+C)\"\n        )\n    finally:\n        node.shutdown_ops()\n        while not node.shutdown:\n            continue\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>Image Cropping </p> <p>Apply some cropping to the raw camera image (<code>cv_img</code>). </p> <p>Crop it to around 1/5 of its original height, and to a width so that the pink line is just visible at the edge of the image. </p> <p>Call your new cropped image something like <code>cropped_img</code>. You could then use the <code>cv2.imshow()</code> method to display this in an additional pop-up window when the node is run: </p> <pre><code>cv2.imshow(\"cropped_image\", cropped_img)\n</code></pre> <p></p> <p></p> </li> <li> <p>Colour Detection</p> <p>Filter the cropped image by selecting appropriate HSV values so that the pink line can be isolated from the rest of the image.</p> <p>You may need to use the <code>tuos_examples\\image_colours.py</code> node again to help you identify the correct Hue and Saturation value range.</p> <p>Use <code>cv2.cvtColor()</code> to convert your <code>cropped_img</code> into an HSV colour representation:</p> <pre><code>hsv_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2HSV)\n</code></pre> <p>Use <code>cv2.inRange()</code> to create a mask with the HSV value range that you have determined:</p> <pre><code>line_mask = cv2.inRange(\n    hsv_img, {lower_hsv_values}, {upper_hsv_values}\n)\n</code></pre> <p>And then use <code>cv2.bitwise_and()</code> to create a new image with the mask applied, so that the coloured line is isolated:</p> <pre><code>line_isolated = cv2.bitwise_and(\n    cropped_img, cropped_img, mask = line_mask\n)\n</code></pre> <p></p> <p></p> </li> <li> <p>Locating the line</p> <p>Finally, find the horizontal position of the line in the robot's viewpoint.</p> <p>Calculate the image moments of the pink colour blob that represents the line (<code>line_mask</code>) using the <code>cv2.moments()</code> method. Remember that it's the \\(c_{y}\\) component that we're interested in here:</p> \\[ c_{y}=\\dfrac{M_{10}}{M_{00}} \\] <p>Ultimately, this will provide us with the feedback signal that we can use for a proportional controller that we will implement in the next part of the exercise.</p> <p>Once you've obtained the image moments (and <code>cy</code>), use <code>cv2.circle()</code> to mark the centroid of the line on the filtered image (<code>line_isolated</code>) with a circle. For this, you'll also need to calculate the \\(c_{z}\\) component of the centroid:</p> \\[ c_{z}=\\dfrac{M_{01}}{M_{00}} \\] <p>Remember that once you've done all this you can display the filtered image of the isolated line (with the circle to denote the centroid location) using <code>cv2.imshow()</code> again:</p> <pre><code>cv2.imshow(\"filtered line\", line_isolated)\n</code></pre> <p></p> <p></p> </li> </ol>"},{"location":"course/part6/object_detection/","title":"Part 6 Object Detection Node","text":""},{"location":"course/part6/object_detection/#the-code","title":"The Code","text":"<p>Copy all the code below into your <code>object_detection.py</code> file, and make sure you read the annotations!</p> object_detection.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy \nfrom rclpy.node import Node # (1)!\n\nimport cv2\nfrom cv_bridge import CvBridge, CvBridgeError # (2)!\n\nfrom sensor_msgs.msg import Image # (3)!\n\nfrom pathlib import Path # (4)!\n\nclass ObjectDetection(Node):\n\n    def __init__(self): # (5)!\n        super().__init__(\"object_detection\")\n\n        self.camera_sub = self.create_subscription(\n            msg_type=Image,\n            topic=\"/camera/image_raw\",\n            callback=self.camera_callback,\n            qos_profile=10\n        )\n\n        self.waiting_for_image = True # (6)!\n\n    def camera_callback(self, img_data): # (7)!\n        cvbridge_interface = CvBridge() # (8)!\n        try:\n            cv_img = cvbridge_interface.imgmsg_to_cv2(\n                img_data, desired_encoding=\"bgr8\"\n            ) # (9)!\n        except CvBridgeError as e:\n            self.get_logger().warning(f\"{e}\")\n\n        if self.waiting_for_image: # (10)!\n            height, width, channels = cv_img.shape\n\n            self.get_logger().info(\n                f\"Obtained an image of height {height}px and width {width}px.\"\n            )\n\n            self.show_image(img=cv_img, img_name=\"step1_original\")\n\n            self.waiting_for_image = False # (15)!\n            cv2.destroyAllWindows() # (16)!\n\n    def show_image(self, img, img_name, save_img=True): # (11)!\n\n        self.get_logger().info(f\"Opening {img_name}...\")\n        cv2.imshow(img_name, img) # (12)!\n\n        cv2.waitKey(4000) # (14)!\n\n        if save_img: # (13)!\n            self.save_image(img, img_name)\n\n    def save_image(self, img, img_name): # (17)!\n        self.get_logger().info(f\"Saving the image...\")\n\n        base_image_path = Path.home().joinpath(\"object_detection\")\n        base_image_path.mkdir(parents=True, exist_ok=True) # (18)!\n        full_image_path = base_image_path.joinpath(\n            f\"{img_name}.jpg\") # (19)!\n\n        cv2.imwrite(str(full_image_path), img) # (20)!\n\n        self.get_logger().info(\n            f\"\\nSaved an image to '{full_image_path}'\\n\"\n            f\"  - image dims: {img.shape[0]}x{img.shape[1]}px\\n\"\n            f\"  - file size: {full_image_path.stat().st_size} bytes\"\n        ) # (21)!\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ObjectDetection()\n    while node.waiting_for_image:\n        rclpy.spin_once(node) # (22)!\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>Nothing new here, moving on...</p> </li> <li> <p>We're importing the OpenCV library for Python (remember the Python API that we talked about earlier), which is called <code>cv2</code>, and also that ROS-to-OpenCV bridge interface that we talked about earlier too: <code>cv_bridge</code>.</p> <p>From <code>cv_bridge</code> we're importing the <code>CvBridge</code> and <code>CvBridgeError</code> classes from the <code>cv_bridge</code> library specifically.</p> </li> <li> <p>We need to subscribe to an image topic in order to obtain the data being published to it. You should've already identified the type of interface that is published to the <code>/camera/image_raw</code> topic, so we import that interface type here (from the <code>sensor_msgs</code> package) so that we can build a subscriber to the topic later.</p> </li> <li> <p>We're also importing the Python <code>Path</code> class from the <code>pathlib</code> module. A very handy tool for doing file operations.</p> </li> <li> <p>Initialising our <code>ObjectDetection()</code> Class (should be very familiar to you by now):</p> <ol> <li>Giving our node a name.</li> <li>Creating a subscriber to the <code>/camera/image_raw</code> topic, providing the interface type used by the topic (<code>sensor_msgs/msg/Image</code> - as imported above), and pointing it to a callback function (<code>camera_callback</code>, in this case), to define the processes that should be performed every time a message is obtained on this topic (in this case, the messages will be our camera images)</li> </ol> </li> <li> <p>We're creating a flag to indicate whether the node has obtained an image yet or not. For this exercise, we only want to obtain a single image, so we will set the <code>waiting_for_image</code> flag to <code>False</code> once an image has been obtained and processed, to avoid capturing any more. </p> <p>This flag will then be used to shut down the node when it's done its job.</p> </li> <li> <p>Here, we're defining a callback function for our <code>self.camera_sub</code> subscriber...</p> </li> <li> <p>Here, we create an instance of the <code>CvBridge</code> class that we imported earlier, and which we'll use later on to convert ROS image data into a format that OpenCV can understand.</p> </li> <li> <p>We're using the CvBridge interface to take our ROS image data and convert it to a format that OpenCV will be able to understand.  In this case we are specifying conversion (or \"encoding\") to an 8-bit BGR (Blue-Green-Red) image format: <code>\"bgr8\"</code>.</p> <p>We contain this within a <code>try</code>-<code>except</code> block though, which is the recommended procedure when doing this.  Here we try to convert an image using the desired encoding, and if a <code>CvBridgeError</code> is raised then we print a warning message to the terminal.  Should this happen, this particular execution of the camera callback function will stop.</p> </li> <li> <p>Then we check the <code>waiting_for_image</code> flag to see if this is the first image that has been received by the node.  If so, then:</p> <ol> <li>Obtain the height and width of the image (in pixels), as well as the number of colour channels.</li> <li>Print a log message containing the image dimensions.</li> <li>Pass the image data to the <code>show_image()</code> function (defined below). We also pass a descriptive name for the image to this function too (<code>img_name</code>).</li> </ol> </li> <li> <p>This class method presents the image to us in a pop-up window and also calls another method which saves the image to file for us.    </p> </li> <li> <p>Display the actual image in a pop-up window:</p> <ol> <li>The image data is passed into the function via the <code>img</code> argument,</li> <li>We need to give the pop-up window a name, so in this case we are using the <code>img_name</code> argument that is passed into this class method.</li> </ol> </li> <li> <p>The <code>show_image()</code> class method has a <code>save_img</code> argument, which is set to <code>True</code> by default, so that this <code>if</code> condition is triggered, and another class method is called to save the image to file.    </p> </li> <li> <p>We're supplying a value of <code>4000</code> here, which tells this function to keep this window open for 4000 milliseconds (4 seconds) before closing it again.  </p> <p>Once the window has closed, the execution of our code is able to continue...    </p> </li> <li> <p>We then set the <code>waiting_for_image</code> flag to <code>False</code> so that we only ever perform these processing steps once (we only want to capture a single image).  This will then trigger the main <code>while</code> loop to stop (see below), thus causing the overall execution of the node to stop too.</p> </li> <li> <p><code>cv2.destroyAllWindows()</code> ensures that any OpenCV image pop-up windows that may still be active or in memory are destroyed before the class method exits (and the node shuts down).     </p> </li> <li> <p>This class method handles the saving of the image to a file using <code>cv2</code> tools and <code>pathlib</code>.</p> </li> <li> <p>Here, we define a filesystem location to save images to. </p> <p>We want this to exist in a folder called \"<code>object_detection</code>\" in the home directory, so we can use Pathlib's <code>Path.home().joinpath(...)</code> to define it (a handy way to access the User's home directory, without needing to know the Users name).</p> <p>Then, we use the Pathlib <code>Path.mkdir()</code> method to create this directory if it doesn't exist already.    </p> </li> <li> <p>A full file path is constructed for the image here (using the <code>Path.joinpath()</code> method), based on:</p> <ol> <li>The <code>base_image_path</code> that we defined above </li> <li>An image name that is passed into this class method via the <code>img_name</code> argument.</li> </ol> </li> <li> <p>This saves the image to a <code>.jpg</code> file.  We're supplying the <code>full_image_path</code> that was created above, and also the actual image data (<code>self.cv_img</code>) so that the function knows what image we want to save.</p> </li> <li> <p>We're printing a log message to the terminal to inform us of:</p> <ol> <li>Where the image has been saved to</li> <li>How big the image is (in terms of its pixel dimensions)</li> <li>How big the image file is (in bytes).</li> </ol> </li> <li> <p>We're using <code>spin_once()</code> inside a <code>while</code> loop here so that we can keep an eye on the value of the <code>wait_for_image</code> flag, and stop spinning (i.e. break out of the <code>while</code> loop) once it turns <code>False</code>.</p> </li> </ol>"},{"location":"course/part6/object_detection/#dependencies","title":"Dependencies","text":"<p>Modify your <code>package.xml</code> file to accommodate the various dependencies of the <code>object_detection.py</code> node:</p> package.xml<pre><code>&lt;exec_depend&gt;opencv2&lt;/exec_depend&gt;\n&lt;exec_depend&gt;cv_bridge&lt;/exec_depend&gt;\n&lt;exec_depend&gt;sensor_msgs&lt;/exec_depend&gt;\n</code></pre>"},{"location":"course/part6/object_detection_complete/","title":"Part 6 Object Detection Node (Complete)","text":""},{"location":"course/part6/object_detection_complete/#part-6-object-detection-node-complete","title":"Part 6 Object Detection Node (Complete)","text":"<p>Here's a full example of the <code>object_detection.py</code> node that you should have developed during Part 6 Exercise 2.  Also included here is an illustration of how to use the <code>cv2.circle()</code> method to create a marker on an image illustrating the centroid of the detected feature, as discussed here.</p> object_detection_complete.py<pre><code>#!/usr/bin/env python3\n\nimport rclpy \nfrom rclpy.node import Node \n\nimport cv2\nfrom cv_bridge import CvBridge, CvBridgeError \n\nfrom sensor_msgs.msg import Image \n\nfrom pathlib import Path \n\nclass ObjectDetection(Node):\n\n    def __init__(self): \n        super().__init__(\"object_detection\")\n\n        self.camera_sub = self.create_subscription(\n            msg_type=Image,\n            topic=\"/camera/image_raw\",\n            callback=self.camera_callback,\n            qos_profile=10\n        )\n\n        self.waiting_for_image = True \n\n    def camera_callback(self, img_data): \n        cvbridge_interface = CvBridge() \n        try:\n            cv_img = cvbridge_interface.imgmsg_to_cv2(\n                img_data, desired_encoding=\"bgr8\"\n            ) \n        except CvBridgeError as e:\n            self.get_logger().warning(f\"{e}\")\n\n        if self.waiting_for_image: \n            height, width, channels = cv_img.shape\n\n            self.get_logger().info(\n                f\"Obtained an image of height {height}px and width {width}px.\"\n            )\n\n            self.show_image(img=cv_img, img_name=\"step1_original\")\n\n            crop_width = width - 400\n            crop_height = 400\n            crop_y0 = int((width / 2) - (crop_width / 2))\n            crop_z0 = int((height / 2) - (crop_height / 2))\n            cropped_img = cv_img[\n                crop_z0:crop_z0+crop_height, \n                crop_y0:crop_y0+crop_width\n            ]\n\n            self.show_image(img=cropped_img, img_name=\"step2_cropping\")\n\n            hsv_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2HSV)\n            lower_threshold = (115, 225, 100)\n            upper_threshold = (130, 255, 255)\n            img_mask = cv2.inRange(hsv_img, lower_threshold, upper_threshold)\n\n            self.show_image(img=img_mask, img_name=\"step3_image_mask\")\n\n            filtered_img = cv2.bitwise_and(cropped_img, cropped_img, mask = img_mask)\n\n            # Finding the Image Centroid: (1) \n            m = cv2.moments(img_mask) # (2)!\n            cy = m['m10'] / (m['m00'] + 1e-5)\n            cz = m['m01'] / (m['m00'] + 1e-5) # (3)! \n            cv2.circle(\n                filtered_img,\n                (int(cy), int(cz)),\n                10, (0, 0, 255), 2\n            ) # (4)!\n\n            self.show_image(img=filtered_img, img_name=\"step4_filtered_image\")\n\n            self.waiting_for_image = False \n            cv2.destroyAllWindows() \n\n    def show_image(self, img, img_name, save_img=True): \n\n        self.get_logger().info(f\"Opening {img_name}...\")\n        cv2.imshow(img_name, img) \n\n        cv2.waitKey(4000) \n\n        if save_img: \n            self.save_image(img, img_name)\n\n    def save_image(self, img, img_name): \n        self.get_logger().info(f\"Saving the image...\")\n\n        base_image_path = Path.home().joinpath(\"object_detection\")\n        base_image_path.mkdir(parents=True, exist_ok=True) \n        full_image_path = base_image_path.joinpath(\n            f\"{img_name}.jpg\") \n\n        cv2.imwrite(str(full_image_path), img) \n\n        self.get_logger().info(\n            f\"\\nSaved an image to '{full_image_path}'\\n\"\n            f\"  - image dims: {img.shape[0]}x{img.shape[1]}px\\n\"\n            f\"  - file size: {full_image_path.stat().st_size} bytes\"\n        ) \n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ObjectDetection()\n    while node.waiting_for_image:\n        rclpy.spin_once(node) \n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <ol> <li> <p>Everything here should be familiar to you from earlier in this exercise, except for this section...</p> </li> <li> <p>Here, we obtain the moments of our colour blob by providing the boolean representation of it (i.e. the <code>img_mask</code>) to the <code>cv2.moments()</code> function.</p> </li> <li> <p>Then, we are determining where the central point of this colour blob is located by calculating the <code>cy</code> and <code>cz</code> coordinates of it.  This provides us with pixel coordinates relative to the top left-hand corner of the image.</p> </li> <li> <p>Finally, this function allows us to draw a circle on our image at the centroid location so that we can visualise it.  Into this function we pass:</p> <ol> <li>The image that we want the circle to be drawn on.  In this case: <code>filtered_img</code>.</li> <li>The location that we want the circle to be placed, specifying the horizontal and vertical pixel coordinates respectively: <code>(int(cy), int(cz))</code>.</li> <li>How big we want the circle to be: here we specify a radius of 10 pixels.</li> <li>The colour of the circle, specifying this using a Blue-Green-Red colour space: <code>(0, 0, 255)</code> (i.e.: pure red in this case)</li> <li>Finally, the thickness of the line that will be used to draw the circle, in pixels.</li> </ol> </li> </ol>"},{"location":"software/","title":"Accessing or Installing ROS 2","text":"<p>In order to engage with the ROS 2 Course you'll need access to a ROS environment. </p> <p>ROS can be a bit tricky to install, and is primarily supported on only a handful of \"Tier 1\" operating systems (as listed here), the first (and best) choice being Ubuntu Linux. In order to install or access ROS 2 for this course, we recommend one of the following options. Click on the relevant link below to access further details:</p>"},{"location":"software/#wsl-ros2-windows","title":"WSL-ROS2 (Windows)","text":"<p>University of Sheffield Staff/Students Only</p> <p>We've created our own custom ROS 2 (Jazzy) and Ubuntu (24.04) environment for WSL specifically for this course, which we call \"WSL-ROS2\". The environment contains all the tools and ROS packages that you will need for the course exercises and assignments. </p> <p>This is our recommended option, and there are two ways to access this: </p> <ol> <li>Install it on your own Windows 10 or 11 machine via the IT Services Software Download Service.</li> <li>Access it on a range of managed desktop computers across the University campus.</li> </ol>"},{"location":"software/#mac-linux","title":"Mac &amp; Linux","text":"<p>For Mac users, the best option for installing ROS 2 on your machine is by using Docker, but please note that performance is not optimal. </p> <p>The Docker option works very well on Linux.</p> <ul> <li>Installing ROS 2 with Docker.</li> </ul>"},{"location":"software/docker-ros2/","title":"Installing ROS 2 Using Docker (Linux & Mac)","text":"<p>Applicable to: Linux and Mac Users</p>"},{"location":"software/docker-ros2/#acknowledgments","title":"Acknowledgments","text":"<p>This solution has been put together by Atri Hegde, a COM2009 student from 2024 (and a course demonstrator in 2025!)</p> <p>Thanks Atri!!</p>"},{"location":"software/docker-ros2/#setup","title":"Setup","text":"<p>See here for the <code>docker-ros2</code> repo and the instructions on how to install it: </p> <p></p>https://github.com/hegde-atri/ros2-docker<p></p>"},{"location":"software/docker-ros2/#launch","title":"Launching the ROS Environment","text":""},{"location":"software/docker-ros2/#mac-linux","title":"Mac &amp; Linux","text":"<p>Instructions on how to launch the ROS2 docker container are provided in the README, so please consult this for all the details. Essentially though (if you're on a Mac or Linux machine) then you first need to fire up the docker container using:</p> <pre><code>ros_start\n</code></pre> <p>... and once that's done, you need to run the following command to enter the ROS2 environment:</p> <pre><code>ros_shell\n</code></pre> <p>The above assumes that you have already set up your shell appropriately (again see the README).</p>"},{"location":"software/installing-wsl-ros2/","title":"Installing WSL-ROS2 on Windows","text":"<p>Applicable to: Windows 10 or 11 personal (unmanaged) computers</p>"},{"location":"software/installing-wsl-ros2/#the-wsl-ros2-simulation-environment","title":"The WSL-ROS2 Simulation Environment","text":"<p>To support this course we've created a custom ROS 2 environment which runs on Windows 10 or 11 using the Windows Subsystem for Linux (WSL). We call this \"WSL-ROS2\" and you can download it via the University of Sheffield Software Download Service (University login required).</p> <p>Note</p> <p>When you download WSL-ROS2 from the Software Download Service you will receive an email with installation instructions. We recommend that you follow the instructions provided on this page instead, as this page will be kept more up-to-date throughout the semester.</p>"},{"location":"software/installing-wsl-ros2/#prerequisites","title":"Prerequisites","text":"<ol> <li>Your computer must be running Windows 10 Build 19044 or higher, or Windows 11.</li> <li>Update the GPU drivers for your machine.</li> <li>Install or update WSL:<ol> <li>If you don't already have WSL installed on your machine then follow these instructions to install it.</li> <li>If you do already have WSL installed on your machine, then follow these instructions to update it.</li> </ol> </li> <li>Install the Windows Terminal.</li> <li>Install Visual Studio Code and the WSL VS Code extension.</li> <li>Install the VcXsrv Windows X Server.</li> </ol>"},{"location":"software/installing-wsl-ros2/#installing","title":"Installing","text":"<ol> <li>Go to the IT Services Software Downloads page (you'll need to log in with your university MUSE credentials).</li> <li> <p>Scroll down to the bottom, where you should see WSL-ROS listed.</p> <p>Click on the blue \"Request WSL-ROS\" button and then wait to receive an email to your university email address.</p> </li> <li> <p>The email will contain a link to download WSL-ROS2 to your machine as a <code>.zip</code> file (~2 GB).</p> </li> <li> <p>On your computer, create a new folder in the root of your <code>C:\\</code> drive called <code>WSL-ROS2</code>.</p> </li> <li>Extract the content of the downloaded <code>.zip</code> file into to the <code>C:\\WSL-ROS2\\</code> folder.</li> <li> <p>Launch PowerShell and enter the follow command to install WSL-ROS2 as a WSL distro on your machine:</p> <pre><code>wsl --import WSL-ROS2 $env:localappdata\\WSL-ROS2 `\nC:\\WSL-ROS2\\wsl-ros2-v2526.01.tar --version 2\n</code></pre> </li> <li> <p>This may take a couple of minutes. Once it's done, you can verify that it was successful with the following command:</p> <pre><code>wsl -l -v\n</code></pre> <p>Where <code>WSL-ROS2</code> should be listed.</p> </li> <li> <p>Next (optional), open up the Windows Terminal App, then:</p> <ol> <li>Go to Settings (Ctrl+,)</li> <li>Under <code>Profiles</code> in the left-hand menu, find <code>WSL-ROS2</code> (or scroll further down and click the <code>Add a new profile</code> button to create it).</li> <li> <p>Configure the settings for the <code>WSL-ROS2</code> profile as shown below:</p> <p></p> <p></p> </li> <li> <p>Then, in <code>Startup</code> (back in the left-hand menu again), under <code>Default profile</code> select WSL-ROS2 from the drop-down list.</p> <p>This will ensure that each time you open the Windows Terminal App or you press the New Tab () button a WSL-ROS2 Terminal Instance will be launched by default.</p> </li> </ol> </li> </ol>"},{"location":"software/installing-wsl-ros2/#using-a-dedicated-x-server","title":"Using a Dedicated X Server","text":"<p>WSL-ROS2 requires an XServer in order to render graphical applications (such as Gazebo and RViz)<sup>1</sup>. You need to make sure you have one up and running whenever you're working with WSL-ROS2.</p> <ol> <li>Having installed VcXsrv in the Prerequisites Section...</li> <li> <p>Download this config file for VcXsrv and save it to your desktop as <code>wsl_ros_config.xlaunch</code>.</p> <p></p> <p></p> </li> <li> <p>Double click this to launch VcXsrv with the appropriate settings. An icon should then appear in your notification tray (bottom-right) to indicate that the X Server is running:</p> <p></p> <p></p> </li> <li> <p>Launch the WSL-ROS2 environment by launching the Windows Terminal App:</p> <p></p> <p></p> </li> <li> <p>In the WSL-ROS2 terminal instance, try running the empty world Gazebo simulation:</p> <pre><code>ros2 launch turtlebot3_gazebo empty_world.launch.py\n</code></pre> <p>This should hopefully present you with something like this:</p> <p></p> <p></p> <p>Important</p> <p>You need to make sure you have the X Server running (by clicking the <code>wsl_ros_config.xlaunch</code> shortcut) every time you work with WSL-ROS2. </p> </li> </ol>"},{"location":"software/installing-wsl-ros2/#see-also","title":"See Also","text":"<ul> <li>Setting up VS Code for WSL</li> <li>A Quick Introduction to the Linux Terminal</li> </ul> <ol> <li> <p>WSL2 does support GUI applications natively, however we are finding that the latest version of Gazebo (Ignition) doesn't seem to play nicely with this, and performance is generally pretty poor. Using a dedicated XServer seems to offer slightly better performance.\u00a0\u21a9</p> </li> </ol>"},{"location":"software/using-wsl-ros/","title":"Accessing WSL-ROS2 on University Computers","text":"<p>Applicable to: Students who do not have a Windows 10 or 11 personal computer.</p>"},{"location":"software/using-wsl-ros/#the-wsl-ros2-simulation-environment","title":"The WSL-ROS2 Simulation Environment","text":"<p>To support this course we've created a custom ROS 2 environment which runs on Windows 10 or 11 using the Windows Subsystem for Linux (WSL). We call this \"WSL-ROS2\" and it is available on a range of managed desktop computers across the university campus. </p>"},{"location":"software/using-wsl-ros/#diamond","title":"In the Diamond","text":"<p>WSL-ROS2 is pre-installed on all machines in the following Computer Rooms in the Diamond:</p> <ul> <li>Computer Rooms 1, 2, 4 &amp; 6</li> <li>Short Term Loan Laptops in Computer Room 5</li> <li>40 dedicated WSL-ROS laptops (made available in Computer Room 5 during the COM2009 lab sessions)</li> </ul>"},{"location":"software/using-wsl-ros/#other","title":"Other Campus Computer Rooms","text":"<p>In addition to this, WSL-ROS2 can also be installed on all machines in the following additional Campus Computer Rooms (via Software Center):</p> <ul> <li>The Information Commons:<ul> <li>Classrooms 3 and 4</li> <li>Computers on Floors 1, 2, 3, 4 (including Northlights and Pavillion), and Floor 6</li> </ul> </li> <li>Sir Frederick Mappin Building:<ul> <li>Room F110</li> <li>Heartspace Room E095</li> </ul> </li> </ul>"},{"location":"software/using-wsl-ros/#see-also","title":"See Also","text":"<ul> <li>Getting Started with WSL-ROS2 on the University Managed Desktops </li> <li>A Quick Introduction to the Linux Terminal</li> <li>Setting up Visual Studio Code (VS Code)</li> </ul>"},{"location":"software/using-wsl-ros/linux-term/","title":"A Quick Introduction to the Linux Terminal","text":"<p>You'll work extensively with the Linux Terminal throughout this course. An idle WSL-ROS2 terminal instance will look like this:</p> <p></p> <p>Here, the presence of the <code>$</code> symbol indicates that the terminal is ready to accept a command. Text before the <code>$</code> symbol has two parts separated by the <code>:</code> symbol:</p> <ul> <li> <p>Text to the left of the <code>:</code> tells us the name of the Linux user (\"student\" in this case) followed by the WSL-ROS2 version that you are working with.</p> <p>Note</p> <p>The current WSL-ROS2 version is <code>2526</code>.</p> </li> <li> <p>Text to the right of the <code>:</code> tells us where in the Linux Filesystem we are currently located (<code>~</code> means \"The Home Directory\", which is an alias for the path: <code>/home/student/</code>).</p> </li> </ul> <p>If you don't see the <code>$</code> symbol at all, then this means that a process is currently running. To stop any running process enter Ctrl+C simultaneously on your keyboard.</p>"},{"location":"software/using-wsl-ros/man-win/","title":"Getting Started with WSL-ROS2 on the University Managed Desktops","text":""},{"location":"software/using-wsl-ros/man-win/#prerequisites","title":"Prerequisites","text":"<ul> <li>Accessing WSL-ROS2 on a University Managed Desktop Computer</li> </ul>"},{"location":"software/using-wsl-ros/man-win/#launching-wsl-ros2","title":"Launching WSL-ROS2","text":"<ol> <li> <p>Click the Windows Start Menu button: </p> </li> <li> <p>Then, start typing <code>wsl-ros</code> and click on the application shortcut that should appear in the list:</p> <p></p> <p></p> <p>Make sure you select WSL-ROS2! </p> <p>You'll then be presented with the following screen:</p> <p></p> <p></p> <p>WSL-ROS2 is now being installed, which may take a couple of minutes to complete.  Once it's done, the Windows Terminal should automatically launch:</p> <p></p> <p></p> </li> </ol> <p>This is a WSL-ROS2 Terminal Instance!</p>"},{"location":"software/using-wsl-ros/man-win/#backing-up-and-restoring-your-data","title":"Backing-Up (and Restoring) your Data","text":"<p>If you're working with WSL-ROS on a university managed desktop machine, the WSL-ROS Environment will only be preserved for a limited time on the machine that you installed it on. As such, any work that you do within WSL-ROS will not be preserved between sessions or across different machines automatically! It's therefore really important that you run a backup script before you close WSL-ROS down. To do so is very easy, simply run the command below from any WSL-ROS Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will create an archive of your Home Directory (more detail here) and save it to your University <code>U:</code> Drive. Whenever you launch a fresh WSL-ROS2 Environment again on another day, or on a different machine, simply run the following command to restore your work to it:</p> <pre><code>wsl_ros restore\n</code></pre> <p>To make things a little easier, on launching WSL-ROS the system will check to see if a backup file already exists from a previous session. If it does, then you will be asked if you want to restore it straight away:</p> <pre><code>It looks like you already have a backup from a previous session:\n  U:\\wsl-ros\\ros2-backup-XXX.tar.gz\nDo you want to restore this now? [y/n]\n</code></pre> <p>Enter Y+Enter to restore your data from this backup file, or N+Enter to leave the backup file alone and work from fresh (none of your previous work will be restored). </p>"},{"location":"software/using-wsl-ros/man-win/#re-launching-the-environment","title":"Re-Launching the Environment","text":"<p>As discussed above, the WSL-ROS2 environment is not preserved on the university managed desktops indefinitely. If you log back in to the same machine within a few hours however, then it may still be there, and you'll be presented with the following message when you launch it:</p> <p></p> <p>Enter Y+Enter to continue where you left things previously, or N+Enter to start from a fresh installation.</p> <p>Warning</p> <p>If you select N then any work that you have created in the existing environment will be deleted! Always make sure you back up your work using the procedure outlined above!</p>"},{"location":"software/using-wsl-ros/vscode/","title":"VS Code and WSL","text":""},{"location":"software/using-wsl-ros/vscode/#the-top","title":"Launching VS Code from the Terminal","text":"<ol> <li> <p>You can launch VS Code directly from a WSL-ROS2 terminal instance. Simply type <code>code .</code> at the terminal prompt and then hit Enter :</p> <pre><code>code .\n</code></pre> </li> <li> <p>A warning message may then pop up:</p> <p></p> <p></p> <p>Check the box to \"Permanently allow ...\" and then click the <code>Allow</code> button.</p> </li> <li> <p>VS Code should then launch, and you'll be presented with another trust dialogue:</p> <p></p> <p></p> <p>Click the blue <code>Yes, I trust the authors</code> button.</p> </li> </ol>"},{"location":"software/using-wsl-ros/vscode/#wsl-ext","title":"Installing the WSL Extension","text":"<ol> <li> <p>The first time you launch VS Code (as above) you should be presented with a pop-up in the bottom-right of the screen, asking if you would like to \"install the recommended 'WSL' extension from Microsoft...\"</p> <p></p> <p></p> <p>Click on the blue \"Install\" button.</p> Don't see the pop-up? <p>You can also install the 'WSL' extension manually.</p> <p>Click on the \"Extensions\" icon in the left-hand toolbar (or hit Ctrl+Shift+X ), type \"wsl\" in the search box and hit the install button on the right extension, as show below:</p> <p></p> <p></p> </li> <li> <p>Once installed, close down VS Code, go back to the WSL-ROS2 terminal instance and re-launch it using the <code>code .</code> command again.</p> <p>This time, you'll be presented with yet another trust pop-up dialogue. Once again, check the box to \"Trust the authors\" and then click the blue <code>Yes, I trust the authors</code> button. </p> </li> <li> <p>You can now navigate the WSL-ROS2 filesystem in the explorer window on the left-hand side of the VS Code screen. You'll need to use this to locate the packages and scripts that you create throughout this course!</p> <p></p> <p></p> </li> </ol>"},{"location":"software/using-wsl-ros/vscode/#verify","title":"Always make sure that the \"WSL\" extension is enabled!!","text":"<p>Check that your blue \"Remote Window\" icon in the bottom-left of the VS Code screen always looks like this:</p> <p></p> <p>If not, then go back to the top of this page and try again!</p>"},{"location":"waffles/","title":"Working with our Real TurtleBot3 Waffles","text":""},{"location":"waffles/#working-with-our-real-turtlebot3-waffles","title":"Working with our Real TurtleBot3 Waffles","text":"<p>This section of the Course Site contains a range of resources to support your work with our real TurtleBot3 Waffles in the Diamond. </p> <p></p>"},{"location":"waffles/basics/","title":"Waffle (& ROS) Basics","text":""},{"location":"waffles/basics/#waffle-ros-basics","title":"Waffle (&amp; ROS) Basics","text":"<p>Having completed the steps on the previous page, your robot and laptop should now be paired, and ROS should be up and running (on the robot). Now, you're ready to bring the robot to life! </p> <p>On this page are a series of exercises for you to work through, to see how the robots work. We'll also talk through some core ROS concepts and use some key ROS tools, in case you haven't had a chance to explore these in simulation yet.</p>"},{"location":"waffles/basics/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Making the Robot Move</li> <li>Exercise 2: Seeing the Sensors in Action</li> <li>Exercise 3: Visualising the ROS Network</li> <li>Exercise 4: Exploring ROS Topics and Interfaces</li> <li>Exercise 5: A Python Velocity Control Node</li> <li>Exercise 6: Using SLAM to create a map of the environment</li> </ul>"},{"location":"waffles/basics/#manual-control","title":"Manual Control","text":""},{"location":"waffles/basics/#exMove","title":"Exercise 1: Making the Robot Move","text":"<p>There's a very useful ready-made ROS application called <code>teleop_keyboard</code> (from the <code>turtlebot3_teleop</code> package) that we will use to drive a Waffle around. This node works in exactly the same way in both simulation and in the real-world!</p> <ol> <li> <p>You should already have two terminal instances active:</p> <ul> <li>TERMINAL 1: The robot terminal with the \"bringup\" processes running (Launching ROS, Step 3)</li> <li>TERMINAL 2: The laptop terminal with the <code>rmw_zenohd</code> node running (Launching ROS, Step 4)</li> </ul> </li> <li> <p>Open up a new terminal instance on the laptop either by using the Ctrl+Alt+T keyboard shortcut, or by clicking the Terminal App icon, we'll refer to this as TERMINAL 3. In this terminal enter the following <code>ros2 run</code> command to fire up the <code>teleop_keyboard</code> node:</p> <pre><code>ros2 run turtlebot3_teleop teleop_keyboard\n</code></pre> </li> <li> <p>Follow the instructions provided in the terminal to drive the robot around using specific buttons on the keyboard:</p> <p></p> <p></p> <p>Warning</p> <p>Take care to avoid any obstacles or other people in the lab as you do this!</p> </li> <li> <p>Once you've spent a bit of time on this, close the application down by entering Ctrl+C in TERMINAL 3.</p> </li> </ol>"},{"location":"waffles/basics/#packages-and-nodes","title":"Packages and Nodes","text":"<p>ROS applications are organised into packages. Packages are basically folders containing scripts, configurations and launch files (ways to launch those scripts and configurations).  </p> <p>Scripts tell the robot what to do and how to act. In ROS, these scripts are called nodes. ROS Nodes are executable programs that perform specific robot tasks and operations. These are typically written in C++ or Python, but it's possible to write ROS Nodes using other programming languages too.</p> <p>There are two key ways to launch ROS applications:</p> <ol> <li><code>ros2 launch</code></li> <li><code>ros2 run</code></li> </ol> <p>Recall that we just used the <code>ros2 run</code> command in Exercise 1 to launch the <code>teleop_keyboard</code> node. This command has the following structure:</p> <pre><code>ros2 run {[1] Package name} {[2] Node name}\n</code></pre> <p>Part [1] specifies the name of the ROS package containing the functionality that we want to execute. Part [2] is used to specify a single script within that package that we want to execute. We therefore use <code>ros2 run</code> commands to launch single executables (aka Nodes) onto the ROS network (in Exercise 1 for example, we launched the <code>teleop_keyboard</code> node).</p> <p>The <code>ros2 launch</code> command has a similar structure:</p> <pre><code>ros2 launch {[1] Package name} {[2] Launch file}\n</code></pre> <p>Here, Part [1] is the same as the <code>ros2 run</code> command, but Part [2] is slightly different: <code>{[2] Launch file}</code>. In this case, Part [2] is a file within that package that specifies any number of Nodes that we want to launch onto the ROS network. We can therefore launch multiple nodes at the same time from a single launch file.</p>"},{"location":"waffles/basics/#sensors-visualisation-tools","title":"Sensors &amp; Visualisation Tools","text":"<p>Our Waffles have some pretty sophisticated sensors on them, allowing them to \"see\" the world around them. Let's now see what our robot sees, using some handy ROS tools.</p>"},{"location":"waffles/basics/#exViz","title":"Exercise 2: Seeing the Sensors in Action","text":"<ol> <li> <p>There shouldn't be anything running in TERMINAL 3 now, after you closed down the <code>teleop_keyboard</code> node (using Ctrl+C) at the end of the previous exercise. Return to this terminal and enter the following command:</p> <pre><code>ros2 launch tuos_tb3_tools rviz.launch.py\n</code></pre> <p>This will launch an application called RViz, which is a handy tool that allows us to visualise the data from all the sensors on-board our robots. When RViz opens, you should see something similar to the following:</p> <p></p> <p></p> <p>Click on the checkbox next to \"Camera\" in the \"Displays\" list to enable camera subscription. A live feed from the robot's camera should then be displayed in the Camera panel in the bottom left.</p> </li> <li> <p>In the main RViz panel you should see a digital model of the robot, surrounded by lots of green dots. This is a representation of the laser displacement data coming from the LiDAR sensor (the black device on the top of the robot). The LiDAR sensor spins continuously, sending out laser pulses into the environment as it does so. When a pulse hits an object it is reflected back to the sensor, and the time it takes for this to happen is used to calculate how far away the object is.</p> <p>The LiDAR sensor spins and performs this process continuously, so a full 360\u00b0 scan of the environment can be generated. This data is therefore really useful for things like obstacle avoidance and mapping.</p> </li> <li> <p>Place your hand in front of the robot and see if the position of the green dots changes to match your hand's location. Move your hand up and down and consider at what height the LiDAR sensor is able to detect it.</p> </li> <li> <p>Then, move your hand closer and further away and watch how the green dots move to match this. </p> </li> <li> <p>Open up a new terminal instance (TERMINAL 4) and launch the <code>teleop_keyboard</code> node as you did in Exercise 1. Watch how the data in the RViz screen changes as you drive the robot around a bit.</p> </li> </ol>"},{"location":"waffles/basics/#exNet","title":"Exercise 3: Visualising the ROS Network","text":"<p>Using <code>ros2 run</code> and <code>ros2 launch</code>, as we have done so far, it's easy to end up with a lot of different processes or ROS Nodes running on the network, some of which we will interact with, but others may just be running in the background. It is often useful to know exactly what is running on the ROS network, and there are a few ways to do this.</p> <ol> <li> <p>Open up a new terminal instance now (TERMINAL 5) and from here use the <code>ros2 node</code> command to list the nodes that are currently running:</p> <pre><code>ros2 node list\n</code></pre> <p>At a minimum, you should be presented with the following list: </p> <pre><code>/camera\n/diff_drive_controller\n/hlds_laser_publisher\n/robot_state_publisher\n/tb3_status_node\n/turtlebot3_node\n</code></pre> </li> <li> <p>We can visualise the connections between the active nodes by using a ROS node called <code>rqt_graph</code>. In the same terminal, launch this as follows:</p> <pre><code>rqt\n</code></pre> <p>A window should then open:</p> <p></p> <p></p> </li> <li> <p>From here, we then want to load the Node Graph plugin. From the top menu select <code>Plugins</code> &gt; <code>Introspection</code> &gt; <code>Node Graph</code>.</p> Shortcut <p>You can also launch the <code>rqt_graph</code> node directly with <code>ros2 run</code>:</p> <pre><code>ros2 run rqt_graph rqt_graph\n</code></pre> </li> <li> <p>In the window that opens, select <code>Nodes/Topics (active)</code> from the dropdown menu in the top left. </p> <p>What you should then see is a map of all the nodes in the list from above (as ovals), and arrows to illustrate the flow of information between them. This is a visual representation of the ROS network!</p> <p></p> <p></p> <p>Items that have a rectangular border are ROS Topics. ROS Topics are essentially communication channels, and ROS Nodes can read (subscribe) or write (publish) to these topics to access sensor data, pass information around the network and make things happen.</p> <p>If the <code>teleop_keyboard</code> Node is still active (in TERMINAL 4) then the graph should show us that this node is publishing messages to a topic called <code>/cmd_vel</code>, which in turn is being subscribed to by <code>turtlebot3_node</code>. </p> <p></p> <p></p> <p>This node runs on the robot and controls its velocity. We send instructions to this (by publishing to the <code>/cmd_vel</code> topic) to actually make the robot move.</p> </li> </ol> <p>A ROS Robot could have hundreds of individual nodes running simultaneously to carry out all its necessary operations and actions. Each node runs independently, but uses ROS communication methods to communicate and share data with the other nodes on the ROS Network.</p>"},{"location":"waffles/basics/#publishers-and-subscribers-a-ros-communication-method","title":"Publishers and Subscribers: A ROS Communication Method","text":"<p>ROS Topics are therefore key to making things happen on a robot. Nodes can publish (write) and/or subscribe to (read) ROS Topics in order to share data around the ROS network. Data is published to topics via message-type interfaces.</p> <p>Let's have a look at this in a bit more detail...</p>"},{"location":"waffles/basics/#exTopicMsg","title":"Exercise 4: Exploring ROS Topics and Interfaces","text":"<p>Much like the <code>ros2 node list</code> command, we can use <code>ros2 topic list</code> to list all the topics that are currently active on the ROS network.</p> <ol> <li> <p>Close down the RQT Graph window if you haven't done so already. This will release TERMINAL 5 so that we can enter commands in it again. Return to this terminal window and enter the following:</p> <pre><code>ros2 topic list\n</code></pre> <p>A new list of items should be printed to the terminal now. See if you can spot the <code>/cmd_vel</code> item in the list.</p> <p>As we learnt above, this topic is used to control the velocity of the robot ('command velocity').</p> </li> <li> <p>Let's find out more about this using the <code>ros2 topic info</code> command.</p> <pre><code>ros2 topic info /cmd_vel\n</code></pre> <p>This should provide an output similar to the following: </p> <pre><code>Type: geometry_msgs/msg/TwistStamped\nPublisher count: 1\nSubscription count: 1\n</code></pre> <p>This tells us that the type of data being communicated on the <code>/cmd_vel</code> topic is called: <code>geometry_msgs/msg/TwistStamped</code>. </p> <p>The interface description has three parts:</p> <ol> <li><code>geometry_msgs</code>: The name of the ROS package that this interface belongs to.</li> <li><code>msg</code>: The type of interface. In this case message, but there are other types too. </li> <li><code>TwistStamped</code>: The name of the message interface. </li> </ol> <p>We have just learnt then, that if we want to make the robot move we need to publish <code>TwistStamped</code> messages to the <code>/cmd_vel</code> topic. </p> </li> <li> <p>We can use the <code>ros2 interface</code> command to find out more about the <code>TwistStamped</code> message:</p> <pre><code>ros2 interface show geometry_msgs/msg/TwistStamped\n</code></pre> <p>From this, we should obtain the following:</p> <pre><code>std_msgs/Header header\n        builtin_interfaces/Time stamp\n                int32 sec\n                uint32 nanosec\n        string frame_id\nTwist twist\n        Vector3  linear\n                float64 x\n                float64 y\n                float64 z\n        Vector3  angular\n                float64 x\n                float64 y\n                float64 z\n</code></pre> <p>Here we have a list of fields, subfields and data types. The interface has two base fields (indicated by the lines that are not indented):</p> <p></p><p></p> # Field Name Field Type 1 <code>header</code> <code>std_msgs/Header</code> 2 <code>twist</code> <code>Twist</code> <p></p><p></p> <p>Of the above, we're most interested in Field 2, which contains a further two subfields: </p> <p></p><p></p> # Field Name Field Type 1 <code>linear</code> <code>Vector3</code> 2 <code>angular</code> <code>Vector3</code> <p></p><p></p> <p>Each of these contains 3 further subfields: <code>x</code>, <code>y</code> and <code>z</code>:</p> <p></p><p></p> # Field Name Data Type 1 <code>x</code> <code>float64</code> 2 <code>y</code> <code>float64</code> 3 <code>z</code> <code>float64</code> <p></p><p></p> <p>Let's find out what this all means...</p> </li> </ol>"},{"location":"waffles/basics/#velocity-control","title":"Velocity Control","text":"<p>The motion of any mobile robot can be defined in terms of its three principal axes: <code>X</code>, <code>Y</code> and <code>Z</code>. In the context of our TurtleBot3 Waffle, these axes (and the motion about them) are defined as follows:</p> <p></p> <p>In theory then, a robot can move linearly or angularly about any of these three axes, as shown by the arrows in the figure. That's six Degrees of Freedom (DOFs) in total, achieved based on a robot's design and the actuators it is equipped with. Take a look back at the <code>ros2 interface show</code> output above. Hopefully it's a bit clearer now that the <code>twist</code> subfield of the <code>TwistStamped</code> interface is formatted to give a ROS Programmer the ability to ask a robot to move in any one of its six DOFs. </p> <pre><code>Vector3  linear\n        float64 x  &lt;-- Forwards (or Backwards)\n        float64 y  &lt;-- Left (or Right)\n        float64 z  &lt;-- Up (or Down)\nVector3  angular\n        float64 x  &lt;-- \"Roll\"\n        float64 y  &lt;-- \"Pitch\"\n        float64 z  &lt;-- \"Yaw\"\n</code></pre> <p>Our TurtleBot3 robot only has two motors, so it doesn't actually have six DOFs! The two motors can be controlled independently, which gives it what is called a \"differential drive\" configuration, but this still only allows it to move with two degrees of freedom in total, as illustrated below.</p> <p></p> <p>It can therefore only move linearly in the x-axis (Forwards/Backwards) and angularly in the z-axis (Yaw). </p>"},{"location":"waffles/basics/#exSimpleVelCtrl","title":"Exercise 5: A Python Velocity Control Node","text":"<p>Important</p> <p>Before you start this, make sure you have nothing running in TERMINALS 3, 4 &amp; 5 (enter Ctrl+C in each of these terminals to stop any processes that may be running there).</p> <p>As we've seen, making a robot move with ROS is simply a case of publishing the right data (<code>geometry_msgs/msg/TwistStamped</code>) to the right ROS Topic (<code>/cmd_vel</code>). Earlier we used the <code>teleop_keyboard</code> node to drive the robot around, a bit like a remote control car. In the background here all that was really happening was that the node was converting our keyboard button presses into velocity commands and publishing these to the <code>/cmd_vel</code> topic.</p> <p>In reality, robots need to be able to navigate complex environments autonomously, which is quite a difficult task, and requires us to build bespoke applications. We can build these applications using Python, and we'll look at the core concepts behind this now by building a simple node that will allow us to make our robot a bit more \"autonomous\". What we will do here forms the basis of the more complex applications that you will learn about in the lab course!</p> <ol> <li> <p>Above, we talked about how ROS Nodes should be contained within packages, so let's create one now using a helper script that we've already put together. (This is covered in more detail in the ROS course, but for the purposes of this exercise let's just go ahead and run the script without worrying too much about it!)</p> <p>In TERMINAL 3, navigate to the ROS2 Workspace on the laptop:</p> <pre><code>cd ~/ros2_ws/src/\n</code></pre> </li> <li> <p>From here, use <code>git</code> to clone our \"ROS 2 Package Template\" from GitHub:</p> <pre><code>git clone https://github.com/tom-howard/ros2_pkg_template.git\n</code></pre> </li> <li> <p>This package template contains a script called <code>init_pkg.sh</code>, which can be called to turn the template into a proper ROS 2 package. Run the script as follows, which will convert the template into a ROS 2 package called <code>waffle_demo</code>:</p> <pre><code>./ros2_pkg_template/init_pkg.sh waffle_demo\n</code></pre> </li> <li> <p>Navigate into this new package directory (using <code>cd</code>):</p> <pre><code>cd waffle_demo/ \n</code></pre> </li> <li> <p>This package contains a subdirectory called <code>scripts</code>, and within this there are two basic nodes to get us started:</p> <p></p><pre><code>tree scripts/\n</code></pre> <pre><code>scripts/\n\u251c\u2500\u2500 basic_velocity_control.py\n\u2514\u2500\u2500 stop_me.py\n</code></pre><p></p> </li> <li> <p>Let's open up our package now in Visual Studio Code (VS Code). </p> <pre><code>code .\n</code></pre> <p>Note</p> <p>Don't forget to include the <code>.</code> at the end there, it's important!!</p> </li> <li> <p>Next, in the VS Code file explorer, open up the <code>scripts</code> directory, find the <code>basic_velocity_control.py</code> file and click it to open it up in the editor.</p> <p></p> <p>This is a (fairly) basic ROS 2 Python Node that will control the velocity of the robot. Let's talk through it:</p> <ol> <li> <p>First, we have some imports:</p> <pre><code>import rclpy # (1)!\nfrom geometry_msgs.msg import TwistStamped # (2)!\nimport time # (3)!\n</code></pre> <ol> <li><code>rclpy</code> is the ROS client library for Python. We need this so that our Python node can interact with ROS.</li> <li>We know from earlier that in order to make a robot move we need to publish messages to the <code>/cmd_vel</code> topic, and that this topic uses a data structure (or Interface) called <code>geometry_msgs/msg/TwistStamped</code>. This is how we import the interface into our Python node so that we can create velocity commands for our robot (which we'll get to shortly...)</li> <li>We'll use this to control timing in our node.</li> </ol> <p>Click on the  icons above to reveal more information about each line of the code.</p> </li> <li> <p>Next, we declare some variables that we can use and adapt during the main execution of our code:</p> <pre><code>state = 1 # (1)!\nvel = TwistStamped() # (2)!\n</code></pre> <ol> <li>Inside the <code>while</code> loop (explained shortly) we define two different operational states for the robot, and we can control which one is active by changing this value from <code>1</code> to <code>2</code> (and visa-versa).</li> <li> <p>We're instantiating a <code>TwistStamped</code> Interface message here and calling it <code>vel</code>. We'll assign velocity values to this in the <code>while</code> loop later on.</p> <p>Recall that a <code>TwistStamped</code> message contains six different components that we can assign values to. Which two are relevant to our robot?</p> </li> </ol> </li> <li> <p>Next we configure some important ROS-related things:</p> <pre><code>rclpy.init(args=None) # (1)!\nnode = rclpy.create_node(\"basic_velocity_control\")  # (2)!\nvel_pub = node.create_publisher(TwistStamped, \"cmd_vel\", 10)  # (3)!\n</code></pre> <ol> <li>Initialise <code>rclpy</code> and all the ROS communications that are necessary for our node. </li> <li>Initialise this Python script as an actual ROS node, providing a name for it to be registered on the ROS network with (\"basic_velocity_control\" in this case).</li> <li>Here we're setting up a publisher to the <code>/cmd_vel</code> topic so that the node can send velocity commands to the robot (using <code>TwistStamped</code> data).</li> </ol> </li> <li> <p>After this, we're defining another variable:</p> <pre><code>timestamp = node.get_clock().now().nanoseconds # (1)!\n</code></pre> <ol> <li>What time is it right now? This tells us the current \"ROS Time\" (in nanoseconds), which will be useful to compare against in the while loop.</li> </ol> </li> <li> <p>Now, we enter into a <code>while</code> loop, which is where our code will spend the majority of its time once it's up and running:</p> <pre><code>while rclpy.ok(): # (1)!\n    time_now = node.get_clock().now().nanoseconds # (2)!\n    elapsed_time = (time_now - timestamp) * 1e-9 # (3)!\n\n    ...\n</code></pre> <ol> <li>This returns <code>True</code> as long as the node is alive, so all the code inside the <code>while</code> loop will continue to execute as long as this is the case.</li> <li>What time is it now? Check the time at the start of each iteration of the <code>while</code> loop, and assign this to a variable called <code>time_now</code>.</li> <li>Determine how much time has elapsed (in seconds) since the <code>timestamp</code> was last updated.</li> </ol> <p></p> <ol> <li> <p>An <code>if</code> statement now controls the state of operation for our robot. </p> <ol> <li> <p>In state <code>1</code> we set velocities that will make the robot move forwards (linear X velocity only) for a certain amount of time and then stop. How long will the robot move forwards for, and at what velocity?</p> <pre><code>if state == 1: \n    if elapsed_time &lt; 2: # (1)!\n        vel.twist.linear.x = 0.05 # (2)!\n        vel.twist.angular.z = 0.0\n    else: # (3)!\n        vel.twist.linear.x = 0.0 # (4)!\n        vel.twist.angular.z = 0.0\n        state = 2 # (5)!\n        timestamp = node.get_clock().now().nanoseconds # (6)!\n</code></pre> <ol> <li>If the elapsed time is less than 2 seconds...</li> <li>Set a linear velocity so that the robot will move forwards.</li> <li>If the elapsed time has exceeded 2 seconds...</li> <li>Set our robot's velocities to <code>0.0</code> to make it stop.</li> <li>In the next loop iteration, go into state 2 instead.</li> <li>Reset the timestamp to start counting up again. </li> </ol> </li> <li> <p>In state <code>2</code> we set velocities that will make the robot turn on the spot (angular Z velocity only) for a certain amount of time and then stop. How long will it do this for, and at what velocity?</p> <pre><code>elif state == 2:\n    if elapsed_time &lt; 4: # (1)!\n        vel.twist.linear.x = 0.0\n        vel.twist.angular.z = 0.2 # (2)!\n    else: # (3)!\n        vel.twist.linear.x = 0.0 # (4)!\n        vel.twist.angular.z = 0.0 \n        state = 1 # (5)!\n        timestamp = node.get_clock().now().nanoseconds # (6)!\n</code></pre> <ol> <li>While the elapsed time is less than 4 seconds...</li> <li>Apply an angular velocity to the robot to make it turn on the spot.</li> <li>Once the elapsed time has exceeded 4 seconds...</li> <li>Set the robot's velocities back to <code>0.0</code> to make it stop.</li> <li>In the next loop iteration, go back into state 1 again (moving forwards).</li> <li>Reset the timestamp to start counting up once more. </li> </ol> </li> </ol> </li> <li> <p>And after the <code>if</code> statement:</p> <pre><code>node.get_logger().info( # (1)!\n    f\"\\n[State = {state}] Publishing velocities:\\n\"\n    f\"  - linear.x: {vel.twist.linear.x:.2f} [m/s]\\n\"\n    f\"  - angular.z: {vel.twist.angular.z:.2f} [rad/s].\",\n    throttle_duration_sec=1,\n)\nvel_pub.publish(vel) # (2)!\n</code></pre> <ol> <li>This (and the following 5 lines) will print a message to the terminal, to provide us with regular updates on what state the node is currently in and what velocities have been set (in the <code>if</code> statement above).</li> <li> <p>This line is crucial: this operation actual publishes the velocity commands to the <code>/cmd_vel</code> topic, to make the robot actual act on our instructions.</p> <p>Regardless of what happens in the <code>if</code> states above, we always publish a velocity command to the <code>/cmd_vel</code> topic here (every loop iteration).</p> </li> </ol> </li> </ol> </li> </ol> </li> <li> <p>We're now ready to build our package so that we can run it. We use a tool called \"Colcon\" to do this, but this MUST be run from the root of the ROS 2 Workspace (i.e.: <code>~/ros2_ws/</code>), so let's navigate there now using <code>cd</code>. Head back to TERMINAL 3 and run the following:</p> <pre><code>cd ~/ros2_ws/ \n</code></pre> <p>Then, use the <code>colcon build</code> command to build your package:</p> <pre><code>colcon build --packages-select waffle_demo --symlink-install\n</code></pre> <p>And finally, \"re-source\" the environment:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> <li> <p>Now we can run the code.</p> <p>Note</p> <p>Make sure the robot is on the floor and has enough room to roam around before you do this!</p> <pre><code>ros2 run waffle_demo basic_velocity_control.py\n</code></pre> <p>Observe what the robot does. When you've seen enough, enter Ctrl+C to stop the node.</p> <p>Warning</p> <p>The robot will continue to move even after you've stopped the node! Run the following command to stop it:</p> <pre><code>ros2 run waffle_demo stop_me.py\n</code></pre> </li> <li> <p>Now it's time to adapt the code:</p> <p>The aim here is to make the robot follow a square motion path. What you may have observed when you actually ran the code is that the robot doesn't actually do that! We're using a time-based approach to make the robot switch between two different states continuously:</p> <ol> <li>Moving forwards</li> <li>Turning on the spot</li> </ol> <p>Have a look at the code to work out how much time the robot will currently spend in each state.</p> <p>We want the robot to follow a 0.5m x 0.5m square motion path.  In order to properly achieve this you'll need to adjust the timings, or the robot's velocity, or both. Edit the code so that the robot actually follows a 0.5m x 0.5m square motion path!</p> </li> </ol>"},{"location":"waffles/basics/#slam","title":"SLAM","text":"<p>Simultaneous Localisation and Mapping (SLAM) is a sophisticated tool that is built into ROS. Using data from the robot's LiDAR sensor, plus knowledge of how far the robot has moved<sup>1</sup> a robot is able to create a map of its environment and keep track of its location within that environment at the same time. In the exercise that follows you'll see how easy it is to implement SLAM with the Waffle.  </p>"},{"location":"waffles/basics/#exSlam","title":"Exercise 6: Using SLAM to create a map of the environment","text":"<ol> <li> <p>In TERMINAL 3 enter the following command to launch all the necessary SLAM nodes:</p> <pre><code>ros2 launch tuos_tb3_tools slam.launch.py environment:=real\n</code></pre> Tip <p>On the laptop, this command is also available as an alias: <code>tb3_slam</code>!</p> <p>This will launch a new RViz instance, showing a top-down view of the environment, and dots of various colours representing the real-time LiDAR data. </p> <p></p> <p></p> <p>SLAM will already have begun processing this data to start building a map of the boundaries that are currently visible to the Waffle based on its location in the environment.</p> </li> <li> <p>Return to TERMINAL 4 now and launch the <code>teleop_keyboard</code> node. Start to drive the robot around slowly and carefully to build up a complete map of the area.</p> <p>Tip</p> <p>It's best to do this slowly and perform multiple circuits of the area to build up a more accurate map.</p> <p></p> <p></p> <p></p> </li> <li> <p>Once you're happy that your robot has built up a good map of its environment, you can save this map using the <code>map_saver_cli</code> node from a package called <code>nav2_map_server</code>:</p> <ol> <li> <p>First, create a new directory within your ROS package on the laptop. Return to TERMINAL 5 and navigate to the root of the <code>waffle_demo</code> package that you created earlier:</p> <pre><code>cd ~/ros2_ws/src/waffle_demo\n</code></pre> </li> <li> <p>Create a directory in here called <code>maps</code>: </p> <pre><code>mkdir maps\n</code></pre> </li> <li> <p>Navigate into this directory:</p> <pre><code>cd maps/\n</code></pre> </li> <li> <p>Then, use <code>ros2 run</code> to run the <code>map_saver_cli</code> node and save a copy of your robot's map:</p> <pre><code>ros2 run nav2_map_server map_saver_cli -f MAP_NAME\n</code></pre> <p>Replacing <code>MAP_NAME</code> with an appropriate name for your map. This will create two files: </p> <ol> <li>a <code>MAP_NAME.pgm</code> </li> <li>a <code>MAP_NAME.yaml</code> file</li> </ol> <p>...both of which contain data related to the map that you have just created.</p> </li> <li> <p>The <code>.pgm</code> file can be opened using an application called <code>eog</code> on the laptop: </p> <pre><code>eog MAP_NAME.pgm\n</code></pre> </li> </ol> </li> <li> <p>Return to TERMINAL 3 and close down SLAM by pressing Ctrl+C. The process should stop and RViz should close down.</p> </li> <li> <p>Close down the <code>teleop_keyboard</code> node in TERMINAL 4 as well, if that's still running.</p> </li> </ol>"},{"location":"waffles/basics/#next-steps","title":"Next Steps","text":"<p>\"Pro Tips\": There are some important things to consider when working with the Real Waffles. Move onto the next page to find out more...</p> <p>... and when you've done that, don't forget to power off your robot properly. </p> <ol> <li> <p>You'll learn much more about \"Robot Odometry\" in the lab course.\u00a0\u21a9</p> </li> </ol>"},{"location":"waffles/essentials/","title":"Further (Essential) Exercises","text":"<p>By working through the additional short exercises below, you will become aware of the key differences in how our TurtleBot3 Waffles work in the real world, compared to how they work in simulation. Be mindful of the differences that we are trying to highlight here and the implications that they will have on the applications that you develop. </p> <p>As a general rule, when developing code for real-world applications, it's a good idea to get the basics working in simulation first, but always test out your code in the real-world... just because it works in simulation, doesn't automatically mean it will work on the real robots!!</p>"},{"location":"waffles/essentials/#ex1","title":"Essential Exercise 1: Publishing Velocity Commands","text":"<p>This one actually applies to both the real Waffles and the simulation too. The issue is more critical when working with real thing though, since we're working with real hardware in a real-world environment, where things could get damaged or people could get hurt. </p> <p>In the previous exercises you made the robot move using the <code>teleop_keyboard</code> node, and also built a Python node to control the robot's velocity. Ultimately, making a robot move is achieved by publishing velocity commands (i.e. <code>geometry_msgs/msg/Twist</code> interface messages) to the <code>/cmd_vel</code> topic. Another way to do this is by using the <code>ros2 topic pub</code> command in a terminal. Run the following command and observe what the robot does:</p> <pre><code>ros2 topic pub /cmd_vel geometry_msgs/msg/Twist \"linear:\n  x: 0.0\n  y: 0.0\n  z: 0.0\nangular:\n  x: 0.0\n  y: 0.0\n  z: 0.2\"\n</code></pre> <p>... the robot should turn on the spot.</p> <p>Enter Ctrl+C now to stop the <code>ros2 topic pub</code> command, what happens now?</p> <p>... the robot continues to turn on the spot!</p> <p>In order to actually stop the robot, we need to run the command again, but this time set all the velocities back to zero:</p> <pre><code>ros2 topic pub /cmd_vel geometry_msgs/msg/Twist \"linear:\n  x: 0.0\n  y: 0.0\n  z: 0.0\nangular:\n  x: 0.0\n  y: 0.0\n  z: 0.0\"\n</code></pre>"},{"location":"waffles/essentials/#why-does-this-matter","title":"Why Does This Matter?","text":"<p>If we don't issue a stop command to a robot that is moving, then it will continue to move with the last velocity command that was sent to it. </p> <p>The same applies to ROS nodes when we shut them down: if a stop command is not sent to <code>cmd_vel</code> before the node stops running (i.e. via Ctrl+C) the robot will continue to move, which clearly isn't very good!</p> <p>You must ensure that your nodes are programmed with correct shutdown procedures, to ensure that the robot actually stops moving when a node is terminated. This is covered in Part 2 of Assignment #1, but you can also see this in action in the Velocity Control Node that we created in the previous section. </p>"},{"location":"waffles/essentials/#ex2","title":"Essential Exercise 2: Out of Range LiDAR Data","text":"<p>The robot's LiDAR sensor can only obtain measurements from objects within a certain distance range. In Part 3 we look at how to work out what this range is, using the <code>ros2 topic echo</code> command. Let's apply the same techniques to the real robot now to discover the maximum and minimum distances that the real robot's LiDAR sensor can measure:</p> <p></p><pre><code>ros2 topic echo /scan --field range_min --once\n</code></pre> <pre><code>ros2 topic echo /scan --field range_max --once\n</code></pre><p></p> <p>The LiDAR sensor's measurement range is the same in simulation and on the real robots, but how out-of-range values are reported is different!</p> <p>If the LiDAR sensor detects an object that falls within this range then it will report the exact distance to this object (in meters). Conversely, if it doesn't detect anything within this range then it will report a default out-of-range value instead. In simulation, the out-of-range value is <code>inf</code>.</p> <p>Warning</p> <p>The out-of-range value reported by the real robot's LiDAR sensor is not <code>inf</code>!</p> <p>Use the <code>ros2 topic echo</code> command to view the raw LiDAR data:</p> <pre><code>ros2 topic echo /scan --field ranges\n</code></pre> <p>See if you can position the robot in the environment so that some LiDAR measurements will fall outside the measurement range that you have just determined. How are these values reported in the terminal? </p>"},{"location":"waffles/essentials/#why-does-this-matter_1","title":"Why Does This Matter?","text":"<p>In Part 3 of Course Assignment #1 we illustrate how the LiDAR <code>ranges</code> array can be filtered to remove any out-of-range values: </p> <pre><code>valid_data = front[front != float(\"inf\")] # (1)!\n</code></pre> <ol> <li>Assuming <code>front</code> is a <code>numpy</code> array (see Part 3).</li> </ol> <p>If you apply the same technique to a real-world application, then this filtering will be ineffective, because out-of-range values are not <code>inf</code> here... You'll need to adapt this for real-world scenarios<sup>1</sup>.</p>"},{"location":"waffles/essentials/#ex3","title":"Essential Exercise 3: The Camera Image Topic","text":"<p>In Part 6 of Assignment #1 we work extensively with the robot's camera and the processing of the images that are captured by it. This is done in simulation (of course), where the image data is published to a topic called <code>/camera/image_raw</code>. The name of the camera image topic is not the same on the real robots!</p> <p>With the real robot to hand now, use ROS command-line tools such as <code>ros2 topic list</code> and <code>ros2 topic info</code> to interrogate the real robot ROS Network and identify the name of the camera image topic used by the real robot.</p>"},{"location":"waffles/essentials/#why-does-this-matter_2","title":"Why Does This Matter?","text":"<p>You'll likely do a lot of development work for your real-robot applications in simulation, outside the lab sessions. Some of these applications may involve camera data and image processing. If you set up a subscriber to a topic that doesn't exist, then ROS will not warn you about it! It will simply sit quietly and wait, assuming that the topic will (sooner or later) become available. As a result, if you are running an application on a real robot, that is subscribing to image data on the <code>/camera/image_raw</code> topic, then your application will never receive any image data and any associated callback functions will never execute!<sup>2</sup></p>"},{"location":"waffles/essentials/#ex4","title":"Essential Exercise 4: Camera Image Resolution","text":"<p>In Part 6 of Assignment #1 we also explore how images can be reduced in size by cropping them, to make the data processing a bit quicker and easier. We need to be aware of the original image size (i.e. resolution) however, in order to apply cropping techniques appropriately. </p> <p>As we learn in Part 6, it's possible to discover the resolution of each image that is published to the camera image topic by echoing the <code>height</code> and <code>width</code> parameters that are published as part of the image message interface (alongside the image data itself). We can of course do this now with <code>ros2 topic echo</code>, to determine the resolution of the camera images obtained on the real robot:</p> <pre><code>ros2 topic echo /camera/color/image_raw --field height --once\n</code></pre> <pre><code>ros2 topic echo /camera/color/image_raw --field width --once\n</code></pre> <p>The outputs here will indicate the <code>height</code> and <code>width</code> of the images, in pixels. See how these compare with values that you obtain from the simulation, when you get to Part 6 of the Course.</p> <p>Warning</p> <p>The real robot's camera captures images at a lower image resolution! </p>"},{"location":"waffles/essentials/#why-does-this-matter_3","title":"Why Does This Matter?","text":"<p>We'll learn a lot about image cropping and other image processing techniques through simulation, but (as above) the native image resolution of the simulated robot's camera is much larger than that of the real robot. As such, if you apply the same cropping techniques to real-world applications, without adjustment, then you will end up cropping too much of the image out, leaving nothing to actually apply any further processing to!<sup>3</sup></p>"},{"location":"waffles/essentials/#ex5","title":"Essential Exercise 5: Object Detection","text":"<p>In general, image detection gets a little more challenging in the real-world, where the same object might appear (to a robot's camera) to have slightly different colour tones under different light conditions, from different angles, in different levels of shade, etc. In simulation (again in Part 6 of the Course), you may build an extremely effective <code>colour_search.py</code> node to detect each of the four coloured pillars in the <code>tuos_simulations/coloured_pillars</code> world, but this might not perform as well in the real world without some fine-tuning</p>"},{"location":"waffles/essentials/#why-does-this-matter_4","title":"Why Does This Matter?","text":"<p>Always test out your code in the real-world, just because it works in simulation, doesn't mean it will work on the real robots!!</p>"},{"location":"waffles/essentials/#summary","title":"Summary","text":"<p>You will naturally do a fair bit of development work in simulation, where it's easier to test things out and less disastrous if things go wrong! Overall, you'll be able to develop things much faster this way, and you can do this outside of your weekly lab sessions too. Whilst you're doing this though, keep in mind all the differences that we have identified above, so that there are less nasty surprises when you come to deploy your ROS applications in the real world. </p> <p>Throughout the design phase, think about how your applications could be developed more flexibly to accommodate these variations, or how you could design things so that only small/quick changes/switches need to be made when you transition from testing in simulation, to deploying on a real Waffle. </p> <ol> <li> <p>Exercise 2 Hint: Out-of-range values on the real robots are actually reported as <code>0.0</code>!\u00a0\u21a9</p> </li> <li> <p>Exercise 3 Hint: On the real robots, the camera image topic is <code>/camera/color/image_raw</code>!\u00a0\u21a9</p> </li> <li> <p>Exercise 4 Hint: In simulation, camera images have a resolution of 1080x1920 pixels, whereas on the real robots the resolution is 640x480 pixels.\u00a0\u21a9</p> </li> </ol>"},{"location":"waffles/intro/","title":"Introduction","text":""},{"location":"waffles/intro/#introduction","title":"Introduction","text":""},{"location":"waffles/intro/#handling-the-robots","title":"Handling the Robots","text":"<p>Health and Safety</p> <p>Everyone needs to complete a health and safety quiz related to working with the real robots. This quiz (and the deadline for completing it) is available on Blackboard.</p> <p>Make sure at least one member of your team has completed this before proceeding!</p> <p></p> <p>As you can see from the figure above, the robots have lots of exposed sensors and electronics, so you must take great care when handling them to avoid the robots becoming damaged in any way.  When handling a robot, always hold it by either the black Waffle Layers, or the vertical Support Pillars (as highlighted in the figure above).</p> <p>Important</p> <p>Do not pick the robot up or carry it by the camera or LiDAR sensor! These are delicate devices that could be easily damaged!</p> <p>A lot of the robot's electronics are housed on the middle waffle layer. Try not to touch any of the circuit boards, and take care not to pull on any of the cabling or try to remove or rehouse any of the connections. If you have any concerns with any of the electronics or cabling, if something has come loose, or if your robot doesn't seem to be working properly then ask a member of the teaching team to have a look for you.</p> <p>The robots will be provided to you with a battery already installed and ready to go. Don't try to disconnect or remove the battery yourselves! The robot will beep when the battery is low, and if this happens ask a member of the team to get you a replacement (we have plenty).</p>"},{"location":"waffles/intro/#laptops","title":"The Robot Laptops","text":"<p>You'll be provided with one of our pre-configured Robot Laptops in the lab when working with the real Waffles. This will have all the right software installed and ready to go. </p> <p>There's a <code>student</code> user account already set up, and you'll need to use this when working in the lab. You'll need password to log into this, which we'll provide you with during the lab sessions.</p>"},{"location":"waffles/intro/#network","title":"Network","text":"<p>The Robots and Laptops must be able to connect to one another over an internet connection. The robots connect to a dedicated wireless network running in the Diamond called <code>DIA-LAB</code>. In order for the laptops to be able to \"see\" the robots, they must be connected to the university network using any of the following options:</p> <ol> <li>The <code>eduroam</code> WiFi SSID</li> <li>The <code>DIA-LAB</code> WiFi SSID (no internet access!)</li> <li>A direct wired (ethernet) connection between the robot and laptop</li> </ol> <p>WiFi credentials for <code>DIA-LAB</code> and <code>eduroam</code> have already been set on the laptops, allowing you to connect to either network straight away, but speak to a member of the teaching team if you are having any issues.</p>"},{"location":"waffles/intro/#vs-code","title":"VS Code","text":"<p>Visual Studio Code is installed on the laptops for you to use when working on your real-robot ROS 2 applications. Launch VS Code from any terminal by simply typing <code>code</code>. You can also launch it by clicking the icon in the favourites bar on the left-hand side of the screen:</p> <p></p>"},{"location":"waffles/launching-ros/","title":"Launching ROS (and Pairing the Laptop)","text":"<p>The first step is to launch ROS on the Waffle.</p> <p>Important</p> <p>This ensures that all the core ROS functionality is executed on the robot, without this the robot won't be able to do anything!</p>"},{"location":"waffles/launching-ros/#step-1-identify-your-waffle","title":"Step 1: Identify your Waffle","text":"<p>Robots are named as follows:</p> <pre><code>dia-waffleX\n</code></pre> <p>... where <code>X</code> is the 'Robot Number' (a number between 1 and 50). Make sure you know which robot you are working with by checking the label printed on top of it!</p>"},{"location":"waffles/launching-ros/#step-2-pairing-your-waffle-to-a-laptop","title":"Step 2: Pairing your Waffle to a Laptop","text":"<p>As discussed earlier, you'll be provided with one of our Robotics Laptops to work with in the lab, and the robot needs to be paired with this in order for the two to work together.  </p> <ol> <li> <p>Open up a terminal instance on the laptop, either by using the Ctrl+Alt+T keyboard shortcut, or by clicking the Terminal App icon in the favourites bar on the left-hand side of the desktop:</p> <p></p> <p></p> </li> <li> <p>We'll use our purpose-built <code>waffle</code> CLI to handle the pairing process. Run this in the terminal by entering the following command to pair the laptop and robot:</p> <p></p><pre><code>waffle X pair\n</code></pre> Replacing <code>X</code> with the number of the robot that you are working with.<p></p> </li> <li> <p>You may see a message like this early on in the pairing process:</p> <p></p> <p></p> <p>If so, just type <code>yes</code> and then hit Enter to confirm that you want to continue.</p> </li> <li> <p>Enter the password for the robot when requested (we'll tell you what this is in the lab!)</p> <p>Note</p> <p>You won't see anything change on the screen when you are entering the password. This is normal, just keep typing!!</p> </li> <li> <p>The pairing process will take a minute, but once it's finished you should see a message saying <code>pairing complete</code>, displayed in blue in the terminal. </p> </li> <li> <p>Then, in the same terminal, enter the following command: </p> <p></p><pre><code>waffle X term\n</code></pre> (again, replacing <code>X</code> with the number of your robot).<p></p> <p>A green banner should appear across the bottom of the terminal window:</p> <p></p> <p></p> <p>This is a terminal instance running on the robot, and any commands that you enter here will be executed on the robot (not the laptop!)</p> </li> </ol>"},{"location":"waffles/launching-ros/#step3","title":"Step 3: Launching ROS","text":"<p>Launch ROS on the robot by entering the following command:</p> <pre><code>ros2 launch tuos_tb3_tools ros.launch.py\n</code></pre> <p>If all is well then the robot will play a nice \"do-re-me\" sound and a message like this should appear (amongst all the other text):</p> <pre><code>[tb3_status.py-#] ######################################\n[tb3_status.py-#] ### dia-waffleX is up and running! ###\n[tb3_status.py-#] ######################################\n</code></pre> <p>You shouldn't need to interact with this terminal instance any more now, but the screen will provide you with some regular real-time info related to the status of the robot. As such, keep this terminal open in the background and check on the <code>Battery</code> indicator every now and then:</p> <pre><code>Battery: 12.40V [100%]\n</code></pre> <p>Low Battery </p> <p>The robot's battery won't last a full 2-hour lab session!!</p> <p>When the capacity indicator reaches around 15% then it will start to beep, and when it reaches ~10% it will stop working all together.  Let a member of the teaching team know when the battery is running low and we'll replace it for you. (It's easier to do this when it reaches 15%, rather than waiting until it runs below 10%!)</p>"},{"location":"waffles/launching-ros/#step4","title":"Step 4: Connecting to the Zenoh Router","text":"<p>We use a ROS 2 Middleware (RMW) implementation called Zenoh to enable robot-to-laptop communication via the university wireless network. The Waffle acts as the Zenoh Router, and this was enabled as part of the bringup operations that you launched in Step 3 above. We now need to launch a Session on the laptop to connect to this router. </p> <p>This is Essential!</p> <p>You always need to have a Zenoh session running on the laptop in order to be able to communicate with your robot!</p> <p>Open up a new terminal instance on the laptop (either by using the Ctrl+Alt+T keyboard shortcut, or by clicking the Terminal App icon) and enter the following command:</p> <pre><code>ros2 run rmw_zenoh_cpp rmw_zenohd\n</code></pre> <p>You should now have two terminals active: </p> <ol> <li>The robot terminal where you ran the <code>ros2 launch tuos_tb3_tools ros.launch.py</code> operation (aka \"the bringup\") in Step 3<sup>1</sup></li> <li>The laptop terminal where you just ran the <code>rmw_zenohd</code> node</li> </ol> <p>Leave both of these terminals alone, but keep them running in the background at all times while working with your robot.</p>"},{"location":"waffles/launching-ros/#shutting-down-at-the-end-of-a-lab-session","title":"Shutting Down (at the end of a Lab Session)","text":"<p>When you've finished working with a robot it's really important to shut it down properly before turning off the power switch. Please refer to the safe shutdown procedures for more info.</p> <ol> <li> <p>If you happen to have closed down the robot terminal, you can return to it by entering <code>waffle X term</code> from a new terminal instance on the laptop.\u00a0\u21a9</p> </li> </ol>"},{"location":"waffles/shutdown/","title":"Shutdown Procedures","text":""},{"location":"waffles/shutdown/#robots","title":"Robots","text":"<p>The Waffles are powered by a Single Board Computer (SBC), which runs a full-blown operating system. As with any operating system, it's important to shut it down properly, rather than simply disconnecting the power, to avoid any data loss or other issues. </p> <p>Therefore, once you've finished working with a robot during a lab session, follow the steps below to shut it down.</p> <ol> <li> <p>Open a new terminal instance on the laptop (Ctrl+Alt+T), and enter the following:</p> <p></p><pre><code>waffle X off\n</code></pre> ... replacing <code>X</code> with the number of the robot that you have been working with.<p></p> </li> <li> <p>You'll be asked to confirm that you want to shut the robot down: </p> <pre><code>[INPUT] Are you sure you want to shutdown dia-waffleX? [y/n] &gt;&gt; \n</code></pre> <p>Enter Y and hit Enter and the robot's SBC will be shut down. </p> </li> <li> <p>Once the blue light on the corner of the SBC goes out, it's then safe to slide the power button to the left to completely turn off the device. </p> <p></p> <p></p> </li> </ol>"},{"location":"waffles/shutdown/#laptops","title":"Laptops","text":"<p>Once you've turned off the robot, remember to shut down the laptop too! Do this by clicking the battery icon in the top right of the desktop and selecting the \"Power Off / Log Out\" option in the drop-down menu.</p> <p></p> <p></p><p></p> <p>Hand your robot and laptop back to a member of the teaching team who will put it away for you!</p> <p></p><p></p>"}]}